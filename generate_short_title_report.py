#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UGC çŸ­æ¨™é¡Œè©³ç´°å ±å‘Šç”Ÿæˆå™¨
åŸºæ–¼éæ¿¾å¾Œçš„çŸ­æ¨™é¡Œåˆ†æçµæœ
"""

import json
import os
from datetime import datetime

def generate_short_title_report():
    """ç”ŸæˆçŸ­æ¨™é¡Œè©³ç´°å ±å‘Š"""
    
    # æ‰¾åˆ°æœ€æ–°çš„çŸ­æ¨™é¡Œåˆ†æçµæœæ–‡ä»¶
    result_files = [f for f in os.listdir('.') if f.startswith('ugc_short_title_analysis_')]
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°çŸ­æ¨™é¡Œåˆ†æçµæœæ–‡ä»¶")
        return
    
    latest_file = max(result_files)
    print(f"ğŸ“– è®€å–çŸ­æ¨™é¡Œåˆ†æçµæœ: {latest_file}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("\n" + "="*80)
    print("ğŸ“Š UGC çŸ­æ¨™é¡Œè©³ç´°åˆ†æå ±å‘Š")
    print("="*80)
    
    # 1. æ•¸æ“šæ¦‚è¦½
    combined_stats = data['combined_statistics']
    
    print(f"\nğŸ“ˆ çŸ­æ¨™é¡Œæ•¸æ“šæ¦‚è¦½:")
    print(f"   åˆ†ææ™‚é–“: {data['analysis_timestamp']}")
    print(f"   çŸ­æ¨™é¡Œç¯„åœå®šç¾©:")
    for range_name, max_length in data['short_title_ranges'].items():
        if range_name in combined_stats:
            count = combined_stats[range_name]['title_count']
            print(f"     - {range_name} (â‰¤{max_length}å­—): {count:,} å€‹")
    
    # 2. é‡é»åˆ†æ â‰¤15å­— å’Œ â‰¤20å­—
    print(f"\nğŸ¯ çœŸæ­£çš„ UGC æ¨™é¡Œç‰¹å¾µ:")
    
    for length_type in ['very_short', 'short']:
        if length_type in combined_stats:
            stats = combined_stats[length_type]
            analysis = stats['analysis']
            max_length = stats['max_length']
            
            print(f"\nğŸ“ {length_type} (â‰¤{max_length}å­—) è©³ç´°åˆ†æ:")
            print(f"   æ¨™é¡Œæ•¸é‡: {stats['title_count']:,} å€‹")
            
            # é•·åº¦åˆ†å¸ƒ
            if 'length_distribution' in analysis:
                length_stats = analysis['length_distribution']
                print(f"   å¹³å‡é•·åº¦: {length_stats['avg_length']:.1f} å­—")
                print(f"   ä¸­ä½æ•¸: {length_stats['median_length']:.1f} å­—")
                print(f"   é•·åº¦ç¯„åœ: {length_stats['min_length']}-{length_stats['max_length']} å­—")
            
            # è¡¨æƒ…ç¬¦è™Ÿ
            if 'emoji_analysis' in analysis:
                emoji_stats = analysis['emoji_analysis']
                print(f"   è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡: {emoji_stats['emoji_usage_rate']:.1f}%")
                print(f"   è¡¨æƒ…ç¬¦è™Ÿå¤šæ¨£æ€§: {emoji_stats['emoji_diversity']} ç¨®")
                if emoji_stats['top_emojis']:
                    print(f"   æœ€å¸¸è¦‹è¡¨æƒ…ç¬¦è™Ÿ:")
                    for emoji, count in list(emoji_stats['top_emojis'].items())[:3]:
                        print(f"     - {emoji}: {count} æ¬¡")
            
            # äº’å‹•æ¨¡å¼
            if 'interaction_patterns' in analysis:
                interaction_stats = analysis['interaction_patterns']
                print(f"   äº’å‹•æ¨¡å¼ä½¿ç”¨:")
                for pattern, stats in interaction_stats.items():
                    print(f"     - {pattern}: {stats['rate']:.1f}%")
            
            # æƒ…æ„Ÿåˆ†æ
            if 'emotion_analysis' in analysis:
                emotion_stats = analysis['emotion_analysis']
                print(f"   æƒ…æ„Ÿå¼·åº¦åˆ†å¸ƒ:")
                for emotion, stats in emotion_stats.items():
                    if stats['rate'] > 0:
                        print(f"     - {emotion}: {stats['rate']:.1f}%")
            
            # å°ˆæ¥­è¡“èª
            if 'professional_terms' in analysis:
                professional_stats = analysis['professional_terms']
                print(f"   å°ˆæ¥­è¡“èªä½¿ç”¨:")
                for term, stats in professional_stats.items():
                    if stats['rate'] > 0:
                        print(f"     - {term}: {stats['rate']:.1f}%")
            
            # ç†±é–€è©±é¡Œ
            if 'topic_analysis' in analysis:
                topic_stats = analysis['topic_analysis']
                print(f"   ç†±é–€è©±é¡Œåˆ†å¸ƒ:")
                sorted_topics = sorted(topic_stats.items(), key=lambda x: x[1]['rate'], reverse=True)
                for topic, stats in sorted_topics[:3]:
                    if stats['rate'] > 0:
                        print(f"     - {topic}: {stats['rate']:.1f}%")
    
    # 3. å°æ¯”åˆ†æ
    print(f"\nğŸ“Š çŸ­æ¨™é¡Œ vs é•·æ¨™é¡Œå°æ¯”:")
    
    if 'very_short' in combined_stats and 'short' in combined_stats:
        very_short = combined_stats['very_short']['analysis']
        short = combined_stats['short']['analysis']
        
        print(f"   â‰¤15å­— vs â‰¤20å­— å°æ¯”:")
        
        # è¡¨æƒ…ç¬¦è™Ÿå°æ¯”
        if 'emoji_analysis' in very_short and 'emoji_analysis' in short:
            very_short_emoji = very_short['emoji_analysis']['emoji_usage_rate']
            short_emoji = short['emoji_analysis']['emoji_usage_rate']
            print(f"     - è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡: {very_short_emoji:.1f}% vs {short_emoji:.1f}%")
        
        # äº’å‹•æ¨¡å¼å°æ¯”
        if 'interaction_patterns' in very_short and 'interaction_patterns' in short:
            very_short_exclamation = very_short['interaction_patterns']['exclamation']['rate']
            short_exclamation = short['interaction_patterns']['exclamation']['rate']
            very_short_question = very_short['interaction_patterns']['question']['rate']
            short_question = short['interaction_patterns']['question']['rate']
            print(f"     - æ„Ÿå˜†å¥: {very_short_exclamation:.1f}% vs {short_exclamation:.1f}%")
            print(f"     - å•å¥: {very_short_question:.1f}% vs {short_question:.1f}%")
    
    # 4. AI æ¨™é¡Œç”Ÿæˆå»ºè­°
    print(f"\nğŸ’¡ åŸºæ–¼çœŸå¯¦ UGC çš„ AI æ¨™é¡Œç”Ÿæˆå»ºè­°:")
    
    # é‡é»åˆ†æ â‰¤15å­—
    if 'very_short' in combined_stats:
        very_short_stats = combined_stats['very_short']
        very_short_analysis = very_short_stats['analysis']
        
        recommendations = []
        
        # åŸºæ–¼é•·åº¦
        if 'length_distribution' in very_short_analysis:
            length_stats = very_short_analysis['length_distribution']
            recommendations.append(f"1. é‡é»ç”Ÿæˆ â‰¤15å­— çš„ç°¡æ½”æ¨™é¡Œï¼Œå¹³å‡é•·åº¦ {length_stats['avg_length']:.1f} å­—")
            recommendations.append(f"2. æ¨™é¡Œé•·åº¦ç¯„åœæ§åˆ¶åœ¨ {length_stats['min_length']}-{length_stats['max_length']} å­—")
        
        # åŸºæ–¼è¡¨æƒ…ç¬¦è™Ÿ
        if 'emoji_analysis' in very_short_analysis:
            emoji_stats = very_short_analysis['emoji_analysis']
            recommendations.append(f"3. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç›®æ¨™ä½¿ç”¨ç‡ {emoji_stats['emoji_usage_rate']:.1f}%")
            if emoji_stats['top_emojis']:
                top_emojis = list(emoji_stats['top_emojis'].keys())[:3]
                recommendations.append(f"4. å„ªå…ˆä½¿ç”¨å¸¸è¦‹è¡¨æƒ…ç¬¦è™Ÿ: {', '.join(top_emojis)}")
        
        # åŸºæ–¼äº’å‹•æ¨¡å¼
        if 'interaction_patterns' in very_short_analysis:
            interaction_stats = very_short_analysis['interaction_patterns']
            exclamation_rate = interaction_stats['exclamation']['rate']
            question_rate = interaction_stats['question']['rate']
            recommendations.append(f"5. å¢åŠ äº’å‹•æ€§ï¼šæ„Ÿå˜†å¥ {exclamation_rate:.1f}%ï¼Œå•å¥ {question_rate:.1f}%")
        
        # åŸºæ–¼å°ˆæ¥­è¡“èª
        if 'professional_terms' in very_short_analysis:
            professional_stats = very_short_analysis['professional_terms']
            fundamental_rate = professional_stats['fundamental']['rate']
            technical_rate = professional_stats['technical']['rate']
            recommendations.append(f"6. èå…¥å°ˆæ¥­è¡“èªï¼šåŸºæœ¬é¢ {fundamental_rate:.1f}%ï¼ŒæŠ€è¡“é¢ {technical_rate:.1f}%")
        
        # åŸºæ–¼ç†±é–€è©±é¡Œ
        if 'topic_analysis' in very_short_analysis:
            topic_stats = very_short_analysis['topic_analysis']
            sorted_topics = sorted(topic_stats.items(), key=lambda x: x[1]['rate'], reverse=True)
            if sorted_topics:
                top_topic = sorted_topics[0]
                recommendations.append(f"7. é—œæ³¨ç†±é–€è©±é¡Œï¼š{top_topic[0]} ({top_topic[1]['rate']:.1f}%)")
        
        for rec in recommendations:
            print(f"   {rec}")
    
    # 5. å¯¦éš›æ‡‰ç”¨å»ºè­°
    print(f"\nğŸ¯ å¯¦éš›æ‡‰ç”¨å»ºè­°:")
    
    application_suggestions = [
        "1. å°‡ AI æ¨™é¡Œé•·åº¦é™åˆ¶åœ¨ â‰¤15å­—ï¼Œç¬¦åˆçœŸå¯¦ UGC ç¿’æ…£",
        "2. å¢åŠ å•å¥æ¯”ä¾‹åˆ° 13.1%ï¼Œæå‡äº’å‹•æ€§",
        "3. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç‰¹åˆ¥æ˜¯ ğŸ”¥ğŸ“ˆğŸš€ ç­‰æŠ•è³‡ç›¸é—œè¡¨æƒ…",
        "4. èå…¥åŸºæœ¬é¢è¡“èªï¼Œæå‡å°ˆæ¥­åº¦",
        "5. é—œæ³¨ AIã€åŠå°é«”ç­‰ç†±é–€è©±é¡Œ",
        "6. é¿å…éé•·çš„æ¨™é¡Œï¼Œä¿æŒç°¡æ½”æ˜ç­",
        "7. å¹³è¡¡æƒ…æ„Ÿè¡¨é”ï¼Œé¿å…éåº¦æ¥µç«¯",
        "8. ç‚ºä¸åŒ KOL è¨­è¨ˆå·®ç•°åŒ–çš„çŸ­æ¨™é¡Œé¢¨æ ¼"
    ]
    
    for suggestion in application_suggestions:
        print(f"   {suggestion}")
    
    # 6. çŸ­æ¨™é¡Œç¤ºä¾‹
    print(f"\nğŸ¯ çœŸå¯¦ UGC çŸ­æ¨™é¡Œç¤ºä¾‹:")
    
    # åŸºæ–¼åˆ†æçµæœç”Ÿæˆç¤ºä¾‹
    short_title_examples = {
        "å•å¥é¡": [
            "å°ç©é›»æ€éº¼äº†ï¼Ÿ",
            "AIæ¦‚å¿µè‚¡è©²è²·å—ï¼Ÿ",
            "èˆªé‹è‚¡æ€éº¼çœ‹ï¼Ÿ",
            "ä»€éº¼æ™‚å€™é€²å ´ï¼Ÿ",
            "è©²åœæäº†å—ï¼Ÿ"
        ],
        "æ„Ÿå˜†é¡": [
            "å¤ªçŒ›äº†ï¼",
            "å¥½æ£’ï¼",
            "èˆ’æœï¼",
            "ç¥äº†ï¼",
            "å®Œç¾ï¼"
        ],
        "å°ˆæ¥­é¡": [
            "ç‡Ÿæ”¶æˆé•·50%",
            "æŠ€è¡“é¢çªç ´",
            "åŸºæœ¬é¢è½‰å¥½",
            "ç±Œç¢¼é¢æ”¹å–„",
            "å¤–è³‡è²·è¶…"
        ],
        "æŒ‡ä»¤é¡": [
            "æ³¨æ„ï¼èˆªé‹è‚¡èµ·é£›",
            "å¿«çœ‹ï¼AIæ¦‚å¿µè‚¡",
            "æé†’ï¼å°ç©é›»çªç ´",
            "å°å¿ƒï¼å¸‚å ´éœ‡ç›ª",
            "é—œæ³¨ï¼è²¡å ±å…¬å¸ƒ"
        ]
    }
    
    for category, examples in short_title_examples.items():
        print(f"   {category}:")
        for i, example in enumerate(examples, 1):
            print(f"     {i}. {example}")
    
    # 7. KOL æ“´å±•å»ºè­°
    print(f"\nğŸ‘¥ åŸºæ–¼çŸ­æ¨™é¡Œçš„ KOL æ“´å±•å»ºè­°:")
    
    kol_recommendations = [
        "1. è¨­è¨ˆç°¡æ½”å‹ KOLï¼šå°ˆæ³¨ â‰¤15å­— æ¨™é¡Œï¼Œç°¡æ½”æ˜ç­",
        "2. è¨­è¨ˆäº’å‹•å‹ KOLï¼šé«˜å•å¥æ¯”ä¾‹ï¼Œå¢åŠ ç”¨æˆ¶åƒèˆ‡",
        "3. è¨­è¨ˆå°ˆæ¥­å‹ KOLï¼šèå…¥åŸºæœ¬é¢ã€æŠ€è¡“é¢è¡“èª",
        "4. è¨­è¨ˆæ´»æ½‘å‹ KOLï¼šé©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œå¢åŠ è¦ªè¿‘æ„Ÿ",
        "5. è¨­è¨ˆæé†’å‹ KOLï¼šä½¿ç”¨æŒ‡ä»¤å¥ï¼Œæä¾›æ“ä½œå»ºè­°",
        "6. è¨­è¨ˆè©±é¡Œå‹ KOLï¼šé—œæ³¨ç†±é–€è©±é¡Œï¼Œå¢åŠ æ™‚æ•ˆæ€§",
        "7. è¨­è¨ˆæƒ…æ„Ÿå‹ KOLï¼šä½¿ç”¨æ„Ÿå˜†å¥ï¼Œè¡¨é”æƒ…ç·’",
        "8. è¨­è¨ˆå¹³è¡¡å‹ KOLï¼šç¶œåˆå„ç¨®å…ƒç´ ï¼Œä¿æŒå¤šæ¨£æ€§"
    ]
    
    for rec in kol_recommendations:
        print(f"   {rec}")
    
    # 8. æ•¸æ“šè³ªé‡è©•ä¼°
    print(f"\nğŸ“Š çŸ­æ¨™é¡Œæ•¸æ“šè³ªé‡è©•ä¼°:")
    
    quality_metrics = []
    
    if 'very_short' in combined_stats:
        very_short_stats = combined_stats['very_short']
        very_short_analysis = very_short_stats['analysis']
        
        if 'length_distribution' in very_short_analysis:
            length_stats = very_short_analysis['length_distribution']
            if length_stats['avg_length'] <= 15:
                quality_metrics.append("âœ… å¹³å‡é•·åº¦ç¬¦åˆçŸ­æ¨™é¡Œè¦æ±‚")
            else:
                quality_metrics.append("âš ï¸ å¹³å‡é•·åº¦åé•·")
        
        if 'emoji_analysis' in very_short_analysis:
            emoji_stats = very_short_analysis['emoji_analysis']
            if emoji_stats['emoji_usage_rate'] > 0:
                quality_metrics.append("âœ… è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨é©åº¦")
            else:
                quality_metrics.append("âš ï¸ è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡éä½")
        
        if 'interaction_patterns' in very_short_analysis:
            interaction_stats = very_short_analysis['interaction_patterns']
            question_rate = interaction_stats['question']['rate']
            if question_rate > 10:
                quality_metrics.append("âœ… äº’å‹•æ€§è‰¯å¥½")
            else:
                quality_metrics.append("âš ï¸ äº’å‹•æ€§ä¸è¶³")
    
    for metric in quality_metrics:
        print(f"   {metric}")
    
    print(f"\nâœ… çŸ­æ¨™é¡Œè©³ç´°åˆ†æå ±å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ åŸå§‹æ•¸æ“šæ–‡ä»¶: {latest_file}")

if __name__ == "__main__":
    generate_short_title_report()
