#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½åˆ†æå¤§å‹ UGC æ•¸æ“šé›† - å¤šæ–‡ä»¶èåˆåˆ†æ
æ”¯æŒå¤§æ•¸æ“šè™•ç†å’Œ KOL æ“´å±•
"""

import re
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import json
import time
from datetime import datetime
import gc
import os

class SmartUGCAnalyzer:
    """æ™ºèƒ½ UGC åˆ†æå™¨"""
    
    def __init__(self):
        self.data_files = [
            'anya-forumteam-1756973422036-23159383983751583.tsv',  # ç¬¬ä¸€å€‹æ–‡ä»¶
            'anya-forumteam-1756973297558-23159259505732695.tsv'   # æ–°çš„å¤§æ–‡ä»¶
        ]
        self.results = {}
        self.combined_stats = {}
        
        # é å®šç¾©çš„åˆ†ææ¨¡å¼
        self.analysis_patterns = {
            'length_distribution': self.analyze_length_distribution,
            'emoji_analysis': self.analyze_emojis,
            'stock_codes': self.analyze_stock_codes,
            'time_patterns': self.analyze_time_patterns,
            'number_patterns': self.analyze_number_patterns,
            'topic_analysis': self.analyze_topics,
            'emotion_analysis': self.analyze_emotions,
            'interaction_patterns': self.analyze_interaction_patterns,
            'professional_terms': self.analyze_professional_terms,
            'kol_patterns': self.analyze_kol_patterns,
            'content_clustering': self.analyze_content_clustering
        }
    
    def smart_read_large_file(self, filepath, sample_size=100000):
        """æ™ºèƒ½è®€å–å¤§æ–‡ä»¶ï¼Œä½¿ç”¨æŠ½æ¨£æ–¹æ³•"""
        print(f"ğŸ“– æ­£åœ¨æ™ºèƒ½è®€å–å¤§æ–‡ä»¶: {filepath}")
        
        # ç²å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        # æ ¹æ“šæ–‡ä»¶å¤§å°æ±ºå®šæŠ½æ¨£ç­–ç•¥
        if file_size > 100:  # å¤§æ–¼ 100MB
            print("   ğŸ”„ ä½¿ç”¨æ™ºèƒ½æŠ½æ¨£ç­–ç•¥...")
            
            # è®€å–å‰ 10% å’Œå¾Œ 10% çš„æ•¸æ“š
            total_lines = sum(1 for _ in open(filepath, 'r', encoding='utf-8'))
            sample_lines = min(sample_size, total_lines // 10)
            
            titles = []
            
            # è®€å–é–‹é ­éƒ¨åˆ†
            with open(filepath, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= sample_lines:
                        break
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            # è®€å–çµå°¾éƒ¨åˆ†
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                end_start = max(0, len(lines) - sample_lines)
                for line in lines[end_start:]:
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            # éš¨æ©ŸæŠ½æ¨£ä¸­é–“éƒ¨åˆ†
            if len(lines) > sample_lines * 2:
                import random
                middle_lines = lines[sample_lines:-sample_lines]
                random_sample = random.sample(middle_lines, min(sample_lines, len(middle_lines)))
                for line in random_sample:
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            print(f"   âœ… æŠ½æ¨£å®Œæˆï¼Œå…±è®€å– {len(titles)} å€‹æ¨™é¡Œ")
            
        else:
            # å°æ–‡ä»¶ç›´æ¥è®€å–
            print("   ğŸ“– ç›´æ¥è®€å–å®Œæ•´æ–‡ä»¶...")
            with open(filepath, 'r', encoding='utf-8') as f:
                titles = [line.strip() for line in f if line.strip() and line.strip() != 'title' and line.strip() != '""']
        
        return titles
    
    def analyze_length_distribution(self, titles):
        """åˆ†ææ¨™é¡Œé•·åº¦åˆ†å¸ƒ"""
        lengths = [len(title) for title in titles]
        
        return {
            'total_count': len(titles),
            'avg_length': np.mean(lengths),
            'median_length': np.median(lengths),
            'min_length': min(lengths),
            'max_length': max(lengths),
            'std_length': np.std(lengths),
            'distribution': {
                'short': len([l for l in lengths if l <= 10]),
                'medium': len([l for l in lengths if 10 < l <= 20]),
                'long': len([l for l in lengths if l > 20])
            }
        }
    
    def analyze_emojis(self, titles):
        """åˆ†æè¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨"""
        emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ï¸â¤ï¸ğŸ’ğŸ’°ğŸ’µğŸ’¸ğŸ‰ğŸŠğŸ¯ğŸªğŸ­ğŸ¨ğŸ¬ğŸ¤ğŸ§ğŸµğŸ¶ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ†ğŸˆğŸ‰ğŸŠğŸ‹ï¸ğŸŒï¸ğŸï¸ğŸï¸ğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ï¸ğŸ•ï¸ğŸ–ï¸ğŸ—ï¸ğŸ˜ï¸ğŸ™ï¸ğŸšï¸ğŸ›ï¸ğŸœï¸ğŸï¸ğŸï¸ğŸŸï¸ğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ï¸ğŸ´ï¸ğŸµï¸ğŸ¶ğŸ·ï¸ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ…ğŸ†ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ğŸ•ğŸ–ğŸ—ğŸ˜ğŸ™ğŸšğŸ›ğŸœğŸğŸğŸŸğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ğŸ´ğŸµğŸ¶ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ï¸ğŸ‘€ğŸ‘ï¸ğŸ‘‚ğŸ‘ƒğŸ‘„ğŸ‘…ğŸ‘†ğŸ‘‡ğŸ‘ˆğŸ‘‰ğŸ‘ŠğŸ‘‹ğŸ‘ŒğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘‘ğŸ‘’ğŸ‘“ğŸ‘”ğŸ‘•ğŸ‘–ğŸ‘—ğŸ‘˜ğŸ‘™ğŸ‘šğŸ‘›ğŸ‘œğŸ‘ğŸ‘ğŸ‘ŸğŸ‘ ğŸ‘¡ğŸ‘¢ğŸ‘£ğŸ‘¤ğŸ‘¥ğŸ‘¦ğŸ‘§ğŸ‘¨ğŸ‘©ğŸ‘ªğŸ‘«ğŸ‘¬ğŸ‘­ğŸ‘®ğŸ‘¯ğŸ‘°ğŸ‘±ğŸ‘²ğŸ‘³ğŸ‘´ğŸ‘µğŸ‘¶ğŸ‘·ğŸ‘¸ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ‘¼ğŸ‘½ğŸ‘¾ğŸ‘¿ğŸ’€ğŸ’ğŸ’‚ğŸ’ƒğŸ’„ğŸ’…ğŸ’†ğŸ’‡ğŸ’ˆğŸ’‰ğŸ’ŠğŸ’‹ğŸ’ŒğŸ’ğŸ’ğŸ’ğŸ’ğŸ’‘ğŸ’’ğŸ’“ğŸ’”ğŸ’•ğŸ’–ğŸ’—ğŸ’˜ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ’ğŸ’ğŸ’ŸğŸ’ ğŸ’¡ğŸ’¢ğŸ’£ğŸ’¤ğŸ’¥ğŸ’¦ğŸ’§ğŸ’¨ğŸ’©ğŸ’ªğŸ’«ğŸ’¬ğŸ’­ğŸ’®ğŸ’¯ğŸ’°ğŸ’±ğŸ’²ğŸ’³ğŸ’´ğŸ’µğŸ’¶ğŸ’·ğŸ’¸ğŸ’¹ğŸ’ºğŸ’»ğŸ’¼ğŸ’½ğŸ’¾ğŸ’¿ğŸ“€ğŸ“ğŸ“‚ğŸ“ƒğŸ“„ğŸ“…ğŸ“†ğŸ“‡ğŸ“ˆğŸ“‰ğŸ“ŠğŸ“‹ğŸ“ŒğŸ“ğŸ“ğŸ“ğŸ“ğŸ“‘ğŸ“’ğŸ““ğŸ“”ğŸ“•ğŸ“–ğŸ“—ğŸ“˜ğŸ“™ğŸ“šğŸ“›ğŸ“œğŸ“ğŸ“ğŸ“ŸğŸ“ ğŸ“¡ğŸ“¢ğŸ“£ğŸ“¤ğŸ“¥ğŸ“¦ğŸ“§ğŸ“¨ğŸ“©ğŸ“ªğŸ“«ğŸ“¬ğŸ“­ğŸ“®ğŸ“¯ğŸ“°ğŸ“±ğŸ“²ğŸ“³ğŸ“´ğŸ“µğŸ“¶ğŸ“·ğŸ“¸ğŸ“¹ğŸ“ºğŸ“»ğŸ“¼ğŸ“½ï¸ğŸ“¾ğŸ“¿ğŸ”€ğŸ”ğŸ”‚ğŸ”ƒğŸ”„ğŸ”…ğŸ”†ğŸ”‡ğŸ”ˆğŸ”‰ğŸ”ŠğŸ”‹ğŸ”ŒğŸ”ğŸ”ğŸ”ğŸ”ğŸ”‘ğŸ”’ğŸ”“ğŸ””ğŸ”•ğŸ”–ğŸ”—ğŸ”˜ğŸ”™ğŸ”šğŸ”›ğŸ”œğŸ”ğŸ”ğŸ”ŸğŸ” ğŸ”¡ğŸ”¢ğŸ”£ğŸ”¤ğŸ”¥ğŸ”¦ğŸ”§ğŸ”¨ğŸ”©ğŸ”ªğŸ”«ğŸ”¬ğŸ”­ğŸ”®ğŸ”¯ğŸ”°ğŸ”±ğŸ”²ğŸ”³ğŸ”´ğŸ”µğŸ”¶ğŸ”·ğŸ”¸ğŸ”¹ğŸ”ºğŸ”»ğŸ”¼ğŸ”½ğŸ”¾ğŸ”¿ğŸ•€ğŸ•ğŸ•‚ğŸ•ƒğŸ•„ğŸ•…ğŸ•†ğŸ•‡ğŸ•ˆğŸ•‰ğŸ•ŠğŸ•‹ğŸ•ŒğŸ•ğŸ•ğŸ•ğŸ•ğŸ•‘ğŸ•’ğŸ•“ğŸ•”ğŸ••ğŸ•–ğŸ•—ğŸ•˜ğŸ•™ğŸ•šğŸ•›ğŸ•œğŸ•ğŸ•ğŸ•ŸğŸ• ğŸ•¡ğŸ•¢ğŸ•£ğŸ•¤ğŸ•¥ğŸ•¦ğŸ•§ğŸ•¨ğŸ•©ğŸ•ªğŸ•«ğŸ•¬ğŸ•­ğŸ•®ğŸ•¯ğŸ•°ï¸ğŸ•±ğŸ•²ğŸ•³ğŸ•´ï¸ğŸ•µï¸ğŸ•¶ï¸ğŸ•·ï¸ğŸ•¸ï¸ğŸ•¹ï¸ğŸ•ºğŸ•»ğŸ•¼ğŸ•½ğŸ•¾ğŸ•¿ğŸ–€ğŸ–ğŸ–‚ğŸ–ƒğŸ–„ğŸ–…ğŸ–†ğŸ–‡ï¸ğŸ–ˆğŸ–‰ğŸ–Šï¸ğŸ–‹ï¸ğŸ–Œï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–‘ğŸ–’ğŸ–“ğŸ–”ï¸ğŸ–•ğŸ––ğŸ–—ğŸ–˜ğŸ–™ğŸ–šğŸ–›ğŸ–œğŸ–ğŸ–ğŸ–ŸğŸ– ğŸ–¡ğŸ–¢ğŸ–£ğŸ–¤ğŸ–¥ï¸ğŸ–¦ğŸ–§ğŸ–¨ï¸ğŸ–©ğŸ–ªğŸ–«ğŸ–¬ğŸ–­ğŸ–®ğŸ–¯ğŸ–°ï¸ğŸ–±ï¸ğŸ–²ï¸ğŸ–³ï¸ğŸ–´ï¸ğŸ–µï¸ğŸ–¶ğŸ–·ï¸ğŸ–¸ğŸ–¹ğŸ–ºğŸ–»ğŸ–¼ï¸ğŸ–½ğŸ–¾ğŸ–¿ğŸ—€ğŸ—ğŸ—‚ï¸ğŸ—ƒï¸ğŸ—„ï¸ğŸ—…ğŸ—†ğŸ—‡ğŸ—ˆğŸ—‰ğŸ—ŠğŸ—‹ğŸ—ŒğŸ—ğŸ—ğŸ—ğŸ—ğŸ—‘ğŸ—’ğŸ—“ï¸ğŸ—”ï¸ğŸ—•ï¸ğŸ—–ï¸ğŸ——ğŸ—˜ğŸ—™ğŸ—šğŸ—›ğŸ—œğŸ—ğŸ—ï¸ğŸ—Ÿï¸ğŸ— ï¸ğŸ—¡ï¸ğŸ—¢ï¸ğŸ—£ï¸ğŸ—¤ï¸ğŸ—¥ï¸ğŸ—¦ï¸ğŸ—§ï¸ğŸ—¨ï¸ğŸ—©ï¸ğŸ—ªï¸ğŸ—«ï¸ğŸ—¬ï¸ğŸ—­ï¸ğŸ—®ï¸ğŸ—¯ï¸ğŸ—°ï¸ğŸ—±ï¸ğŸ—²ï¸ğŸ—³ï¸ğŸ—´ï¸ğŸ—µï¸ğŸ—¶ï¸ğŸ—·ï¸ğŸ—¸ï¸ğŸ—¹ï¸ğŸ—ºï¸ğŸ—»ï¸ğŸ—¼ï¸ğŸ—½ï¸ğŸ—¾ï¸ğŸ—¿ï¸]')
        
        emoji_titles = [title for title in titles if emoji_pattern.search(title)]
        emoji_counts = Counter()
        
        for title in emoji_titles:
            emojis = emoji_pattern.findall(title)
            emoji_counts.update(emojis)
        
        return {
            'emoji_titles_count': len(emoji_titles),
            'emoji_usage_rate': len(emoji_titles) / len(titles) * 100,
            'top_emojis': dict(emoji_counts.most_common(10)),
            'emoji_diversity': len(emoji_counts)
        }
    
    def analyze_stock_codes(self, titles):
        """åˆ†æè‚¡ç¥¨ä»£è™Ÿä½¿ç”¨"""
        stock_pattern = re.compile(r'\b\d{4}\b')
        stock_titles = [title for title in titles if stock_pattern.search(title)]
        stock_codes = []
        
        for title in stock_titles:
            codes = stock_pattern.findall(title)
            stock_codes.extend(codes)
        
        stock_counts = Counter(stock_codes)
        
        return {
            'stock_titles_count': len(stock_titles),
            'stock_usage_rate': len(stock_titles) / len(titles) * 100,
            'top_stocks': dict(stock_counts.most_common(10)),
            'stock_diversity': len(stock_counts)
        }
    
    def analyze_time_patterns(self, titles):
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        time_patterns = {
            'date': re.compile(r'\d{1,2}/\d{1,2}'),
            'month': re.compile(r'\d{1,2}æœˆ'),
            'year': re.compile(r'20\d{2}'),
            'time': re.compile(r'\d{1,2}:\d{2}')
        }
        
        results = {}
        for pattern_name, pattern in time_patterns.items():
            matching_titles = [title for title in titles if pattern.search(title)]
            results[pattern_name] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_number_patterns(self, titles):
        """åˆ†ææ•¸å­—æ¨¡å¼"""
        number_patterns = {
            'percentage': re.compile(r'\d+%'),
            'price': re.compile(r'\d+\.?\d*å…ƒ'),
            'shares': re.compile(r'\d+å¼µ'),
            'ten_thousand': re.compile(r'\d+è¬'),
            'hundred_million': re.compile(r'\d+å„„'),
            'points': re.compile(r'\d+é»')
        }
        
        results = {}
        for pattern_name, pattern in number_patterns.items():
            matching_titles = [title for title in titles if pattern.search(title)]
            results[pattern_name] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_topics(self, titles):
        """åˆ†æç†±é–€è©±é¡Œ"""
        topics = {
            'AI': ['AI', 'äººå·¥æ™ºæ…§', 'æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’'],
            'semiconductor': ['åŠå°é«”', 'æ™¶ç‰‡', 'IC', 'æ™¶åœ“', 'å°ç©é›»', 'è¯ç™¼ç§‘'],
            'shipping': ['èˆªé‹', 'è²¨æ«ƒ', 'æµ·é‹', 'é•·æ¦®', 'é™½æ˜', 'è¬æµ·'],
            'biotech': ['ç”ŸæŠ€', 'é†«è—¥', 'è—¥å“', 'ç–«è‹—', 'åŸºå› '],
            'finance': ['é‡‘è', 'éŠ€è¡Œ', 'ä¿éšª', 'è­‰åˆ¸', 'é‡‘æ§'],
            'ev': ['é›»å‹•è»Š', 'ç‰¹æ–¯æ‹‰', 'é›»è»Š', 'æ–°èƒ½æº'],
            'metaverse': ['å…ƒå®‡å®™', 'VR', 'AR', 'è™›æ“¬å¯¦å¢ƒ'],
            '5g': ['5G', 'é€šè¨Š', 'é›»ä¿¡'],
            'green_energy': ['ç¶ èƒ½', 'å¤ªé™½èƒ½', 'é¢¨é›»', 'ç’°ä¿', 'ç¢³ä¸­å’Œ']
        }
        
        results = {}
        for topic, keywords in topics.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[topic] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_emotions(self, titles):
        """åˆ†ææƒ…æ„Ÿæ¨¡å¼"""
        emotions = {
            'very_positive': ['å¤ªæ£’äº†', 'è¶…è®š', 'ç¥äº†', 'å®Œç¾', 'ç„¡æ•µ', 'å¤ªç¥äº†'],
            'positive': ['å¥½æ£’', 'èˆ’æœ', 'é–‹å¿ƒ', 'æœŸå¾…', 'çœ‹å¥½', 'å¼·å‹¢'],
            'neutral': ['æ³¨æ„', 'é—œæ³¨', 'æé†’', 'å»ºè­°', 'åˆ†æ', 'è§€å¯Ÿ'],
            'negative': ['çˆ›', 'æ…˜', 'æ­»', 'é¨™', 'ç‹ ', 'è·Œ'],
            'very_negative': ['çˆ›é€äº†', 'æ…˜å…®å…®', 'æ²’æ•‘', 'å®Œè›‹', 'å´©æ½°']
        }
        
        results = {}
        for emotion, keywords in emotions.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[emotion] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_interaction_patterns(self, titles):
        """åˆ†æäº’å‹•æ¨¡å¼"""
        patterns = {
            'question': ['?', 'ï¼Ÿ', 'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'è©²', 'å—'],
            'exclamation': ['!', 'ï¼', 'å•Š', 'å“‡', 'å–”'],
            'command': ['è«‹', 'æ³¨æ„', 'æé†’', 'å»ºè­°', 'å¿«'],
            'invitation': ['ä¸€èµ·', 'å¤§å®¶', 'æˆ‘å€‘', 'ä¾†', 'ä¾†å§']
        }
        
        results = {}
        for pattern_type, keywords in patterns.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[pattern_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_professional_terms(self, titles):
        """åˆ†æå°ˆæ¥­è¡“èª"""
        terms = {
            'technical': ['çªç ´', 'æ”¯æ’', 'å£“åŠ›', 'å‡ç·š', 'Kç·š', 'è¶¨å‹¢', 'æ´—ç›¤', 'åšé ­', 'èƒŒé›¢'],
            'fundamental': ['ç‡Ÿæ”¶', 'è²¡å ±', 'EPS', 'ç²åˆ©', 'æ¥­ç¸¾', 'è‚¡åˆ©', 'é…æ¯'],
            'sentiment': ['å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'ä¸»åŠ›', 'æ•£æˆ¶', 'èè³‡', 'èåˆ¸'],
            'news': ['æ–°è', 'å ±å°', 'å…¬å‘Š', 'åˆä½œ', 'æ“´ç”¢', 'å¸ƒå±€', 'é¡Œæ']
        }
        
        results = {}
        for term_type, keywords in terms.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[term_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_kol_patterns(self, titles):
        """åˆ†æ KOL æ¨¡å¼ï¼ˆç‚ºæœªä¾†æ“´å±•æº–å‚™ï¼‰"""
        # é€™è£¡å¯ä»¥åˆ†æä¸åŒ KOL çš„ç™¼æ–‡æ¨¡å¼
        # ç›®å‰å…ˆåˆ†æä¸€äº›é€šç”¨çš„ KOL ç‰¹å¾µ
        kol_patterns = {
            'personal_pronoun': ['æˆ‘çš„', 'è‡ªå·±', 'æˆ‘'],
            'experience_share': ['åˆ†äº«', 'ç¶“é©—', 'å¿ƒå¾—', 'æ“ä½œ'],
            'expert_opinion': ['åˆ†æ', 'çœ‹æ³•', 'è§€é»', 'å»ºè­°'],
            'community_interaction': ['å¤§å®¶', 'ä¸€èµ·', 'æˆ‘å€‘', 'ä¾†']
        }
        
        results = {}
        for pattern_type, keywords in kol_patterns.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[pattern_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_content_clustering(self, titles):
        """å…§å®¹èšé¡åˆ†æ"""
        # ç°¡å–®çš„å…§å®¹é¡å‹åˆ†é¡
        content_types = {
            'news_announcement': ['å…¬å‘Š', 'å¿«è¨Š', 'æ–°è', 'å ±å°'],
            'analysis_report': ['åˆ†æ', 'å ±å‘Š', 'ç ”ç©¶', 'è§€å¯Ÿ'],
            'trading_advice': ['å»ºè­°', 'æ“ä½œ', 'è²·é€²', 'è³£å‡º'],
            'market_sentiment': ['æƒ…ç·’', 'æ°›åœ', 'ç†±åº¦', 'ç˜‹ç‹‚'],
            'personal_share': ['åˆ†äº«', 'å¿ƒå¾—', 'ç¶“é©—', 'æˆ‘çš„'],
            'question_discussion': ['?', 'ï¼Ÿ', 'ä»€éº¼', 'å¦‚ä½•']
        }
        
        results = {}
        for content_type, keywords in content_types.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[content_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def run_comprehensive_analysis(self):
        """é‹è¡Œå…¨é¢åˆ†æ"""
        print("ğŸš€ é–‹å§‹æ™ºèƒ½ UGC æ•¸æ“šåˆ†æ")
        print("="*80)
        
        all_titles = []
        
        # åˆ†ææ¯å€‹æ–‡ä»¶
        for i, filepath in enumerate(self.data_files):
            if os.path.exists(filepath):
                print(f"\nğŸ“Š åˆ†ææ–‡ä»¶ {i+1}: {filepath}")
                titles = self.smart_read_large_file(filepath)
                all_titles.extend(titles)
                
                # å–®æ–‡ä»¶åˆ†æ
                file_results = {}
                for analysis_name, analysis_func in self.analysis_patterns.items():
                    try:
                        file_results[analysis_name] = analysis_func(titles)
                    except Exception as e:
                        print(f"   âš ï¸ {analysis_name} åˆ†æå¤±æ•—: {e}")
                
                self.results[f'file_{i+1}'] = {
                    'filepath': filepath,
                    'title_count': len(titles),
                    'analysis': file_results
                }
                
                # æ¸…ç†è¨˜æ†¶é«”
                del titles
                gc.collect()
        
        # åˆä½µåˆ†æ
        print(f"\nğŸ”„ åˆä½µåˆ†æ {len(all_titles)} å€‹æ¨™é¡Œ...")
        self.combined_stats = {}
        
        for analysis_name, analysis_func in self.analysis_patterns.items():
            try:
                self.combined_stats[analysis_name] = analysis_func(all_titles)
            except Exception as e:
                print(f"   âš ï¸ åˆä½µ {analysis_name} åˆ†æå¤±æ•—: {e}")
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_report()
        
        return self.results, self.combined_stats
    
    def generate_report(self):
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ™ºèƒ½åˆ†æå ±å‘Š")
        print("="*80)
        
        # 1. æ•¸æ“šæ¦‚è¦½
        total_titles = sum(result['title_count'] for result in self.results.values())
        print(f"ğŸ“Š æ•¸æ“šæ¦‚è¦½:")
        print(f"   ç¸½æ¨™é¡Œæ•¸: {total_titles:,}")
        print(f"   åˆ†ææ–‡ä»¶æ•¸: {len(self.results)}")
        
        # 2. é—œéµç™¼ç¾
        print(f"\nğŸ¯ é—œéµç™¼ç¾:")
        
        # é•·åº¦åˆ†å¸ƒ
        if 'length_distribution' in self.combined_stats:
            length_stats = self.combined_stats['length_distribution']
            print(f"   å¹³å‡æ¨™é¡Œé•·åº¦: {length_stats['avg_length']:.1f} å­—")
            print(f"   çŸ­æ¨™é¡Œæ¯”ä¾‹: {length_stats['distribution']['short']/length_stats['total_count']*100:.1f}%")
            print(f"   ä¸­æ¨™é¡Œæ¯”ä¾‹: {length_stats['distribution']['medium']/length_stats['total_count']*100:.1f}%")
            print(f"   é•·æ¨™é¡Œæ¯”ä¾‹: {length_stats['distribution']['long']/length_stats['total_count']*100:.1f}%")
        
        # è¡¨æƒ…ç¬¦è™Ÿ
        if 'emoji_analysis' in self.combined_stats:
            emoji_stats = self.combined_stats['emoji_analysis']
            print(f"   è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡: {emoji_stats['emoji_usage_rate']:.1f}%")
        
        # è‚¡ç¥¨ä»£è™Ÿ
        if 'stock_codes' in self.combined_stats:
            stock_stats = self.combined_stats['stock_codes']
            print(f"   è‚¡ç¥¨ä»£è™Ÿä½¿ç”¨ç‡: {stock_stats['stock_usage_rate']:.1f}%")
        
        # 3. AI æ¨™é¡Œç”Ÿæˆå»ºè­°
        print(f"\nğŸ’¡ AI æ¨™é¡Œç”Ÿæˆæ™ºèƒ½å»ºè­°:")
        
        # åŸºæ–¼æ•¸æ“šçš„å»ºè­°
        recommendations = self.generate_ai_recommendations()
        for i, rec in enumerate(recommendations, 1):
            print(f"   {i}. {rec}")
        
        # 4. KOL æ“´å±•å»ºè­°
        print(f"\nğŸ‘¥ KOL æ“´å±•å»ºè­°:")
        kol_recommendations = self.generate_kol_recommendations()
        for i, rec in enumerate(kol_recommendations, 1):
            print(f"   {i}. {rec}")
        
        # 5. ä¿å­˜è©³ç´°çµæœ
        self.save_detailed_results()
    
    def generate_ai_recommendations(self):
        """åŸºæ–¼åˆ†æç”Ÿæˆ AI å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼é•·åº¦åˆ†å¸ƒ
        if 'length_distribution' in self.combined_stats:
            length_stats = self.combined_stats['length_distribution']
            medium_rate = length_stats['distribution']['medium'] / length_stats['total_count'] * 100
            recommendations.append(f"é‡é»ç”Ÿæˆ {medium_rate:.1f}% çš„ 11-20 å­—ä¸­ç­‰é•·åº¦æ¨™é¡Œ")
        
        # åŸºæ–¼è¡¨æƒ…ç¬¦è™Ÿ
        if 'emoji_analysis' in self.combined_stats:
            emoji_stats = self.combined_stats['emoji_analysis']
            recommendations.append(f"é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç›®æ¨™ä½¿ç”¨ç‡ {emoji_stats['emoji_usage_rate']:.1f}%")
        
        # åŸºæ–¼äº’å‹•æ¨¡å¼
        if 'interaction_patterns' in self.combined_stats:
            interaction_stats = self.combined_stats['interaction_patterns']
            exclamation_rate = interaction_stats['exclamation']['rate']
            question_rate = interaction_stats['question']['rate']
            recommendations.append(f"å¢åŠ äº’å‹•æ€§ï¼šæ„Ÿå˜†å¥ {exclamation_rate:.1f}%ï¼Œå•å¥ {question_rate:.1f}%")
        
        # åŸºæ–¼å°ˆæ¥­è¡“èª
        if 'professional_terms' in self.combined_stats:
            professional_stats = self.combined_stats['professional_terms']
            fundamental_rate = professional_stats['fundamental']['rate']
            technical_rate = professional_stats['technical']['rate']
            recommendations.append(f"èå…¥å°ˆæ¥­è¡“èªï¼šåŸºæœ¬é¢ {fundamental_rate:.1f}%ï¼ŒæŠ€è¡“é¢ {technical_rate:.1f}%")
        
        return recommendations
    
    def generate_kol_recommendations(self):
        """ç”Ÿæˆ KOL æ“´å±•å»ºè­°"""
        recommendations = [
            "åŸºæ–¼æƒ…æ„Ÿå¼·åº¦åˆ†å¸ƒè¨­è¨ˆä¸åŒ KOL æ€§æ ¼",
            "æ ¹æ“šäº’å‹•æ¨¡å¼å·®ç•°åŒ– KOL è¡¨é”æ–¹å¼",
            "åˆ©ç”¨å°ˆæ¥­è¡“èªåå¥½å€åˆ† KOL å°ˆæ¥­é ˜åŸŸ",
            "çµåˆç†±é–€è©±é¡Œè¨­è¨ˆ KOL å°ˆé•·é ˜åŸŸ",
            "åƒè€ƒè¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç¿’æ…£è¨­è¨ˆ KOL é¢¨æ ¼"
        ]
        
        return recommendations
    
    def save_detailed_results(self):
        """ä¿å­˜è©³ç´°åˆ†æçµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ugc_analysis_results_{timestamp}.json"
        
        output_data = {
            'analysis_timestamp': timestamp,
            'files_analyzed': self.results,
            'combined_statistics': self.combined_stats
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜è‡³: {filename}")

def main():
    """ä¸»å‡½æ•¸"""
    analyzer = SmartUGCAnalyzer()
    results, combined_stats = analyzer.run_comprehensive_analysis()
    
    print("\nâœ… æ™ºèƒ½ UGC åˆ†æå®Œæˆï¼")
    return results, combined_stats

if __name__ == "__main__":
    main()
