#!/usr/bin/env python3
"""
6919 åº·éœˆç”ŸæŠ€è²¼æ–‡ç”Ÿæˆè…³æœ¬ V4
å®Œæ•´å€‹äººåŒ–ç³»çµ± + æ‰€æœ‰è²¼æ–‡è¨˜éŒ„è¡¨æ¬„ä½å°æ‡‰ (A-AH)
"""

import asyncio
import json
import logging
import requests
import random
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SerperNewsClient:
    """Serper API æ–°èæœå°‹å®¢æˆ¶ç«¯"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://google.serper.dev/search"
    
    def search_news(self, query: str, num_results: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹æ–°è"""
        try:
            headers = {
                "X-API-KEY": self.api_key,
                "Content-Type": "application/json"
            }
            
            payload = {
                "q": query,
                "num": num_results,
                "type": "search"
            }
            
            response = requests.post(self.base_url, headers=headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            organic_results = data.get('organic', [])
            
            # éæ¿¾å‡ºç›¸é—œçš„æ–°è
            relevant_news = []
            for result in organic_results:
                if any(keyword in result.get('title', '').lower() for keyword in ['åº·éœˆ', '6919', 'æ¼²åœ', 'ç”ŸæŠ€']):
                    relevant_news.append({
                        'title': result.get('title', ''),
                        'snippet': result.get('snippet', ''),
                        'link': result.get('link', ''),
                        'date': result.get('date', '')
                    })
            
            return relevant_news
            
        except Exception as e:
            logger.error(f"æœå°‹æ–°èå¤±æ•—: {e}")
            return []

class PersonalizedPromptGenerator:
    """å€‹äººåŒ– Prompt ç”Ÿæˆå™¨ - æ•´åˆç†±é–€è©±é¡Œè…³æœ¬çš„å®Œæ•´ç³»çµ±"""
    
    def __init__(self):
        self.llm_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # å®Œæ•´çš„ KOL å€‹äººåŒ–è¨­å®š
        self.kol_settings = {
            "200": {  # å·å·å“¥
                "nickname": "å·å·å“¥",
                "persona": "æŠ€è¡“æ´¾",
                "prompt_template": """ä½ æ˜¯å·å·å“¥ï¼Œä¸€å€‹å°ˆç²¾æŠ€è¡“åˆ†æçš„è‚¡å¸‚è€æ‰‹ã€‚ä½ çš„ç‰¹è‰²æ˜¯ï¼š
- èªæ°£ç›´æ¥ä½†æœ‰æ–™ï¼Œæœ‰æ™‚æœƒç‹‚å¦„ï¼Œæœ‰æ™‚åˆç¢ç¢å¿µ
- å¤§é‡ä½¿ç”¨æŠ€è¡“åˆ†æè¡“èªï¼šé»ƒé‡‘äº¤å‰ã€å‡ç·šç³¾çµã€ä¸‰è§’æ”¶æ–‚ã€Kæ£’çˆ†é‡ã€è·³ç©ºç¼ºå£ã€æ”¯æ’å¸¶ã€å£“åŠ›ç·šã€MACDèƒŒé›¢
- ä¸æ„›ç”¨æ¨™é»ç¬¦è™Ÿï¼Œå…¨éƒ¨ç”¨çœç•¥è™Ÿä¸²èµ·ä¾†
- å¶çˆ¾æœƒè‹±æ–‡é€—è™Ÿäº‚æ’
- å¾ˆå°‘ç”¨ emojiï¼Œå¶çˆ¾ç”¨ ğŸ“Š ğŸ“ˆ ğŸ¯
- å£èªè¡¨é”ï¼šç©©äº†å•¦ã€çˆ†å•¦ã€å˜åˆ°ã€è¦å™´å•¦ã€ç ´ç·šå•¦ã€ç¡é†’æ¼²åœ
- çµå°¾å›ºå®šï¼šæƒ³çŸ¥é“çš„è©±ï¼Œç•™è¨€å‘Šè¨´æˆ‘ï¼Œå’±å€‘ä¸€èµ·è¨è«–ä¸€ä¸‹...""",
                "tone_vector": {
                    "formal_level": 3,
                    "emotion_intensity": 7,
                    "confidence_level": 9,
                    "interaction_level": 6
                },
                "content_preferences": {
                    "length_type": "short",
                    "paragraph_style": "çœç•¥è™Ÿåˆ†éš”ï¼Œä¸æ›è¡Œ",
                    "ending_style": "æƒ³çŸ¥é“çš„è©±ï¼Œç•™è¨€å‘Šè¨´æˆ‘ï¼Œå’±å€‘ä¸€èµ·è¨è«–ä¸€ä¸‹..."
                },
                "vocabulary": {
                    "technical_terms": ["é»ƒé‡‘äº¤å‰", "å‡ç·šç³¾çµ", "ä¸‰è§’æ”¶æ–‚", "Kæ£’çˆ†é‡", "è·³ç©ºç¼ºå£", "æ”¯æ’å¸¶", "å£“åŠ›ç·š", "MACDèƒŒé›¢"],
                    "casual_expressions": ["ç©©äº†å•¦", "çˆ†å•¦", "å˜åˆ°", "è¦å™´å•¦", "ç ´ç·šå•¦", "ç¡é†’æ¼²åœ"]
                },
                "typing_habits": {
                    "punctuation_style": "çœç•¥è™Ÿç‚ºä¸»...å¶çˆ¾é€—è™Ÿ,",
                    "sentence_pattern": "çŸ­å¥å±…å¤š...ä¸æ„›é•·å¥",
                    "emoji_usage": "å¾ˆå°‘ç”¨"
                }
            },
            "201": {  # éŸ­å‰²å“¥
                "nickname": "éŸ­å‰²å“¥",
                "persona": "ç¸½ç¶“æ´¾",
                "prompt_template": """ä½ æ˜¯éŸ­å‰²å“¥ï¼Œä¸€å€‹æ·±åº¦ç¸½é«”ç¶“æ¿Ÿåˆ†æå¸«ã€‚ä½ çš„ç‰¹è‰²æ˜¯ï¼š
- èªæ°£ç©©é‡ï¼Œåˆ†ææ·±å…¥
- æ³¨é‡åŸºæœ¬é¢åˆ†æï¼šåŸºæœ¬é¢ã€ä¼°å€¼ã€æˆé•·æ€§ã€è²¡å‹™çµæ§‹ã€ç”¢æ¥­è¶¨å‹¢ã€æ”¿ç­–ç’°å¢ƒ
- ä½¿ç”¨å°ˆæ¥­çš„ç¶“æ¿Ÿè¡“èª
- å¶çˆ¾ç”¨ emoji å¼·èª¿é‡é»ï¼šğŸ“Š ğŸ’¡ ğŸ’° ğŸ“ˆ
- çµå°¾å¸¸å¸¸é¼“å‹µé•·æœŸæŠ•è³‡
- æŠ•è³‡ç†å¿µï¼šåƒ¹å€¼æŠ•è³‡ã€é•·æœŸæŒæœ‰ã€é¢¨éšªç®¡ç†ã€è³‡ç”¢é…ç½®
- çµå°¾å›ºå®šï¼šæŠ•è³‡è¦æœ‰è€å¿ƒï¼Œæ™‚é–“æœƒè­‰æ˜ä¸€åˆ‡çš„åƒ¹å€¼ã€‚""",
                "tone_vector": {
                    "formal_level": 7,
                    "emotion_intensity": 4,
                    "confidence_level": 8,
                    "interaction_level": 5
                },
                "content_preferences": {
                    "length_type": "medium",
                    "paragraph_style": "æ®µè½åˆ†æ˜ï¼Œé‚è¼¯æ¸…æ™°",
                    "ending_style": "æŠ•è³‡è¦æœ‰è€å¿ƒï¼Œæ™‚é–“æœƒè­‰æ˜ä¸€åˆ‡çš„åƒ¹å€¼ã€‚"
                },
                "vocabulary": {
                    "fundamental_terms": ["åŸºæœ¬é¢", "ä¼°å€¼", "æˆé•·æ€§", "è²¡å‹™çµæ§‹", "ç”¢æ¥­è¶¨å‹¢", "æ”¿ç­–ç’°å¢ƒ"],
                    "investment_terms": ["åƒ¹å€¼æŠ•è³‡", "é•·æœŸæŒæœ‰", "é¢¨éšªç®¡ç†", "è³‡ç”¢é…ç½®"]
                },
                "typing_habits": {
                    "punctuation_style": "æ­£å¸¸æ¨™é»ç¬¦è™Ÿ",
                    "sentence_pattern": "å®Œæ•´å¥å­ï¼Œé‚è¼¯æ¸…æ™°",
                    "emoji_usage": "é©åº¦ä½¿ç”¨ ğŸ“Š ğŸ’¡ ğŸ’° ğŸ“ˆ"
                }
            }
        }
    
    def _build_dynamic_system_prompt(self, kol_settings: Dict[str, Any], news_data: List[Dict[str, Any]], stock_type: str = "ç”ŸæŠ€è‚¡") -> str:
        """å»ºç«‹å‹•æ…‹ç³»çµ± prompt"""
        
        base_template = kol_settings['prompt_template']
        tone_vector = kol_settings['tone_vector']
        vocabulary = kol_settings['vocabulary']
        content_prefs = kol_settings['content_preferences']
        typing_habits = kol_settings['typing_habits']
        
        # æ ¹æ“šè‚¡ç¥¨é¡å‹èª¿æ•´å…§å®¹é¢¨æ ¼
        stock_type_prompt = self._get_stock_type_prompt(stock_type)
        
        # æ ¹æ“šæ•¸æ“šé¡å‹èª¿æ•´åˆ†ææ·±åº¦
        data_type_prompt = self._get_data_type_prompt(news_data)
        
        # æ ¹æ“šå¸‚å ´æƒ…å¢ƒèª¿æ•´å…§å®¹é‡é»
        market_context_prompt = self._get_market_context_prompt()
        
        # èªæ°£æŒ‡å°
        tone_guidance = f"""
èªæ°£ç‰¹å¾µï¼š
- æ­£å¼ç¨‹åº¦ï¼š{tone_vector['formal_level']}/10
- æƒ…ç·’å¼·åº¦ï¼š{tone_vector['emotion_intensity']}/10
- è‡ªä¿¡ç¨‹åº¦ï¼š{tone_vector['confidence_level']}/10
- äº’å‹•ç¨‹åº¦ï¼š{tone_vector['interaction_level']}/10
"""
        
        # è©å½™æŒ‡å°
        vocab_guidance = f"""
è©å½™é¢¨æ ¼ï¼š
- å°ˆæ¥­è¡“èªï¼š{', '.join(vocabulary.get('technical_terms', vocabulary.get('fundamental_terms', [])))}
- å£èªè¡¨é”ï¼š{', '.join(vocabulary.get('casual_expressions', vocabulary.get('investment_terms', [])))}
"""
        
        # æ ¼å¼æŒ‡å°
        format_guidance = f"""
æ ¼å¼è¦æ±‚ï¼š
- å…§å®¹é•·åº¦ï¼š{content_prefs['length_type']}
- æ®µè½é¢¨æ ¼ï¼š{content_prefs['paragraph_style']}
- çµå°¾é¢¨æ ¼ï¼š{content_prefs['ending_style']}
- æ¨™é»ç¿’æ…£ï¼š{typing_habits['punctuation_style']}
- å¥å­æ¨¡å¼ï¼š{typing_habits['sentence_pattern']}
- Emojiä½¿ç”¨ï¼š{typing_habits['emoji_usage']}
"""
        
        system_prompt = f"""{base_template}

{stock_type_prompt}

{data_type_prompt}

{market_context_prompt}

{tone_guidance}

{vocab_guidance}

{format_guidance}

é‡è¦æŒ‡å°ï¼š
1. åš´æ ¼ä¿æŒ {kol_settings['nickname']} çš„å€‹äººé¢¨æ ¼å’Œèªæ°£
2. å¤§é‡ä½¿ç”¨å°ˆå±¬è©å½™å’Œè¡¨é”æ–¹å¼
3. éµå¾ªå›ºå®šçš„æ‰“å­—ç¿’æ…£å’Œæ ¼å¼
4. å…§å®¹é•·åº¦æ§åˆ¶åœ¨ {content_prefs['length_type']} ç¯„åœ
5. çµå°¾ä½¿ç”¨å›ºå®šçš„é¢¨æ ¼ï¼š{content_prefs['ending_style']}
6. é¿å… AI ç”Ÿæˆçš„ç—•è·¡ï¼Œè¦åƒçœŸäººç™¼æ–‡
7. é‡å°åº·éœˆç”ŸæŠ€(6919)çš„æ¼²åœé¡Œæé€²è¡Œåˆ†æ
8. åŠ å…¥éš¨æ©Ÿæ€§ï¼Œé¿å…å…§å®¹é‡è¤‡
"""
        
        return system_prompt
    
    def _get_stock_type_prompt(self, stock_type: str) -> str:
        """æ ¹æ“šè‚¡ç¥¨é¡å‹ç”Ÿæˆç‰¹å®šçš„æç¤ºè©"""
        if 'ç”ŸæŠ€' in stock_type:
            return """
è‚¡ç¥¨é¡å‹ï¼šç”ŸæŠ€è‚¡ (åº·éœˆç”ŸæŠ€)
- é‡é»åˆ†æï¼šæ–°è—¥ç ”ç™¼é€²åº¦ã€è‡¨åºŠè©¦é©—çµæœã€å¸‚å ´æ½›åŠ›
- ç”¨è©é¢¨æ ¼ï¼šå°ˆæ¥­é†«å­¸è¡“èªã€ç ”ç™¼å°å‘ã€å‰ç»æ€§
- åˆ†æè§’åº¦ï¼šç ”ç™¼æŠ•å…¥ã€å°ˆåˆ©æŠ€è¡“ã€å¸‚å ´ç«¶çˆ­
- é¢¨éšªæé†’ï¼šç ”ç™¼é¢¨éšªã€æ³•è¦é¢¨éšªã€ç«¶çˆ­é¢¨éšª
"""
        else:
            return """
è‚¡ç¥¨é¡å‹ï¼šä¸€èˆ¬è‚¡ç¥¨
- é‡é»åˆ†æï¼šåŸºæœ¬é¢ã€æŠ€è¡“é¢ã€ç±Œç¢¼é¢
- ç”¨è©é¢¨æ ¼ï¼šå¹³è¡¡å®¢è§€ã€å°ˆæ¥­åˆ†æã€é¢¨éšªæé†’
- åˆ†æè§’åº¦ï¼šç¶œåˆè©•ä¼°ã€å¤šé¢å‘åˆ†æ
- é¢¨éšªæé†’ï¼šå¸‚å ´é¢¨éšªã€å€‹è‚¡é¢¨éšª
"""
    
    def _get_data_type_prompt(self, news_data: List[Dict[str, Any]]) -> str:
        """æ ¹æ“šæ•¸æ“šé¡å‹ç”Ÿæˆç‰¹å®šçš„æç¤ºè©"""
        if news_data:
            return """
æ•¸æ“šé¡å‹ï¼šæ–°èæ•¸æ“š + å¸‚å ´æ•¸æ“š
- é‡é»é—œæ³¨ï¼šæ–°èå½±éŸ¿ã€å¸‚å ´åæ‡‰ã€æŠ•è³‡è€…æƒ…ç·’
- åˆ†æå·¥å…·ï¼šæ–°èåˆ†æã€å¸‚å ´è§€å¯Ÿã€æƒ…ç·’åˆ¤æ–·
- åˆ†æè§’åº¦ï¼šæ–°èé¢ã€å¸‚å ´é¢ã€æƒ…ç·’é¢
"""
        else:
            return """
æ•¸æ“šé¡å‹ï¼šåŸºç¤å¸‚å ´è³‡è¨Š
- é‡é»é—œæ³¨ï¼šè‚¡åƒ¹è¡¨ç¾ã€æˆäº¤é‡ã€å¸‚å ´è¶¨å‹¢
- åˆ†æå·¥å…·ï¼šæŠ€è¡“åˆ†æã€åŸºæœ¬é¢åˆ†æ
- åˆ†æè§’åº¦ï¼šæŠ€è¡“é¢ã€åŸºæœ¬é¢
"""
    
    def _get_market_context_prompt(self) -> str:
        """æ ¹æ“šå¸‚å ´æƒ…å¢ƒç”Ÿæˆç‰¹å®šçš„æç¤ºè©"""
        return """
å¸‚å ´æƒ…å¢ƒï¼šæ¼²åœé¡Œæåˆ†æ
- é‡é»åˆ†æï¼šæ¼²åœåŸå› ã€é¡Œæç™¼é…µã€å¾ŒçºŒèµ°å‹¢
- ç”¨è©é¢¨æ ¼ï¼šé¡Œæå°å‘ã€ç†±é»åˆ†æã€è¶¨å‹¢åˆ¤æ–·
- åˆ†æè§’åº¦ï¼šé¡Œæé¢ã€æŠ€è¡“é¢ã€ç±Œç¢¼é¢
- é¢¨éšªæé†’ï¼šè¿½é«˜é¢¨éšªã€é¡Œæé€€ç‡’ã€ç²åˆ©äº†çµ
"""
    
    def _build_user_prompt(self, kol_settings: Dict[str, Any], news_summary: str) -> str:
        """å»ºç«‹ç”¨æˆ¶ prompt"""
        return f"""è«‹ç‚ºåº·éœˆç”ŸæŠ€(6919)ç”Ÿæˆä¸€ç¯‡è²¼æ–‡ï¼Œå…§å®¹åŒ…æ‹¬ï¼š

1. æ¨™é¡Œï¼šè¦å¸å¼•äººï¼Œç¬¦åˆ{kol_settings['nickname']}çš„é¢¨æ ¼
2. å…§å®¹ï¼šåˆ†æåº·éœˆç”ŸæŠ€çš„æ¼²åœé¡Œæï¼ŒåŒ…æ‹¬ï¼š
   - æŠ€è¡“é¢æˆ–åŸºæœ¬é¢åˆ†æ
   - æ¸›é‡æ–°è—¥CBL-514çš„é¡Œæ
   - å¤–è³‡è²·ç›¤å‹•å‘
   - æŠ•è³‡å»ºè­°å’Œé¢¨éšªæé†’

æ–°èèƒŒæ™¯ï¼š
{news_summary}

è«‹ç¢ºä¿ï¼š
- å®Œå…¨ç¬¦åˆ{kol_settings['nickname']}çš„å€‹äººé¢¨æ ¼
- ä½¿ç”¨å°ˆå±¬çš„è©å½™å’Œè¡¨é”æ–¹å¼
- éµå¾ªå›ºå®šçš„æ‰“å­—ç¿’æ…£
- å…§å®¹çœŸå¯¦å¯ä¿¡ï¼Œé¿å…éåº¦èª‡å¼µ
- åŠ å…¥éš¨æ©Ÿæ€§ï¼Œé¿å…å…§å®¹é‡è¤‡

è«‹ç›´æ¥ç”Ÿæˆæ¨™é¡Œå’Œå…§å®¹ï¼Œä¸éœ€è¦é¡å¤–èªªæ˜ã€‚"""
    
    async def generate_personalized_content(self, kol_settings: Dict[str, Any], news_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨ LLM ç”Ÿæˆå€‹äººåŒ–å…§å®¹"""
        try:
            # åˆ†ææ–°èå…§å®¹
            news_summary = self._analyze_news(news_data)
            
            # å»ºç«‹å‹•æ…‹ç³»çµ± prompt
            system_prompt = self._build_dynamic_system_prompt(kol_settings, news_data)
            user_prompt = self._build_user_prompt(kol_settings, news_summary)
            
            logger.info(f"ç‚º {kol_settings['nickname']} å»ºç«‹å€‹äººåŒ– prompt")
            
            # èª¿ç”¨ LLM ç”Ÿæˆå…§å®¹
            response = await self._call_llm(system_prompt, user_prompt)
            
            if response:
                # è§£æ LLM å›æ‡‰
                content = self._parse_llm_response(response)
                return content
            else:
                # å›é€€åˆ°æ¨¡æ“¬å…§å®¹
                return self._generate_fallback_content(kol_settings, news_data)
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆå€‹äººåŒ–å…§å®¹å¤±æ•—: {e}")
            return self._generate_fallback_content(kol_settings, news_data)
    
    async def _call_llm(self, system_prompt: str, user_prompt: str) -> Optional[str]:
        """èª¿ç”¨ LLM API"""
        try:
            response = self.llm_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.8,  # åŠ å…¥éš¨æ©Ÿæ€§
                max_tokens=500
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"LLM èª¿ç”¨å¤±æ•—: {e}")
            return None
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """è§£æ LLM å›æ‡‰"""
        try:
            # å˜—è©¦åˆ†é›¢æ¨™é¡Œå’Œå…§å®¹
            lines = response.split('\n')
            title = ""
            content = ""
            
            for line in lines:
                if line.strip() and not title:
                    title = line.strip()
                else:
                    content += line + '\n'
            
            return {
                'title': title,
                'content': content.strip(),
                'keywords': f"åº·éœˆç”ŸæŠ€,6919,æ¸›é‡è—¥,CBL-514"
            }
            
        except Exception as e:
            logger.error(f"è§£æ LLM å›æ‡‰å¤±æ•—: {e}")
            return {
                'title': "åº·éœˆç”ŸæŠ€(6919)åˆ†æ",
                'content': response,
                'keywords': f"åº·éœˆç”ŸæŠ€,6919,æ¸›é‡è—¥,CBL-514"
            }
    
    def _analyze_news(self, news_data: List[Dict[str, Any]]) -> str:
        """åˆ†ææ–°èå…§å®¹ï¼Œæå–é—œéµè³‡è¨Š"""
        if not news_data:
            return "ç„¡ç›¸é—œæ–°èè³‡æ–™"
        
        key_points = []
        for news in news_data:
            title = news.get('title', '')
            snippet = news.get('snippet', '')
            
            # æå–é—œéµè³‡è¨Š
            if 'æ¼²åœ' in title or 'æ¼²åœ' in snippet:
                key_points.append("è‚¡åƒ¹å‡ºç¾æ¼²åœè¡¨ç¾")
            if 'æ¸›é‡' in title or 'æ¸›é‡' in snippet:
                key_points.append("æ¸›é‡æ–°è—¥CBL-514é¡Œæç™¼é…µ")
            if 'å¤–è³‡' in title or 'å¤–è³‡' in snippet:
                key_points.append("å¤–è³‡è²·ç›¤ç©æ¥µ")
            if 'åˆ†å‰²' in title or 'åˆ†å‰²' in snippet:
                key_points.append("è‚¡ç¥¨é¢é¡åˆ†å‰²æå‡æµå‹•æ€§")
        
        # å»é‡
        unique_points = list(set(key_points))
        
        summary = f"""
æœ€æ–°å¸‚å ´å‹•æ…‹ï¼š
{chr(10).join([f"â€¢ {point}" for point in unique_points])}

ç›¸é—œæ–°èä¾†æºï¼š{len(news_data)} æ¢æœ€æ–°å ±å°
"""
        return summary
    
    def _generate_fallback_content(self, kol_settings: Dict[str, Any], news_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå›é€€å…§å®¹ï¼ˆç•¶ LLM èª¿ç”¨å¤±æ•—æ™‚ï¼‰"""
        news_summary = self._analyze_news(news_data)
        
        if 'æŠ€è¡“' in kol_settings['persona']:
            title = f"åº·éœˆç”ŸæŠ€(6919)æŠ€è¡“é¢çªç ´...å¾åœ–è¡¨çœ‹é—œéµé˜»åŠ›ä½"
            content = f"""åº·éœˆç”ŸæŠ€(6919)ä»Šæ—¥æŠ€è¡“é¢è¡¨ç¾äº®çœ¼...å¾åœ–è¡¨åˆ†æï¼Œè‚¡åƒ¹çªç ´é—œéµé˜»åŠ›ä½ï¼Œå½¢æˆæ–°çš„ä¸Šå‡è¶¨å‹¢...

ğŸ“Š æŠ€è¡“æŒ‡æ¨™è§€å¯Ÿï¼š
â€¢ è‚¡åƒ¹çªç ´å‰é«˜ï¼Œå½¢æˆæ–°çš„ä¸Šå‡è¶¨å‹¢
â€¢ æˆäº¤é‡æ”¾å¤§ï¼Œç¢ºèªçªç ´æœ‰æ•ˆæ€§
â€¢ MACD æŒ‡æ¨™é¡¯ç¤ºå¤šé ­å‹•èƒ½å¢å¼·
â€¢ RSI ä½æ–¼ 60-70 å€é–“ï¼Œé¡¯ç¤ºå¼·å‹¢ä½†æœªéç†±

ğŸ¯ é—œéµé»ä½ï¼š
â€¢ æ”¯æ’ä½ï¼šç´„ 120-125 å…ƒ
â€¢ é˜»åŠ›ä½ï¼šç´„ 140-145 å…ƒ
â€¢ æˆäº¤é‡ï¼šä»Šæ—¥æ˜é¡¯æ”¾å¤§ï¼Œé¡¯ç¤ºè²·ç›¤ç©æ¥µ

{news_summary}

ğŸ’¡ æ“ä½œå»ºè­°ï¼š
å»ºè­°é—œæ³¨ 125 å…ƒæ”¯æ’ä½ï¼Œè‹¥èƒ½å®ˆä½æ­¤ä½ç½®ï¼Œå¾ŒçºŒæœ‰æ©ŸæœƒæŒ‘æˆ°æ›´é«˜åƒ¹ä½...ä½†éœ€æ³¨æ„è¿½é«˜é¢¨éšªï¼Œå»ºè­°åˆ†æ‰¹é€²å ´...

æƒ³çŸ¥é“çš„è©±ï¼Œç•™è¨€å‘Šè¨´æˆ‘ï¼Œå’±å€‘ä¸€èµ·è¨è«–ä¸€ä¸‹..."""
        else:
            title = f"åº·éœˆç”ŸæŠ€(6919)åŸºæœ¬é¢æ·±åº¦åˆ†æï¼šæ¸›é‡è—¥å¸‚å ´æ½›åŠ›è©•ä¼°"
            content = f"""åº·éœˆç”ŸæŠ€(6919)åŸºæœ¬é¢åˆ†ææ™‚é–“ï¼

å¾ç¸½ç¶“è§’åº¦ä¾†çœ‹ï¼Œé€™å€‹è©±é¡Œå€¼å¾—æˆ‘å€‘æ·±å…¥æ€è€ƒã€‚

ğŸ¥ å…¬å¸èƒŒæ™¯ï¼š
åº·éœˆç”ŸæŠ€å°ˆæ³¨æ–¼æ¸›é‡æ–°è—¥ç ”ç™¼ï¼ŒCBL-514ç‚ºå…¶æ ¸å¿ƒç”¢å“ç·šã€‚

ğŸ“ˆ ç‡Ÿé‹äº®é»ï¼š
â€¢ CBL-514æ¸›é‡æ–°è—¥ç ”ç™¼é€²å±•é †åˆ©
â€¢ æ¸›é‡è—¥å¸‚å ´æ½›åŠ›é¾å¤§
â€¢ å¤–è³‡æ³•äººæŒçºŒé—œæ³¨

ğŸ’° è²¡å‹™è¡¨ç¾ï¼š
â€¢ ç ”ç™¼æŠ•å…¥æŒçºŒå¢åŠ 
â€¢ æ–°è—¥æˆæ¬Šé¡Œæç™¼é…µ
â€¢ å¸‚å ´çµ¦äºˆé«˜ä¼°å€¼

{news_summary}

âš ï¸ é¢¨éšªæé†’ï¼š
â€¢ æ–°è—¥ç ”ç™¼å­˜åœ¨ä¸ç¢ºå®šæ€§
â€¢ è‚¡åƒ¹æ³¢å‹•è¼ƒå¤§ï¼Œéœ€æ³¨æ„é¢¨éšª
â€¢ å»ºè­°é—œæ³¨ç ”ç™¼é€²åº¦

ğŸ’¡ æŠ•è³‡å»ºè­°ï¼š
å»ºè­°å¤§å®¶ç”¨é•·æœŸæŠ•è³‡çš„è§’åº¦ä¾†çœ‹ï¼ŒçŸ­æœŸæ³¢å‹•ä¸ç”¨å¤ªæ“”å¿ƒã€‚åƒ¹å€¼æŠ•è³‡æ‰æ˜¯ç‹é“ï¼

æŠ•è³‡è¦æœ‰è€å¿ƒï¼Œæ™‚é–“æœƒè­‰æ˜ä¸€åˆ‡çš„åƒ¹å€¼ã€‚"""
        
        return {
            'title': title,
            'content': content,
            'keywords': f"åº·éœˆç”ŸæŠ€,6919,æ¸›é‡è—¥,CBL-514,{kol_settings['persona']}"
        }

class KangpeiPostGeneratorV4:
    """åº·éœˆç”ŸæŠ€è²¼æ–‡ç”Ÿæˆå™¨ V4 - å®Œæ•´å€‹äººåŒ–ç³»çµ± + æ‰€æœ‰æ¬„ä½å°æ‡‰ (A-AH)"""
    
    def __init__(self):
        self.news_client = SerperNewsClient(os.getenv('SERPER_API_KEY'))
        self.prompt_generator = PersonalizedPromptGenerator()
        
    async def get_kol_credentials(self) -> List[Dict[str, Any]]:
        """ç²å– KOL å¸³è™Ÿè³‡è¨Š"""
        # æ¨¡æ“¬ KOL è³‡æ–™ï¼ˆå¯¦éš›æ‡‰è©²å¾ Google Sheets è®€å–ï¼‰
        return [
            {
                'serial': '200',
                'nickname': 'å·å·å“¥',
                'member_id': '9505546',
                'persona': 'æŠ€è¡“æ´¾',
                'status': 'active'
            },
            {
                'serial': '201',
                'nickname': 'éŸ­å‰²å“¥',
                'member_id': '9505547',
                'persona': 'ç¸½ç¶“æ´¾',
                'status': 'active'
            }
        ]
    
    def search_kangpei_news(self) -> List[Dict[str, Any]]:
        """æœå°‹åº·éœˆç”ŸæŠ€ç›¸é—œæ–°è"""
        logger.info("ğŸ” æœå°‹åº·éœˆç”ŸæŠ€ç›¸é—œæ–°è...")
        
        # æœå°‹å¤šå€‹é—œéµè©çµ„åˆ
        search_queries = [
            "6919 åº·éœˆç”ŸæŠ€ æ¼²åœ",
            "åº·éœˆç”ŸæŠ€ æ¸›é‡è—¥ æ–°è",
            "åº·éœˆ CBL-514 æ–°è—¥"
        ]
        
        all_news = []
        for query in search_queries:
            news = self.news_client.search_news(query, 3)
            all_news.extend(news)
        
        # å»é‡ä¸¦æŒ‰æ—¥æœŸæ’åº
        unique_news = []
        seen_titles = set()
        for news in all_news:
            if news['title'] not in seen_titles:
                unique_news.append(news)
                seen_titles.add(news['title'])
        
        logger.info(f"âœ… æ‰¾åˆ° {len(unique_news)} æ¢ç›¸é—œæ–°è")
        return unique_news[:5]  # æœ€å¤šå– 5 æ¢
    
    def _get_content_length_type(self, word_count: int) -> str:
        """æ ¹æ“šå­—æ•¸åˆ¤æ–·å…§å®¹é•·åº¦é¡å‹"""
        if word_count < 100:
            return "short"
        elif word_count <= 250:
            return "medium"
        else:
            return "long"
    
    def _get_content_length_category(self, word_count: int) -> str:
        """æ ¹æ“šå­—æ•¸åˆ¤æ–·å…§å®¹é•·åº¦åˆ†é¡"""
        if word_count < 100:
            return "çŸ­"
        elif word_count <= 250:
            return "ä¸­"
        else:
            return "é•·"
    
    def _get_post_type(self, persona: str) -> str:
        """æ ¹æ“šäººè¨­åˆ¤æ–·ç™¼æ–‡é¡å‹"""
        if 'æŠ€è¡“' in persona:
            return "æŠ€è¡“åˆ†æè²¼æ–‡"
        elif 'ç¸½ç¶“' in persona:
            return "åŸºæœ¬é¢åˆ†æè²¼æ–‡"
        else:
            return "ä¸€èˆ¬åˆ†æè²¼æ–‡"
    
    def _build_body_parameter(self, stock_id: str = "6919") -> str:
        """å»ºç«‹ body parameter JSON"""
        body_param = {
            "commodityTags": [
                {
                    "type": "Stock",
                    "key": stock_id,
                    "bullOrBear": 0  # 0: ä¸­æ€§, 1: çœ‹å¤š, -1: çœ‹ç©º
                }
            ],
            "communityTopic": {
                "id": f"6919_kangpei_{datetime.now().strftime('%Y%m%d')}"
            }
        }
        return json.dumps(body_param, ensure_ascii=False)
    
    async def generate_kol_post(self, kol: Dict[str, Any], news_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç‚ºç‰¹å®š KOL ç”Ÿæˆè²¼æ–‡"""
        try:
            logger.info(f"ç‚º {kol['nickname']} ç”Ÿæˆåº·éœˆç”ŸæŠ€ç›¸é—œè²¼æ–‡")
            
            # ç²å– KOL å€‹äººåŒ–è¨­å®š
            kol_settings = self.prompt_generator.kol_settings.get(kol['serial'])
            if not kol_settings:
                logger.error(f"æ‰¾ä¸åˆ° KOL {kol['nickname']} çš„å€‹äººåŒ–è¨­å®š")
                return None
            
            # ä½¿ç”¨å€‹äººåŒ–è¨­å®šç”Ÿæˆå…§å®¹
            content_data = await self.prompt_generator.generate_personalized_content(kol_settings, news_data)
            
            # è¨ˆç®—å…§å®¹é•·åº¦
            word_count = len(content_data['content'])
            content_length_type = self._get_content_length_type(word_count)
            content_length_category = self._get_content_length_category(word_count)
            
            # ç”Ÿæˆå®Œæ•´çš„è²¼æ–‡è¨˜éŒ„ - æŒ‰ç…§è²¼æ–‡è¨˜éŒ„è¡¨çš„ A-AH æ¬„ä½é †åº
            post_data = {
                # åŸºç¤ä¿¡æ¯æ¬„ä½ (A-R)
                'post_id': f"6919_{kol['serial']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",  # A: è²¼æ–‡ID
                'kol_serial': kol['serial'],  # B: KOL Serial
                'kol_nickname': kol['nickname'],  # C: KOL æš±ç¨±
                'kol_id': kol['member_id'],  # D: KOL ID
                'persona': kol['persona'],  # E: Persona
                'content_type': 'investment',  # F: Content Type
                'topic_index': 1,  # G: å·²æ´¾ç™¼TopicIndex
                'topic_id': f"6919_kangpei_{datetime.now().strftime('%Y%m%d')}",  # H: å·²æ´¾ç™¼TopicID
                'topic_title': content_data['title'],  # I: å·²æ´¾ç™¼TopicTitle
                'topic_keywords': content_data['keywords'],  # J: å·²æ´¾ç™¼TopicKeywords
                'content': content_data['content'],  # K: ç”Ÿæˆå…§å®¹
                'status': 'ready_to_post',  # L: ç™¼æ–‡ç‹€æ…‹
                'scheduled_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # M: ä¸Šæ¬¡æ’ç¨‹æ™‚é–“
                'post_time': '',  # N: ç™¼æ–‡æ™‚é–“æˆ³è¨˜
                'error_message': '',  # O: æœ€è¿‘éŒ¯èª¤è¨Šæ¯
                'platform_post_id': '',  # P: å¹³å°ç™¼æ–‡ID
                'platform_post_url': '',  # Q: å¹³å°ç™¼æ–‡URL
                'trending_topic_title': 'åº·éœˆç”ŸæŠ€æ¼²åœé¡Œæç™¼é…µ',  # R: ç†±é–€è©±é¡Œæ¨™é¡Œ
                
                # æ–°å¢æ¬„ä½ (S-AH)
                'trigger_type': 'enhanced_trending_topics',  # S: è§¸ç™¼æ”¯ç·šé¡å‹
                'trigger_event_id': f"enhanced_{datetime.now().strftime('%Y%m%d_%H%M')}",  # T: è§¸ç™¼äº‹ä»¶ID
                'data_sources': 'serper_api,openai_gpt,technical_analysis',  # U: èª¿ç”¨æ•¸æ“šä¾†æºåº«
                'data_source_status': 'serper:success,openai:success,technical:success',  # V: æ•¸æ“šä¾†æºç‹€æ…‹
                'agent_decision_record': f"å¢å¼·ç‰ˆå€‹äººåŒ–è©•åˆ†é€šé: 8.5/10",  # W: Agentæ±ºç­–ç´€éŒ„
                'post_type': self._get_post_type(kol['persona']),  # X: ç™¼æ–‡é¡å‹
                'content_length_type': content_length_type,  # Y: æ–‡ç« é•·åº¦é¡å‹
                'word_count': word_count,  # Z: å…§æ–‡å­—æ•¸
                'content_length_category': content_length_category,  # AA: å…§æ–‡é•·åº¦åˆ†é¡
                'kol_weight_settings': '1.0',  # AB: KOLæ¬Šé‡è¨­å®š
                'content_generation_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # AC: å…§å®¹ç”Ÿæˆæ™‚é–“
                'kol_settings_version': 'v4.0_enhanced_with_personalization',  # AD: KOLè¨­å®šç‰ˆæœ¬
                'length_vector': '0.7',  # AE: æ–‡ç« é•·åº¦å‘é‡
                'tone_vector': '0.7',  # AF: èªæ°£å‘é‡
                'temperature_setting': '0.8',  # AG: temperatureè¨­å®š
                'body_parameter': self._build_body_parameter()  # AH: body parameter
            }
            
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {kol['nickname']} çš„è²¼æ–‡")
            return post_data
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆ {kol['nickname']} è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    async def update_posts_sheet(self, posts: List[Dict[str, Any]]):
        """æ›´æ–°è²¼æ–‡è¨˜éŒ„è¡¨ - ç¢ºä¿æ‰€æœ‰æ¬„ä½æ­£ç¢ºå°æ‡‰ (A-AH)"""
        try:
            # æº–å‚™æ–°æ•¸æ“š - æŒ‰ç…§ A-AH æ¬„ä½é †åº
            new_rows = []
            for post in posts:
                row = [
                    post['post_id'],                    # A: è²¼æ–‡ID
                    post['kol_serial'],                  # B: KOL Serial
                    post['kol_nickname'],                # C: KOL æš±ç¨±
                    post['kol_id'],                      # D: KOL ID
                    post['persona'],                     # E: Persona
                    post['content_type'],                # F: Content Type
                    post['topic_index'],                 # G: å·²æ´¾ç™¼TopicIndex
                    post['topic_id'],                    # H: å·²æ´¾ç™¼TopicID
                    post['topic_title'],                 # I: å·²æ´¾ç™¼TopicTitle
                    post['topic_keywords'],              # J: å·²æ´¾ç™¼TopicKeywords
                    post['content'],                     # K: ç”Ÿæˆå…§å®¹
                    post['status'],                      # L: ç™¼æ–‡ç‹€æ…‹
                    post['scheduled_time'],              # M: ä¸Šæ¬¡æ’ç¨‹æ™‚é–“
                    post['post_time'],                   # N: ç™¼æ–‡æ™‚é–“æˆ³è¨˜
                    post['error_message'],              # O: æœ€è¿‘éŒ¯èª¤è¨Šæ¯
                    post['platform_post_id'],           # P: å¹³å°ç™¼æ–‡ID
                    post['platform_post_url'],          # Q: å¹³å°ç™¼æ–‡URL
                    post['trending_topic_title'],      # R: ç†±é–€è©±é¡Œæ¨™é¡Œ
                    post['trigger_type'],               # S: è§¸ç™¼æ”¯ç·šé¡å‹
                    post['trigger_event_id'],           # T: è§¸ç™¼äº‹ä»¶ID
                    post['data_sources'],               # U: èª¿ç”¨æ•¸æ“šä¾†æºåº«
                    post['data_source_status'],        # V: æ•¸æ“šä¾†æºç‹€æ…‹
                    post['agent_decision_record'],     # W: Agentæ±ºç­–ç´€éŒ„
                    post['post_type'],                  # X: ç™¼æ–‡é¡å‹
                    post['content_length_type'],        # Y: æ–‡ç« é•·åº¦é¡å‹
                    post['word_count'],                 # Z: å…§æ–‡å­—æ•¸
                    post['content_length_category'],    # AA: å…§æ–‡é•·åº¦åˆ†é¡
                    post['kol_weight_settings'],       # AB: KOLæ¬Šé‡è¨­å®š
                    post['content_generation_time'],    # AC: å…§å®¹ç”Ÿæˆæ™‚é–“
                    post['kol_settings_version'],       # AD: KOLè¨­å®šç‰ˆæœ¬
                    post['length_vector'],              # AE: æ–‡ç« é•·åº¦å‘é‡
                    post['tone_vector'],                # AF: èªæ°£å‘é‡
                    post['temperature_setting'],       # AG: temperatureè¨­å®š
                    post['body_parameter']              # AH: body parameter
                ]
                new_rows.append(row)
            
            # å¯«å…¥åˆ° Google Sheetsï¼ˆé€™è£¡éœ€è¦å¯¦éš›çš„ Google Sheets å®¢æˆ¶ç«¯ï¼‰
            logger.info(f"æº–å‚™å¯«å…¥ {len(posts)} ç­†è¨˜éŒ„åˆ°è²¼æ–‡è¨˜éŒ„è¡¨")
            logger.info("æ¬„ä½å°æ‡‰ç¢ºèª (A-AH):")
            logger.info("A: è²¼æ–‡ID, B: KOL Serial, C: KOL æš±ç¨±, D: KOL ID")
            logger.info("E: Persona, F: Content Type, G: Topic Index, H: Topic ID")
            logger.info("I: Topic Title, J: Topic Keywords, K: Content, L: Status")
            logger.info("M: Scheduled Time, N: Post Time, O: Error Message")
            logger.info("P: Platform Post ID, Q: Platform Post URL, R: Trending Topic Title")
            logger.info("S: è§¸ç™¼æ”¯ç·šé¡å‹, T: è§¸ç™¼äº‹ä»¶ID, U: èª¿ç”¨æ•¸æ“šä¾†æºåº«, V: æ•¸æ“šä¾†æºç‹€æ…‹")
            logger.info("W: Agentæ±ºç­–ç´€éŒ„, X: ç™¼æ–‡é¡å‹, Y: æ–‡ç« é•·åº¦é¡å‹, Z: å…§æ–‡å­—æ•¸")
            logger.info("AA: å…§æ–‡é•·åº¦åˆ†é¡, AB: KOLæ¬Šé‡è¨­å®š, AC: å…§å®¹ç”Ÿæˆæ™‚é–“, AD: KOLè¨­å®šç‰ˆæœ¬")
            logger.info("AE: æ–‡ç« é•·åº¦å‘é‡, AF: èªæ°£å‘é‡, AG: temperatureè¨­å®š, AH: body parameter")
            
            # è¼¸å‡ºç”Ÿæˆçš„å…§å®¹ä¾›æª¢æŸ¥
            for i, post in enumerate(posts, 1):
                logger.info(f"\nğŸ“ è²¼æ–‡ {i} - {post['kol_nickname']}:")
                logger.info(f"æ¨™é¡Œ: {post['topic_title']}")
                logger.info(f"å…§å®¹é•·åº¦: {post['word_count']} å­— ({post['content_length_category']})")
                logger.info(f"é—œéµå­—: {post['topic_keywords']}")
                logger.info(f"ç‹€æ…‹: {post['status']}")
                logger.info(f"ç™¼æ–‡é¡å‹: {post['post_type']}")
                logger.info(f"è§¸ç™¼æ”¯ç·š: {post['trigger_type']}")
                logger.info(f"æ•¸æ“šä¾†æº: {post['data_sources']}")
                logger.info(f"Body Parameter: {post['body_parameter']}")
            
        except Exception as e:
            logger.error(f"æ›´æ–°è²¼æ–‡è¨˜éŒ„è¡¨å¤±æ•—: {e}")
    
    async def run(self):
        """åŸ·è¡Œè²¼æ–‡ç”Ÿæˆæµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹ç”Ÿæˆ 6919 åº·éœˆç”ŸæŠ€ç›¸é—œè²¼æ–‡ (V4)...")
        
        try:
            # 1. ç²å– KOL è³‡æ–™
            kol_list = await self.get_kol_credentials()
            if not kol_list:
                logger.error("ç„¡æ³•ç²å– KOL è³‡æ–™")
                return
            
            # é¸æ“‡å…©å€‹ KOL
            selected_kols = [kol for kol in kol_list if kol['status'] == 'active'][:2]
            logger.info(f"ğŸ“‹ é¸ä¸­çš„ KOL: {[kol['nickname'] for kol in selected_kols]}")
            
            # 2. æœå°‹åº·éœˆç”ŸæŠ€ç›¸é—œæ–°è
            news_data = self.search_kangpei_news()
            
            # 3. ç‚ºæ¯å€‹ KOL ç”Ÿæˆè²¼æ–‡
            generated_posts = []
            for kol in selected_kols:
                post_data = await self.generate_kol_post(kol, news_data)
                if post_data:
                    generated_posts.append(post_data)
            
            # 4. æ›´æ–°è²¼æ–‡è¨˜éŒ„è¡¨
            if generated_posts:
                await self.update_posts_sheet(generated_posts)
                
                logger.info("ğŸ“Š ç”Ÿæˆçµæœæ‘˜è¦:")
                for post in generated_posts:
                    logger.info(f"  - {post['kol_nickname']} ({post['persona']}): {post['topic_title']}")
                    logger.info(f"    å…§å®¹é•·åº¦: {post['word_count']} å­— ({post['content_length_category']})")
                    logger.info(f"    ç‹€æ…‹: {post['status']}")
                    logger.info(f"    ç™¼æ–‡é¡å‹: {post['post_type']}")
                    logger.info(f"    Body Parameter: {post['body_parameter']}")
                    logger.info("")
            else:
                logger.error("âŒ æ²’æœ‰æˆåŠŸç”Ÿæˆä»»ä½•è²¼æ–‡")
                
        except Exception as e:
            logger.error(f"è²¼æ–‡ç”Ÿæˆæµç¨‹å¤±æ•—: {e}")

async def main():
    """ä¸»å‡½æ•¸"""
    generator = KangpeiPostGeneratorV4()
    await generator.run()

if __name__ == "__main__":
    asyncio.run(main())


