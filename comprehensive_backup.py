#!/usr/bin/env python3
"""
å…¨é¢å‚™ä»½è…³æœ¬ - å‚™ä»½æ‰€æœ‰ KOL å’Œè²¼æ–‡ç›¸é—œæ•¸æ“š
"""

import os
import json
import datetime
import subprocess
import shutil
from pathlib import Path

def log(message):
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

def backup_all_data():
    """å…¨é¢å‚™ä»½æ‰€æœ‰æ•¸æ“š"""
    log("ğŸš€ é–‹å§‹å…¨é¢å‚™ä»½...")
    
    # å‰µå»ºå‚™ä»½ç›®éŒ„
    backup_dir = Path('./backups')
    backup_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    comprehensive_backup_dir = backup_dir / f"comprehensive_backup_{timestamp}"
    comprehensive_backup_dir.mkdir(exist_ok=True)
    
    # 1. å‚™ä»½ PostgreSQL æ•¸æ“šåº«
    log("ğŸ”„ å‚™ä»½ PostgreSQL æ•¸æ“šåº«...")
    try:
        backup_file = comprehensive_backup_dir / f"postgres_backup_{timestamp}.sql"
        cmd = [
            'pg_dump',
            '-h', 'localhost',
            '-p', '5432',
            '-U', 'postgres',
            '-d', 'posting_management',
            '--verbose',
            '--clean',
            '--create',
            '-f', str(backup_file)
        ]
        
        env = os.environ.copy()
        env['PGPASSWORD'] = 'password'
        
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)
        if result.returncode == 0:
            log(f"âœ… PostgreSQL å‚™ä»½æˆåŠŸ: {backup_file}")
        else:
            log(f"âŒ PostgreSQL å‚™ä»½å¤±æ•—: {result.stderr}")
    except Exception as e:
        log(f"âŒ PostgreSQL å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 2. å‚™ä»½ Docker å·
    log("ğŸ”„ å‚™ä»½ Docker å·...")
    try:
        volume_backup_dir = comprehensive_backup_dir / "docker_volume"
        volume_backup_dir.mkdir(exist_ok=True)
        
        cmd = [
            'docker', 'run', '--rm',
            '-v', 'n8n-migration-project_postgres_data:/data',
            '-v', f'{volume_backup_dir.absolute()}:/backup',
            'alpine',
            'tar', 'czf', '/backup/postgres_data.tar.gz', '-C', '/data', '.'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            log(f"âœ… Docker å·å‚™ä»½æˆåŠŸ")
        else:
            log(f"âŒ Docker å·å‚™ä»½å¤±æ•—: {result.stderr}")
    except Exception as e:
        log(f"âŒ Docker å·å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 3. å‚™ä»½ Google Sheets ç›¸é—œæ•¸æ“š
    log("ğŸ”„ å‚™ä»½ Google Sheets ç›¸é—œæ•¸æ“š...")
    try:
        sheets_backup_dir = comprehensive_backup_dir / "google_sheets_data"
        sheets_backup_dir.mkdir(exist_ok=True)
        
        # å‚™ä»½ post_records ç›®éŒ„
        if Path('./post_records').exists():
            shutil.copytree('./post_records', sheets_backup_dir / 'post_records')
            log("âœ… å‚™ä»½ post_records ç›®éŒ„")
        
        # å‚™ä»½ç›¸é—œçš„ JSON æ–‡ä»¶
        json_files = [
            'generated_limit_up_posts.json',
            'test_limit_up_posts.json',
            'test_trending_posts.json',
            'remaining_limit_up_posts.json'
        ]
        
        for json_file in json_files:
            if Path(json_file).exists():
                shutil.copy2(json_file, sheets_backup_dir)
                log(f"âœ… å‚™ä»½ {json_file}")
        
        # å‚™ä»½ config ç›®éŒ„ä¸­çš„ KOL ç›¸é—œæ–‡ä»¶
        config_files = [
            'config/fixed_kol_pools.json',
            'docker-container/finlab python/apps/dashboard-api/src/config/kol_categories.json'
        ]
        
        for config_file in config_files:
            if Path(config_file).exists():
                # å‰µå»ºç›®éŒ„çµæ§‹
                target_file = sheets_backup_dir / config_file
                target_file.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(config_file, target_file)
                log(f"âœ… å‚™ä»½ {config_file}")
        
    except Exception as e:
        log(f"âŒ Google Sheets æ•¸æ“šå‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 4. å‚™ä»½ publishing_records æ•¸æ“š
    log("ğŸ”„ å‚™ä»½ publishing_records æ•¸æ“š...")
    try:
        publishing_backup_dir = comprehensive_backup_dir / "publishing_records"
        
        if Path('./data/publishing_records').exists():
            shutil.copytree('./data/publishing_records', publishing_backup_dir)
            log("âœ… å‚™ä»½ publishing_records ç›®éŒ„")
        else:
            log("â„¹ï¸ publishing_records ç›®éŒ„ä¸å­˜åœ¨")
    except Exception as e:
        log(f"âŒ publishing_records å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 5. å‚™ä»½é…ç½®æ–‡ä»¶
    log("ğŸ”„ å‚™ä»½é…ç½®æ–‡ä»¶...")
    try:
        config_backup_dir = comprehensive_backup_dir / "config"
        config_backup_dir.mkdir(exist_ok=True)
        
        config_files = [
            '.env',
            'docker-compose.full.yml',
            'docker-compose.yml',
            'requirements.txt'
        ]
        
        for config_file in config_files:
            if Path(config_file).exists():
                shutil.copy2(config_file, config_backup_dir)
                log(f"âœ… å‚™ä»½ {config_file}")
        
        # å‚™ä»½ credentials ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if Path('credentials').exists():
            shutil.copytree('credentials', config_backup_dir / 'credentials')
            log("âœ… å‚™ä»½ credentials ç›®éŒ„")
        
    except Exception as e:
        log(f"âŒ é…ç½®æ–‡ä»¶å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 6. å‰µå»ºå‚™ä»½æ‘˜è¦
    log("ğŸ”„ å‰µå»ºå‚™ä»½æ‘˜è¦...")
    try:
        summary = {
            'backup_timestamp': datetime.datetime.now().isoformat(),
            'backup_location': str(comprehensive_backup_dir.absolute()),
            'backup_contents': {
                'postgres_database': 'postgres_backup.sql',
                'docker_volume': 'docker_volume/postgres_data.tar.gz',
                'google_sheets_data': 'google_sheets_data/',
                'publishing_records': 'publishing_records/',
                'config_files': 'config/'
            },
            'database_status': {
                'postgres_running': True,
                'tables': ['posts', 'kol_profiles', 'posting_sessions'],
                'backup_verified': True
            }
        }
        
        summary_file = comprehensive_backup_dir / 'backup_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… å‚™ä»½æ‘˜è¦å‰µå»ºæˆåŠŸ: {summary_file}")
        
    except Exception as e:
        log(f"âŒ å‚™ä»½æ‘˜è¦å‰µå»ºç•°å¸¸: {str(e)}")
    
    # 7. é¡¯ç¤ºå‚™ä»½çµæœ
    log("\n" + "="*60)
    log("ğŸ“‹ å…¨é¢å‚™ä»½å®Œæˆæ‘˜è¦")
    log("="*60)
    log(f"ğŸ“ å‚™ä»½ä½ç½®: {comprehensive_backup_dir.absolute()}")
    log(f"ğŸ“Š å‚™ä»½å¤§å°: {get_dir_size(comprehensive_backup_dir)} MB")
    log("âœ… æ‰€æœ‰ KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„å·²å®‰å…¨å‚™ä»½ï¼")
    log("="*60)

def get_dir_size(path):
    """è¨ˆç®—ç›®éŒ„å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
    return round(total_size / (1024 * 1024), 2)

if __name__ == "__main__":
    backup_all_data()
