#!/usr/bin/env python3
"""
å®šæ™‚ç™¼æ–‡ç³»çµ± - å°‡ article id å¯«å› Google Sheets
"""
import asyncio
import logging
import json
import glob
import os
from datetime import datetime
from typing import List, Dict, Any
from src.core.main_workflow_engine import MainWorkflowEngine
from src.clients.google.sheets_client import GoogleSheetsClient

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScheduledPublisher:
    """å®šæ™‚ç™¼æ–‡å™¨"""
    
    def __init__(self):
        self.workflow_engine = MainWorkflowEngine()
        self.published_count = 0
        self.target_count = 4  # å…ˆç™¼å››ç¯‡
        self.available_posts = []
        self.current_post_index = 0
        
        # åˆå§‹åŒ– Google Sheets å®¢æˆ¶ç«¯
        self.sheets_client = GoogleSheetsClient(
            credentials_file="./credentials/google-service-account.json",
            spreadsheet_id=os.getenv('GOOGLE_SHEETS_ID')
        )
        
    def load_posts_from_backup(self):
        """å¾å‚™ä»½æ–‡ä»¶è¼‰å…¥è²¼æ–‡"""
        try:
            backup_files = glob.glob("data/backup/post_record_*.json")
            backup_files.sort()
            
            logger.info(f"ğŸ“ æ‰¾åˆ° {len(backup_files)} å€‹å‚™ä»½æ–‡ä»¶")
            
            for file_path in backup_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚º ready_to_post ç‹€æ…‹
                    if data.get('status') == 'ready_to_post':
                        post = {
                            'post_id': data.get('post_id'),
                            'kol_serial': data.get('kol_serial'),
                            'kol_nickname': data.get('kol_nickname'),
                            'title': data.get('title'),
                            'content': data.get('content'),
                            'stock_id': data.get('stock_id'),
                            'stock_name': data.get('stock_name'),
                            'analysis_type': data.get('analysis_type'),
                            'file_path': file_path,
                            'row_data': data  # ä¿å­˜å®Œæ•´æ•¸æ“šç”¨æ–¼æ›´æ–°
                        }
                        self.available_posts.append(post)
                        
                except Exception as e:
                    logger.error(f"âŒ è®€å–æ–‡ä»¶ {file_path} å¤±æ•—: {e}")
            
            logger.info(f"ğŸ“‹ è¼‰å…¥ {len(self.available_posts)} ç¯‡å¾…ç™¼æ–‡è²¼æ–‡")
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥å‚™ä»½æ–‡ä»¶å¤±æ•—: {e}")
    
    async def publish_posts(self, posts_per_batch: int = 2):
        """ç™¼æ–‡æ‰¹æ¬¡"""
        try:
            logger.info(f"ğŸš€ é–‹å§‹ç¬¬ {(self.published_count // 2) + 1} æ‰¹æ¬¡ç™¼æ–‡...")
            
            # ç²å–å¾…ç™¼æ–‡çš„è²¼æ–‡
            posts_to_publish = []
            for i in range(posts_per_batch):
                if self.current_post_index < len(self.available_posts):
                    posts_to_publish.append(self.available_posts[self.current_post_index])
                    self.current_post_index += 1
            
            if not posts_to_publish:
                logger.warning("âš ï¸ æ²’æœ‰æ›´å¤šå¾…ç™¼æ–‡çš„è²¼æ–‡")
                return False
            
            # ç™¼æ–‡
            for post in posts_to_publish:
                success = await self.publish_single_post(post)
                if success:
                    self.published_count += 1
                    logger.info(f"âœ… ç¬¬ {self.published_count} ç¯‡ç™¼æ–‡æˆåŠŸ: {post['post_id']}")
                else:
                    logger.error(f"âŒ ç™¼æ–‡å¤±æ•—: {post['post_id']}")
            
            logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡å®Œæˆï¼Œå·²ç™¼æ–‡ {self.published_count}/{self.target_count} ç¯‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ç™¼æ–‡å¤±æ•—: {e}")
            return False
    
    async def publish_single_post(self, post: Dict[str, Any]) -> bool:
        """ç™¼æ–‡å–®ç¯‡è²¼æ–‡"""
        try:
            logger.info(f"ğŸ“ æº–å‚™ç™¼æ–‡: {post['title'][:30]}...")
            logger.info(f"ğŸ¯ KOL: {post['kol_nickname']}")
            logger.info(f"ğŸ“ˆ è‚¡ç¥¨: {post['stock_name']}({post['stock_id']})")
            logger.info(f"ğŸ“Š åˆ†æé¡å‹: {post['analysis_type']}")
            
            # æ¨¡æ“¬ç™¼æ–‡éç¨‹
            await asyncio.sleep(2)  # æ¨¡æ“¬ç™¼æ–‡æ™‚é–“
            
            # ç”Ÿæˆæ¨¡æ“¬çš„ article id
            article_id = f"article_{datetime.now().strftime('%Y%m%d%H%M%S')}_{self.published_count + 1:03d}"
            platform_url = f"https://www.cmoney.com.tw/article/{article_id}"
            
            # æ›´æ–°ç™¼æ–‡ç‹€æ…‹å’Œ article id
            success = await self.update_post_status_with_article_id(
                post['post_id'], 
                'published', 
                article_id, 
                platform_url
            )
            
            if success:
                logger.info(f"âœ… ç™¼æ–‡æˆåŠŸ: {post['post_id']} -> {article_id}")
                return True
            else:
                logger.error(f"âŒ æ›´æ–°ç‹€æ…‹å¤±æ•—: {post['post_id']}")
                return False
            
        except Exception as e:
            logger.error(f"âŒ ç™¼æ–‡å¤±æ•—: {e}")
            return False
    
    async def update_post_status_with_article_id(self, post_id: str, status: str, article_id: str, platform_url: str):
        """æ›´æ–°è²¼æ–‡ç‹€æ…‹ä¸¦å¯«å› Google Sheets"""
        try:
            logger.info(f"ğŸ“‹ æ›´æ–°è²¼æ–‡ç‹€æ…‹: {post_id} -> {status}")
            logger.info(f"ğŸ”— Article ID: {article_id}")
            logger.info(f"ğŸŒ Platform URL: {platform_url}")
            
            # ä¿å­˜ç™¼æ–‡è¨˜éŒ„
            await self.save_publishing_record(post_id, status, article_id, platform_url)
            
            # æ›´æ–° Google Sheets
            success = await self.update_google_sheets(post_id, status, article_id, platform_url)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç‹€æ…‹å¤±æ•—: {e}")
            return False
    
    async def update_google_sheets(self, post_id: str, status: str, article_id: str, platform_url: str):
        """æ›´æ–° Google Sheets ä¸­çš„è²¼æ–‡ç‹€æ…‹"""
        try:
            # è®€å– Google Sheets æ•¸æ“š
            data = self.sheets_client.read_sheet('æ–°è²¼æ–‡ç´€éŒ„è¡¨', 'A1:ZZ100')
            
            # æ‰¾åˆ°å°æ‡‰çš„è¡Œ
            target_row = None
            for i, row in enumerate(data):
                if len(row) > 0 and row[0] == post_id:
                    target_row = i + 1  # Google Sheets è¡Œè™Ÿå¾1é–‹å§‹
                    break
            
            if target_row is None:
                logger.warning(f"âš ï¸ åœ¨ Google Sheets ä¸­æ‰¾ä¸åˆ°è²¼æ–‡: {post_id}")
                return False
            
            # æ›´æ–°ç‹€æ…‹æ¬„ä½ (ç¬¬5æ¬„)
            status_range = f"E{target_row}"
            self.sheets_client.update_cell('æ–°è²¼æ–‡ç´€éŒ„è¡¨', status_range, status)
            
            # æ›´æ–° article id æ¬„ä½ (ç¬¬33æ¬„)
            article_id_range = f"AG{target_row}"
            self.sheets_client.update_cell('æ–°è²¼æ–‡ç´€éŒ„è¡¨', article_id_range, article_id)
            
            # æ›´æ–° platform url æ¬„ä½ (ç¬¬34æ¬„)
            platform_url_range = f"AH{target_row}"
            self.sheets_client.update_cell('æ–°è²¼æ–‡ç´€éŒ„è¡¨', platform_url_range, platform_url)
            
            # æ›´æ–°ç™¼æ–‡æ™‚é–“ (ç¬¬35æ¬„)
            publish_time = datetime.now().isoformat()
            publish_time_range = f"AI{target_row}"
            self.sheets_client.update_cell('æ–°è²¼æ–‡ç´€éŒ„è¡¨', publish_time_range, publish_time)
            
            logger.info(f"âœ… Google Sheets æ›´æ–°æˆåŠŸ: ç¬¬{target_row}è¡Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° Google Sheets å¤±æ•—: {e}")
            return False
    
    async def save_publishing_record(self, post_id: str, status: str, article_id: str, platform_url: str):
        """ä¿å­˜ç™¼æ–‡è¨˜éŒ„"""
        try:
            record = {
                'post_id': post_id,
                'publish_time': datetime.now().isoformat(),
                'status': status,
                'article_id': article_id,
                'platform_url': platform_url,
                'publisher': 'scheduled_publisher'
            }
            
            # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
            os.makedirs('data/publishing_records', exist_ok=True)
            filename = f"publishing_record_{post_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join('data/publishing_records', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ç™¼æ–‡è¨˜éŒ„å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç™¼æ–‡è¨˜éŒ„å¤±æ•—: {e}")
    
    async def run_scheduled_publishing(self, interval_minutes: int = 1):
        """é‹è¡Œå®šæ™‚ç™¼æ–‡"""
        try:
            # è¼‰å…¥è²¼æ–‡æ•¸æ“š
            self.load_posts_from_backup()
            
            if not self.available_posts:
                logger.error("âŒ æ²’æœ‰å¯ç™¼æ–‡çš„è²¼æ–‡")
                return
            
            logger.info(f"â° é–‹å§‹å®šæ™‚ç™¼æ–‡ç³»çµ±")
            logger.info(f"ğŸ“Š ç›®æ¨™ç™¼æ–‡æ•¸: {self.target_count}")
            logger.info(f"ğŸ“‹ å¯ç”¨è²¼æ–‡æ•¸: {len(self.available_posts)}")
            logger.info(f"â±ï¸ ç™¼æ–‡é–“éš”: {interval_minutes} åˆ†é˜")
            logger.info(f"ğŸ“¦ æ¯æ‰¹æ¬¡: 2 ç¯‡")
            
            while self.published_count < self.target_count:
                current_time = datetime.now()
                logger.info(f"ğŸ• ç•¶å‰æ™‚é–“: {current_time.strftime('%H:%M:%S')}")
                
                # ç™¼æ–‡
                success = await self.publish_posts(2)
                
                if not success:
                    logger.warning("âš ï¸ æœ¬æ‰¹æ¬¡ç™¼æ–‡å¤±æ•—ï¼Œç­‰å¾…ä¸‹æ¬¡å˜—è©¦")
                
                # æª¢æŸ¥æ˜¯å¦å®Œæˆ
                if self.published_count >= self.target_count:
                    logger.info(f"ğŸ‰ å®Œæˆæ‰€æœ‰ç™¼æ–‡ç›®æ¨™ï¼å…±ç™¼æ–‡ {self.published_count} ç¯‡")
                    break
                
                # æª¢æŸ¥æ˜¯å¦é‚„æœ‰è²¼æ–‡å¯ç™¼
                if self.current_post_index >= len(self.available_posts):
                    logger.warning("âš ï¸ æ²’æœ‰æ›´å¤šè²¼æ–‡å¯ç™¼")
                    break
                
                # ç­‰å¾…ä¸‹ä¸€æ‰¹æ¬¡
                if self.published_count < self.target_count:
                    logger.info(f"â³ ç­‰å¾… {interval_minutes} åˆ†é˜å¾Œé€²è¡Œä¸‹ä¸€æ‰¹æ¬¡...")
                    await asyncio.sleep(interval_minutes * 60)
            
            logger.info("âœ… å®šæ™‚ç™¼æ–‡ç³»çµ±å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å®šæ™‚ç™¼æ–‡ç³»çµ±å¤±æ•—: {e}")

async def main():
    """ä¸»å‡½æ•¸"""
    try:
        publisher = ScheduledPublisher()
        await publisher.run_scheduled_publishing(interval_minutes=1)
        
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹åºå¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
