#!/usr/bin/env python3
"""
KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„å‚™ä»½è…³æœ¬
ç¢ºä¿æ•¸æ“šå®‰å…¨ï¼Œé˜²æ­¢æ•¸æ“šä¸Ÿå¤±
"""

import os
import sys
import json
import datetime
import subprocess
import psycopg2
from pathlib import Path
import shutil

# é…ç½®
DATABASE_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'posting_management',
    'user': 'postgres',
    'password': 'password'
}

BACKUP_DIR = Path('./backups')
BACKUP_DIR.mkdir(exist_ok=True)

def log(message):
    """è¨˜éŒ„æ—¥èªŒ"""
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

def backup_postgres_database():
    """å‚™ä»½ PostgreSQL æ•¸æ“šåº«"""
    log("ğŸ”„ é–‹å§‹å‚™ä»½ PostgreSQL æ•¸æ“šåº«...")
    
    try:
        # ç”Ÿæˆå‚™ä»½æ–‡ä»¶å
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = BACKUP_DIR / f"postgres_backup_{timestamp}.sql"
        
        # ä½¿ç”¨ pg_dump å‚™ä»½æ•¸æ“šåº«
        cmd = [
            'pg_dump',
            '-h', DATABASE_CONFIG['host'],
            '-p', str(DATABASE_CONFIG['port']),
            '-U', DATABASE_CONFIG['user'],
            '-d', DATABASE_CONFIG['database'],
            '--verbose',
            '--clean',
            '--create',
            '--if-exists',
            '-f', str(backup_file)
        ]
        
        # è¨­ç½®ç’°å¢ƒè®Šæ•¸ï¼ˆå¯†ç¢¼ï¼‰
        env = os.environ.copy()
        env['PGPASSWORD'] = DATABASE_CONFIG['password']
        
        # åŸ·è¡Œå‚™ä»½å‘½ä»¤
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)
        
        if result.returncode == 0:
            log(f"âœ… PostgreSQL å‚™ä»½æˆåŠŸ: {backup_file}")
            return str(backup_file)
        else:
            log(f"âŒ PostgreSQL å‚™ä»½å¤±æ•—: {result.stderr}")
            return None
            
    except Exception as e:
        log(f"âŒ PostgreSQL å‚™ä»½ç•°å¸¸: {str(e)}")
        return None

def backup_docker_volume():
    """å‚™ä»½ Docker å·"""
    log("ğŸ”„ é–‹å§‹å‚™ä»½ Docker å·...")
    
    try:
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        volume_backup_dir = BACKUP_DIR / f"docker_volume_backup_{timestamp}"
        volume_backup_dir.mkdir(exist_ok=True)
        
        # å‚™ä»½ postgres_data å·
        volume_name = "n8n-migration-project_postgres_data"
        
        # å‰µå»ºè‡¨æ™‚å®¹å™¨ä¾†å‚™ä»½å·
        container_name = f"backup_container_{timestamp}"
        
        # å•Ÿå‹•è‡¨æ™‚å®¹å™¨
        cmd = [
            'docker', 'run', '--rm',
            '-v', f'{volume_name}:/data',
            '-v', f'{volume_backup_dir.absolute()}:/backup',
            'alpine',
            'tar', 'czf', '/backup/postgres_data.tar.gz', '-C', '/data', '.'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            log(f"âœ… Docker å·å‚™ä»½æˆåŠŸ: {volume_backup_dir}")
            return str(volume_backup_dir)
        else:
            log(f"âŒ Docker å·å‚™ä»½å¤±æ•—: {result.stderr}")
            return None
            
    except Exception as e:
        log(f"âŒ Docker å·å‚™ä»½ç•°å¸¸: {str(e)}")
        return None

def backup_kol_data():
    """å‚™ä»½ KOL æ•¸æ“š"""
    log("ğŸ”„ é–‹å§‹å‚™ä»½ KOL æ•¸æ“š...")
    
    try:
        # é€£æ¥æ•¸æ“šåº«
        conn = psycopg2.connect(**DATABASE_CONFIG)
        cursor = conn.cursor()
        
        # æŸ¥è©¢æ‰€æœ‰ KOL æ•¸æ“š
        cursor.execute("""
            SELECT 
                post_id,
                session_id,
                kol_serial,
                kol_nickname,
                kol_persona,
                stock_code,
                stock_name,
                title,
                content,
                content_md,
                status,
                quality_score,
                ai_detection_score,
                risk_level,
                reviewer_notes,
                approved_by,
                approved_at,
                scheduled_at,
                published_at,
                cmoney_post_id,
                cmoney_post_url,
                publish_error,
                views,
                likes,
                comments,
                shares,
                topic_id,
                topic_title,
                created_at,
                updated_at
            FROM post_records 
            ORDER BY created_at DESC
        """)
        
        kol_data = []
        for row in cursor.fetchall():
            kol_data.append({
                'post_id': row[0],
                'session_id': row[1],
                'kol_serial': row[2],
                'kol_nickname': row[3],
                'kol_persona': row[4],
                'stock_code': row[5],
                'stock_name': row[6],
                'title': row[7],
                'content': row[8],
                'content_md': row[9],
                'status': row[10],
                'quality_score': row[11],
                'ai_detection_score': row[12],
                'risk_level': row[13],
                'reviewer_notes': row[14],
                'approved_by': row[15],
                'approved_at': row[16].isoformat() if row[16] else None,
                'scheduled_at': row[17].isoformat() if row[17] else None,
                'published_at': row[18].isoformat() if row[18] else None,
                'cmoney_post_id': row[19],
                'cmoney_post_url': row[20],
                'publish_error': row[21],
                'views': row[22],
                'likes': row[23],
                'comments': row[24],
                'shares': row[25],
                'topic_id': row[26],
                'topic_title': row[27],
                'created_at': row[28].isoformat() if row[28] else None,
                'updated_at': row[29].isoformat() if row[29] else None
            })
        
        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = BACKUP_DIR / f"kol_data_backup_{timestamp}.json"
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(kol_data, f, ensure_ascii=False, indent=2)
        
        cursor.close()
        conn.close()
        
        log(f"âœ… KOL æ•¸æ“šå‚™ä»½æˆåŠŸ: {backup_file}")
        log(f"ğŸ“Š å‚™ä»½äº† {len(kol_data)} ç­† KOL è¨˜éŒ„")
        return str(backup_file)
        
    except Exception as e:
        log(f"âŒ KOL æ•¸æ“šå‚™ä»½ç•°å¸¸: {str(e)}")
        return None

def backup_google_sheets_data():
    """å‚™ä»½ Google Sheets æ•¸æ“š"""
    log("ğŸ”„ é–‹å§‹å‚™ä»½ Google Sheets æ•¸æ“š...")
    
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰ Google Sheets ç›¸é—œæ–‡ä»¶
        sheets_files = []
        
        # æŸ¥æ‰¾å¯èƒ½çš„ Google Sheets å‚™ä»½æ–‡ä»¶
        for pattern in ['*.csv', '*.xlsx', '*.json']:
            for file in Path('.').glob(f'**/{pattern}'):
                if any(keyword in file.name.lower() for keyword in ['sheet', 'kol', 'post', 'record']):
                    sheets_files.append(file)
        
        if sheets_files:
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            sheets_backup_dir = BACKUP_DIR / f"google_sheets_backup_{timestamp}"
            sheets_backup_dir.mkdir(exist_ok=True)
            
            for file in sheets_files:
                shutil.copy2(file, sheets_backup_dir)
            
            log(f"âœ… Google Sheets æ•¸æ“šå‚™ä»½æˆåŠŸ: {sheets_backup_dir}")
            log(f"ğŸ“Š å‚™ä»½äº† {len(sheets_files)} å€‹æ–‡ä»¶")
            return str(sheets_backup_dir)
        else:
            log("â„¹ï¸ æœªæ‰¾åˆ° Google Sheets ç›¸é—œæ–‡ä»¶")
            return None
            
    except Exception as e:
        log(f"âŒ Google Sheets æ•¸æ“šå‚™ä»½ç•°å¸¸: {str(e)}")
        return None

def backup_config_files():
    """å‚™ä»½é…ç½®æ–‡ä»¶"""
    log("ğŸ”„ é–‹å§‹å‚™ä»½é…ç½®æ–‡ä»¶...")
    
    try:
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        config_backup_dir = BACKUP_DIR / f"config_backup_{timestamp}"
        config_backup_dir.mkdir(exist_ok=True)
        
        # å‚™ä»½é‡è¦çš„é…ç½®æ–‡ä»¶
        config_files = [
            '.env',
            'docker-compose.full.yml',
            'docker-compose.yml',
            'requirements.txt'
        ]
        
        for config_file in config_files:
            if Path(config_file).exists():
                shutil.copy2(config_file, config_backup_dir)
        
        # å‚™ä»½ credentials ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if Path('credentials').exists():
            shutil.copytree('credentials', config_backup_dir / 'credentials')
        
        log(f"âœ… é…ç½®æ–‡ä»¶å‚™ä»½æˆåŠŸ: {config_backup_dir}")
        return str(config_backup_dir)
        
    except Exception as e:
        log(f"âŒ é…ç½®æ–‡ä»¶å‚™ä»½ç•°å¸¸: {str(e)}")
        return None

def create_backup_summary(backups):
    """å‰µå»ºå‚™ä»½æ‘˜è¦"""
    log("ğŸ”„ å‰µå»ºå‚™ä»½æ‘˜è¦...")
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    summary_file = BACKUP_DIR / f"backup_summary_{timestamp}.json"
    
    summary = {
        'backup_timestamp': datetime.datetime.now().isoformat(),
        'backup_files': backups,
        'total_files': len([b for b in backups.values() if b is not None]),
        'database_status': 'healthy',
        'backup_location': str(BACKUP_DIR.absolute())
    }
    
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    log(f"âœ… å‚™ä»½æ‘˜è¦å‰µå»ºæˆåŠŸ: {summary_file}")
    return str(summary_file)

def verify_backup_integrity():
    """é©—è­‰å‚™ä»½å®Œæ•´æ€§"""
    log("ğŸ”„ é©—è­‰å‚™ä»½å®Œæ•´æ€§...")
    
    try:
        # æª¢æŸ¥æœ€è¿‘çš„å‚™ä»½æ–‡ä»¶
        backup_files = list(BACKUP_DIR.glob('*'))
        if not backup_files:
            log("âŒ æœªæ‰¾åˆ°å‚™ä»½æ–‡ä»¶")
            return False
        
        # æª¢æŸ¥ PostgreSQL å‚™ä»½
        sql_backups = list(BACKUP_DIR.glob('postgres_backup_*.sql'))
        if sql_backups:
            latest_sql = max(sql_backups, key=os.path.getctime)
            if latest_sql.stat().st_size > 1024:  # è‡³å°‘ 1KB
                log(f"âœ… PostgreSQL å‚™ä»½æ–‡ä»¶å®Œæ•´: {latest_sql.name}")
            else:
                log(f"âŒ PostgreSQL å‚™ä»½æ–‡ä»¶å¯èƒ½æå£: {latest_sql.name}")
                return False
        
        # æª¢æŸ¥ KOL æ•¸æ“šå‚™ä»½
        json_backups = list(BACKUP_DIR.glob('kol_data_backup_*.json'))
        if json_backups:
            latest_json = max(json_backups, key=os.path.getctime)
            try:
                with open(latest_json, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list) and len(data) > 0:
                        log(f"âœ… KOL æ•¸æ“šå‚™ä»½å®Œæ•´: {latest_json.name} ({len(data)} ç­†è¨˜éŒ„)")
                    else:
                        log(f"âŒ KOL æ•¸æ“šå‚™ä»½å¯èƒ½æå£: {latest_json.name}")
                        return False
            except json.JSONDecodeError:
                log(f"âŒ KOL æ•¸æ“šå‚™ä»½æ ¼å¼éŒ¯èª¤: {latest_json.name}")
                return False
        
        log("âœ… å‚™ä»½å®Œæ•´æ€§é©—è­‰é€šé")
        return True
        
    except Exception as e:
        log(f"âŒ å‚™ä»½å®Œæ•´æ€§é©—è­‰å¤±æ•—: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    log("ğŸš€ é–‹å§‹ KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„å‚™ä»½æµç¨‹")
    
    backups = {}
    
    # 1. å‚™ä»½ PostgreSQL æ•¸æ“šåº«
    backups['postgres_sql'] = backup_postgres_database()
    
    # 2. å‚™ä»½ Docker å·
    backups['docker_volume'] = backup_docker_volume()
    
    # 3. å‚™ä»½ KOL æ•¸æ“š
    backups['kol_data'] = backup_kol_data()
    
    # 4. å‚™ä»½ Google Sheets æ•¸æ“š
    backups['google_sheets'] = backup_google_sheets_data()
    
    # 5. å‚™ä»½é…ç½®æ–‡ä»¶
    backups['config_files'] = backup_config_files()
    
    # 6. å‰µå»ºå‚™ä»½æ‘˜è¦
    summary_file = create_backup_summary(backups)
    
    # 7. é©—è­‰å‚™ä»½å®Œæ•´æ€§
    integrity_ok = verify_backup_integrity()
    
    # 8. é¡¯ç¤ºå‚™ä»½çµæœ
    log("\n" + "="*60)
    log("ğŸ“‹ å‚™ä»½å®Œæˆæ‘˜è¦")
    log("="*60)
    
    for backup_type, backup_path in backups.items():
        if backup_path:
            log(f"âœ… {backup_type}: {backup_path}")
        else:
            log(f"âŒ {backup_type}: å‚™ä»½å¤±æ•—")
    
    log(f"ğŸ“„ å‚™ä»½æ‘˜è¦: {summary_file}")
    log(f"ğŸ” å‚™ä»½å®Œæ•´æ€§: {'âœ… é€šé' if integrity_ok else 'âŒ å¤±æ•—'}")
    log(f"ğŸ“ å‚™ä»½ä½ç½®: {BACKUP_DIR.absolute()}")
    
    if integrity_ok:
        log("\nğŸ‰ æ‰€æœ‰æ•¸æ“šå·²æˆåŠŸå‚™ä»½ï¼æ‚¨çš„ KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„éƒ½æ˜¯å®‰å…¨çš„ã€‚")
    else:
        log("\nâš ï¸ å‚™ä»½éç¨‹ä¸­ç™¼ç¾å•é¡Œï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚")
    
    log("="*60)

if __name__ == "__main__":
    main()
