# äº’å‹•åˆ†æç³»çµ±æŠ€è¡“æ–‡ä»¶

## ğŸ“‹ ç³»çµ±æ¦‚è¿°

äº’å‹•åˆ†æç³»çµ±æ˜¯ n8n-migration-project çš„æ•¸æ“šåˆ†ææ ¸å¿ƒï¼Œè² è²¬æ”¶é›†ã€è™•ç†å’Œåˆ†ææ‰€æœ‰è²¼æ–‡çš„äº’å‹•æ•¸æ“šã€‚ç³»çµ±æä¾›å¤šç¶­åº¦çš„äº’å‹•åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬1å°æ™‚ã€1æ—¥ã€7æ—¥æ•¸æ“šåˆ†æï¼Œå…§å®¹ç‰¹å¾µåˆ†æï¼Œä»¥åŠåŸºæ–¼äº’å‹•æ•¸æ“šçš„æ™ºèƒ½æ´å¯Ÿã€‚

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. **å¤šæ™‚é–“ç¶­åº¦åˆ†æ**
- **1å°æ™‚æ•¸æ“š**: å¯¦æ™‚äº’å‹•ç›£æ§ï¼Œå¿«é€ŸéŸ¿æ‡‰ç†±é»
- **1æ—¥æ•¸æ“š**: æ—¥é–“äº’å‹•è¶¨å‹¢åˆ†æï¼Œå„ªåŒ–ç™¼æ–‡æ™‚æ©Ÿ
- **7æ—¥æ•¸æ“š**: é€±æœŸæ€§äº’å‹•æ¨¡å¼åˆ†æï¼Œé•·æœŸç­–ç•¥åˆ¶å®š

### 2. **å…§å®¹ç‰¹å¾µåˆ†æ**
- æ¨™é¡Œç‰¹å¾µåˆ†æï¼ˆé•·åº¦ã€é—œéµè©ã€æƒ…æ„Ÿè‰²å½©ï¼‰
- å…§å®¹çµæ§‹åˆ†æï¼ˆæ®µè½ã€åˆ—è¡¨ã€å¼•ç”¨ï¼‰
- äº’å‹•å…ƒç´ åˆ†æï¼ˆè¡¨æƒ…ã€æ¨™ç±¤ã€é€£çµï¼‰

### 3. **KOL è¡¨ç¾åˆ†æ**
- å€‹äººäº’å‹•è¡¨ç¾çµ±è¨ˆ
- ç™¼æ–‡é¢¨æ ¼èˆ‡äº’å‹•é—œä¿‚
- æœ€ä½³ç™¼æ–‡æ™‚æ©Ÿè­˜åˆ¥

### 4. **å¸‚å ´è¶¨å‹¢åˆ†æ**
- ç†±é»è©±é¡Œè¿½è¹¤
- å¸‚å ´æƒ…ç·’åˆ†æ
- ç«¶çˆ­å°æ‰‹åˆ†æ

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

### å‰ç«¯çµ„ä»¶æ¶æ§‹

```typescript
// InteractionAnalysisPage.tsx - ä¸»è¦çµ„ä»¶
interface InteractionAnalysisPage {
  // æ•¸æ“šç‹€æ…‹
  posts: InteractionPost[];
  kolStats: Record<number, KOLStats>;
  overallStats: OverallStats | null;
  
  // ç¯©é¸æ¢ä»¶
  selectedKOL: number | undefined;
  dateRange: [any, any] | null;
  includeExternal: boolean;
  searchKeyword: string;
  
  // åˆ†æç‹€æ…‹
  showFeatureAnalysis: boolean;
  showSchedulingSuggestions: boolean;
  schedulingSuggestions: any[];
}

// äº’å‹•è²¼æ–‡æ•¸æ“šæ¨¡å‹
interface InteractionPost {
  post_id: string;
  article_id: string;
  kol_serial: number;
  kol_nickname: string;
  title: string;
  content: string;
  article_url: string;
  create_time: string;
  commodity_tags: Array<{key: string, type: string, bullOrBear: string}>;
  community_topic?: string;
  source: 'system' | 'external';
  status: string;
  views: number;
  likes: number;
  comments: number;
  shares: number;
  bookmarks: number;
  donations?: number;
  engagement_rate: number;
}
```

### å¾Œç«¯ API æ¶æ§‹

```python
# äº’å‹•æ•¸æ“šçµ±è¨ˆ API
@router.get("/stats")
async def get_interaction_stats():
    """ç²å–äº’å‹•æ•¸æ“šçµ±è¨ˆ"""
    
# æ‰¹é‡äº’å‹•æ•¸æ“šæ›´æ–° API
@router.post("/refresh-all")
async def refresh_all_interactions():
    """æ‰¹é‡åˆ·æ–°æ‰€æœ‰äº’å‹•æ•¸æ“š"""
    
# äº’å‹•æ•¸æ“šç²å– API
@router.get("/interactions")
async def get_interactions(
    kol_serial: Optional[int] = None,
    date_from: Optional[str] = None,
    date_to: Optional[str] = None,
    include_external: bool = True
):
    """ç²å–äº’å‹•æ•¸æ“š"""
```

## ğŸ“Š æ•¸æ“šæ¨¡å‹

### äº’å‹•æ•¸æ“šæ¨¡å‹

```python
@dataclass
class InteractionData:
    """äº’å‹•æ•¸æ“š"""
    post_id: str
    member_id: str
    likes: int = 0
    comments: int = 0
    shares: int = 0
    views: int = 0
    click_rate: float = 0.0
    engagement_rate: float = 0.0
    
    # è©³ç´°è¡¨æƒ…çµ±è¨ˆ
    dislikes: int = 0
    laughs: int = 0
    money: int = 0
    shock: int = 0
    cry: int = 0
    think: int = 0
    angry: int = 0
    total_emojis: int = 0
    
    # å…¶ä»–äº’å‹•æ•¸æ“š
    collections: int = 0
    donations: int = 0
    total_interactions: int = 0
    
    raw_data: Optional[Dict[str, Any]] = None
```

### KOL çµ±è¨ˆæ¨¡å‹

```typescript
interface KOLStats {
  kol_nickname: string;
  post_count: number;
  total_interactions: number;
  avg_interactions_per_post: number;
  best_performing_post: string;
  avg_engagement_rate: number;
  total_views: number;
  total_likes: number;
  total_comments: number;
  total_shares: number;
}
```

### æ•´é«”çµ±è¨ˆæ¨¡å‹

```typescript
interface OverallStats {
  total_posts: number;
  total_interactions: number;
  total_views: number;
  total_likes: number;
  total_comments: number;
  total_shares: number;
  avg_interactions_per_post: number;
  avg_engagement_rate: number;
}
```

## ğŸ”„ æ ¸å¿ƒæµç¨‹

### 1. **æ•¸æ“šæ”¶é›†æµç¨‹**

```python
async def collect_interaction_data():
    """æ”¶é›†äº’å‹•æ•¸æ“š"""
    
    # 1. ç²å–æ‰€æœ‰å·²ç™¼å¸ƒè²¼æ–‡
    published_posts = get_post_record_service().get_all_posts(status='published')
    
    # 2. ä¸¦è¡Œç²å–äº’å‹•æ•¸æ“š
    interaction_processor = InteractionDataProcessor(max_concurrent=3)
    interaction_data = await interaction_processor.fetch_interactions_parallel(
        published_posts, 
        progress_callback=update_progress
    )
    
    # 3. è¨ˆç®—äº’å‹•æŒ‡æ¨™
    for post in interaction_data:
        post['total_interactions'] = calculate_total_interactions(post)
        post['engagement_rate'] = calculate_engagement_rate(post)
    
    return interaction_data

def calculate_total_interactions(post: Dict) -> int:
    """è¨ˆç®—ç¸½äº’å‹•æ•¸"""
    return (
        post.get('likes', 0) + 
        post.get('comments', 0) + 
        post.get('shares', 0) + 
        post.get('collections', 0) + 
        post.get('donations', 0) +
        post.get('total_emojis', 0)
    )

def calculate_engagement_rate(post: Dict) -> float:
    """è¨ˆç®—äº’å‹•ç‡"""
    total_interactions = calculate_total_interactions(post)
    views = post.get('views', 1)  # é¿å…é™¤é›¶
    
    return (total_interactions / views) * 100 if views > 0 else 0
```

### 2. **å¤šæ™‚é–“ç¶­åº¦åˆ†æ**

```python
def analyze_time_dimensions(interaction_data: List[Dict]) -> Dict[str, Any]:
    """å¤šæ™‚é–“ç¶­åº¦åˆ†æ"""
    
    now = datetime.now()
    
    # 1å°æ™‚æ•¸æ“šåˆ†æ
    one_hour_ago = now - timedelta(hours=1)
    one_hour_data = filter_by_time_range(interaction_data, one_hour_ago, now)
    
    # 1æ—¥æ•¸æ“šåˆ†æ
    one_day_ago = now - timedelta(days=1)
    one_day_data = filter_by_time_range(interaction_data, one_day_ago, now)
    
    # 7æ—¥æ•¸æ“šåˆ†æ
    seven_days_ago = now - timedelta(days=7)
    seven_days_data = filter_by_time_range(interaction_data, seven_days_ago, now)
    
    return {
        '1hour': analyze_interaction_trends(one_hour_data),
        '1day': analyze_interaction_trends(one_day_data),
        '7days': analyze_interaction_trends(seven_days_data)
    }

def filter_by_time_range(data: List[Dict], start_time: datetime, end_time: datetime) -> List[Dict]:
    """æŒ‰æ™‚é–“ç¯„åœéæ¿¾æ•¸æ“š"""
    filtered_data = []
    
    for post in data:
        post_time = datetime.fromisoformat(post['create_time'].replace('Z', '+00:00'))
        if start_time <= post_time <= end_time:
            filtered_data.append(post)
    
    return filtered_data
```

### 3. **å…§å®¹ç‰¹å¾µåˆ†æ**

```python
def analyze_content_features(posts: List[Dict]) -> Dict[str, Any]:
    """å…§å®¹ç‰¹å¾µåˆ†æ"""
    
    features = {
        'title_features': analyze_title_features(posts),
        'content_features': analyze_content_structure(posts),
        'interaction_features': analyze_interaction_features(posts)
    }
    
    return features

def analyze_title_features(posts: List[Dict]) -> Dict[str, Any]:
    """æ¨™é¡Œç‰¹å¾µåˆ†æ"""
    
    titles = [post['title'] for post in posts]
    
    # æ¨™é¡Œé•·åº¦åˆ†æ
    title_lengths = [len(title) for title in titles]
    avg_title_length = sum(title_lengths) / len(title_lengths)
    
    # é—œéµè©åˆ†æ
    keywords = extract_keywords(titles)
    
    # æƒ…æ„Ÿåˆ†æ
    sentiment_scores = analyze_sentiment(titles)
    
    return {
        'avg_length': avg_title_length,
        'length_distribution': {
            'short': len([l for l in title_lengths if l < 20]),
            'medium': len([l for l in title_lengths if 20 <= l < 40]),
            'long': len([l for l in title_lengths if l >= 40])
        },
        'top_keywords': keywords[:10],
        'sentiment_distribution': sentiment_scores
    }

def analyze_content_structure(posts: List[Dict]) -> Dict[str, Any]:
    """å…§å®¹çµæ§‹åˆ†æ"""
    
    structures = {
        'paragraph_count': [],
        'list_count': [],
        'emoji_count': [],
        'hashtag_count': [],
        'link_count': []
    }
    
    for post in posts:
        content = post['content']
        
        # æ®µè½æ•¸
        paragraphs = content.split('\n\n')
        structures['paragraph_count'].append(len(paragraphs))
        
        # åˆ—è¡¨æ•¸
        lists = content.count('â€¢') + content.count('-') + content.count('*')
        structures['list_count'].append(lists)
        
        # è¡¨æƒ…æ•¸
        emojis = count_emojis(content)
        structures['emoji_count'].append(emojis)
        
        # æ¨™ç±¤æ•¸
        hashtags = content.count('#')
        structures['hashtag_count'].append(hashtags)
        
        # é€£çµæ•¸
        links = content.count('http')
        structures['link_count'].append(links)
    
    # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
    result = {}
    for key, values in structures.items():
        result[key] = {
            'avg': sum(values) / len(values),
            'max': max(values),
            'min': min(values)
        }
    
    return result
```

### 4. **KOL è¡¨ç¾åˆ†æ**

```python
def analyze_kol_performance(interaction_data: List[Dict]) -> Dict[int, KOLStats]:
    """KOL è¡¨ç¾åˆ†æ"""
    
    kol_stats = {}
    
    # æŒ‰ KOL åˆ†çµ„
    kol_groups = group_by_kol(interaction_data)
    
    for kol_serial, posts in kol_groups.items():
        stats = calculate_kol_stats(kol_serial, posts)
        kol_stats[kol_serial] = stats
    
    return kol_stats

def calculate_kol_stats(kol_serial: int, posts: List[Dict]) -> KOLStats:
    """è¨ˆç®— KOL çµ±è¨ˆæ•¸æ“š"""
    
    if not posts:
        return KOLStats(
            kol_nickname="",
            post_count=0,
            total_interactions=0,
            avg_interactions_per_post=0,
            best_performing_post="",
            avg_engagement_rate=0,
            total_views=0,
            total_likes=0,
            total_comments=0,
            total_shares=0
        )
    
    # åŸºæœ¬çµ±è¨ˆ
    post_count = len(posts)
    total_interactions = sum(post['total_interactions'] for post in posts)
    total_views = sum(post.get('views', 0) for post in posts)
    total_likes = sum(post.get('likes', 0) for post in posts)
    total_comments = sum(post.get('comments', 0) for post in posts)
    total_shares = sum(post.get('shares', 0) for post in posts)
    
    # å¹³å‡æ•¸æ“š
    avg_interactions_per_post = total_interactions / post_count
    avg_engagement_rate = sum(post['engagement_rate'] for post in posts) / post_count
    
    # æœ€ä½³è¡¨ç¾è²¼æ–‡
    best_post = max(posts, key=lambda x: x['total_interactions'])
    best_performing_post = best_post['title']
    
    return KOLStats(
        kol_nickname=posts[0]['kol_nickname'],
        post_count=post_count,
        total_interactions=total_interactions,
        avg_interactions_per_post=avg_interactions_per_post,
        best_performing_post=best_performing_post,
        avg_engagement_rate=avg_engagement_rate,
        total_views=total_views,
        total_likes=total_likes,
        total_comments=total_comments,
        total_shares=total_shares
    )
```

## ğŸ§  æ™ºèƒ½åˆ†æç®—æ³•

### 1. **äº’å‹•è¶¨å‹¢åˆ†æç®—æ³•**

```python
def analyze_interaction_trends(posts: List[Dict]) -> Dict[str, Any]:
    """äº’å‹•è¶¨å‹¢åˆ†æ"""
    
    if not posts:
        return {
            'trend': 'stable',
            'growth_rate': 0,
            'peak_hours': [],
            'recommendations': []
        }
    
    # æŒ‰æ™‚é–“æ’åº
    sorted_posts = sorted(posts, key=lambda x: x['create_time'])
    
    # è¨ˆç®—å¢é•·ç‡
    if len(sorted_posts) >= 2:
        early_interactions = sum(post['total_interactions'] for post in sorted_posts[:len(sorted_posts)//2])
        late_interactions = sum(post['total_interactions'] for post in sorted_posts[len(sorted_posts)//2:])
        growth_rate = ((late_interactions - early_interactions) / early_interactions) * 100 if early_interactions > 0 else 0
    else:
        growth_rate = 0
    
    # è­˜åˆ¥è¶¨å‹¢
    if growth_rate > 10:
        trend = 'increasing'
    elif growth_rate < -10:
        trend = 'decreasing'
    else:
        trend = 'stable'
    
    # åˆ†æé«˜å³°æ™‚æ®µ
    peak_hours = analyze_peak_hours(posts)
    
    # ç”Ÿæˆå»ºè­°
    recommendations = generate_recommendations(trend, growth_rate, peak_hours)
    
    return {
        'trend': trend,
        'growth_rate': growth_rate,
        'peak_hours': peak_hours,
        'recommendations': recommendations
    }
```

### 2. **ç†±é»è©±é¡Œè­˜åˆ¥ç®—æ³•**

```python
def identify_hot_topics(posts: List[Dict], time_window: int = 24) -> List[Dict[str, Any]]:
    """è­˜åˆ¥ç†±é»è©±é¡Œ"""
    
    # æå–è©±é¡Œæ¨™ç±¤
    topics = []
    for post in posts:
        if post.get('commodity_tags'):
            for tag in post['commodity_tags']:
                topics.append({
                    'topic': tag['key'],
                    'type': tag['type'],
                    'interactions': post['total_interactions'],
                    'timestamp': post['create_time']
                })
    
    # æŒ‰è©±é¡Œåˆ†çµ„
    topic_groups = {}
    for topic in topics:
        key = f"{topic['topic']}_{topic['type']}"
        if key not in topic_groups:
            topic_groups[key] = []
        topic_groups[key].append(topic)
    
    # è¨ˆç®—è©±é¡Œç†±åº¦
    hot_topics = []
    for key, topic_posts in topic_groups.items():
        total_interactions = sum(t['interactions'] for t in topic_posts)
        post_count = len(topic_posts)
        avg_interactions = total_interactions / post_count
        
        # è¨ˆç®—ç†±åº¦åˆ†æ•¸
        heat_score = (total_interactions * 0.7) + (post_count * 0.3)
        
        hot_topics.append({
            'topic': topic_posts[0]['topic'],
            'type': topic_posts[0]['type'],
            'total_interactions': total_interactions,
            'post_count': post_count,
            'avg_interactions': avg_interactions,
            'heat_score': heat_score
        })
    
    # æŒ‰ç†±åº¦æ’åº
    hot_topics.sort(key=lambda x: x['heat_score'], reverse=True)
    
    return hot_topics[:10]  # è¿”å›å‰10å€‹ç†±é»è©±é¡Œ
```

### 3. **ç™¼æ–‡æ™‚æ©Ÿå„ªåŒ–ç®—æ³•**

```python
def optimize_posting_timing(kol_serial: int, historical_data: List[Dict]) -> Dict[str, Any]:
    """å„ªåŒ–ç™¼æ–‡æ™‚æ©Ÿ"""
    
    # æŒ‰å°æ™‚åˆ†çµ„
    hourly_data = {}
    for post in historical_data:
        if post['kol_serial'] == kol_serial:
            hour = datetime.fromisoformat(post['create_time'].replace('Z', '+00:00')).hour
            if hour not in hourly_data:
                hourly_data[hour] = []
            hourly_data[hour].append(post)
    
    # è¨ˆç®—æ¯å°æ™‚çš„å¹³å‡äº’å‹•
    hourly_performance = {}
    for hour, posts in hourly_data.items():
        avg_interactions = sum(post['total_interactions'] for post in posts) / len(posts)
        hourly_performance[hour] = {
            'avg_interactions': avg_interactions,
            'post_count': len(posts),
            'success_rate': len([p for p in posts if p['total_interactions'] > 10]) / len(posts)
        }
    
    # æ‰¾å‡ºæœ€ä½³æ™‚æ®µ
    best_hours = sorted(
        hourly_performance.items(), 
        key=lambda x: x[1]['avg_interactions'], 
        reverse=True
    )[:3]
    
    return {
        'best_hours': [hour for hour, _ in best_hours],
        'hourly_performance': hourly_performance,
        'recommendations': generate_timing_recommendations(best_hours)
    }
```

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–

### 1. **ä¸¦è¡Œè™•ç†å„ªåŒ–**

```python
class InteractionAnalysisProcessor(ParallelProcessor):
    """äº’å‹•åˆ†æä¸¦è¡Œè™•ç†å™¨"""
    
    def __init__(self, max_concurrent: int = 5, timeout: int = 60):
        super().__init__(max_concurrent, timeout)
    
    async def analyze_interactions_parallel(self, posts: List[Dict]) -> Dict[str, Any]:
        """ä¸¦è¡Œåˆ†æäº’å‹•æ•¸æ“š"""
        
        # ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹åˆ†æä»»å‹™
        tasks = [
            self.analyze_content_features(posts),
            self.analyze_kol_performance(posts),
            self.analyze_time_dimensions(posts),
            self.identify_hot_topics(posts)
        ]
        
        results = await asyncio.gather(*tasks)
        
        return {
            'content_features': results[0],
            'kol_performance': results[1],
            'time_dimensions': results[2],
            'hot_topics': results[3]
        }
```

### 2. **æ•¸æ“šç·©å­˜å„ªåŒ–**

```python
class InteractionCache:
    """äº’å‹•æ•¸æ“šç·©å­˜"""
    
    def __init__(self, cache_duration: int = 1800):  # 30åˆ†é˜ç·©å­˜
        self.cache = {}
        self.cache_duration = cache_duration
    
    def get_cached_analysis(self, key: str) -> Optional[Any]:
        """ç²å–ç·©å­˜çš„åˆ†æçµæœ"""
        if key in self.cache:
            result, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_duration:
                return result
            else:
                del self.cache[key]
        return None
    
    def cache_analysis(self, key: str, result: Any):
        """ç·©å­˜åˆ†æçµæœ"""
        self.cache[key] = (result, time.time())
```

## ğŸ” ç›£æ§èˆ‡æ—¥èªŒ

### 1. **åˆ†æé€²åº¦ç›£æ§**

```python
class AnalysisProgressMonitor:
    """åˆ†æé€²åº¦ç›£æ§å™¨"""
    
    def __init__(self):
        self.progress_data = {}
    
    def update_progress(self, task_id: str, progress: float, message: str):
        """æ›´æ–°åˆ†æé€²åº¦"""
        self.progress_data[task_id] = {
            'progress': progress,
            'message': message,
            'timestamp': datetime.now(),
            'status': 'running' if progress < 100 else 'completed'
        }
        
        logger.info(f"ğŸ“Š åˆ†æé€²åº¦æ›´æ–° - {task_id}: {progress}% - {message}")
    
    def get_progress(self, task_id: str) -> Dict[str, Any]:
        """ç²å–åˆ†æé€²åº¦"""
        return self.progress_data.get(task_id, {
            'progress': 0,
            'message': 'æœªé–‹å§‹',
            'timestamp': None,
            'status': 'pending'
        })
```

### 2. **åˆ†æçµæœè¿½è¹¤**

```python
class AnalysisResultTracker:
    """åˆ†æçµæœè¿½è¹¤å™¨"""
    
    def track_analysis_result(self, analysis_id: str, result: Dict[str, Any]):
        """è¿½è¹¤åˆ†æçµæœ"""
        
        # è¨˜éŒ„åˆ†æçµæœ
        analysis_record = {
            'analysis_id': analysis_id,
            'result': result,
            'timestamp': datetime.now(),
            'data_quality': self.assess_data_quality(result),
            'insights_count': len(result.get('insights', []))
        }
        
        # ä¿å­˜åˆ°æ•¸æ“šåº«
        save_analysis_record(analysis_record)
        
        logger.info(f"ğŸ“ˆ åˆ†æçµæœè¿½è¹¤ - {analysis_id}: æ•¸æ“šå“è³ª={analysis_record['data_quality']}")
    
    def assess_data_quality(self, result: Dict[str, Any]) -> str:
        """è©•ä¼°æ•¸æ“šå“è³ª"""
        sample_size = result.get('sample_size', 0)
        
        if sample_size >= 100:
            return 'high'
        elif sample_size >= 50:
            return 'medium'
        else:
            return 'low'
```

## ğŸš€ éƒ¨ç½²èˆ‡é…ç½®

### 1. **ç’°å¢ƒè®Šé‡é…ç½®**

```bash
# äº’å‹•åˆ†æç³»çµ±é…ç½®
INTERACTION_ANALYSIS_ENABLED=true
ANALYSIS_INTERVAL=1800  # åˆ†æé–“éš”ï¼ˆç§’ï¼‰
CACHE_DURATION=1800  # ç·©å­˜æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
MAX_CONCURRENT_ANALYSIS=5
ANALYSIS_TIMEOUT=60

# æ•¸æ“šæ”¶é›†é…ç½®
COLLECT_EXTERNAL_DATA=true
EXTERNAL_DATA_SOURCES=cmoney,facebook,instagram
DATA_RETENTION_DAYS=90

# æ€§èƒ½é…ç½®
ENABLE_PARALLEL_PROCESSING=true
MAX_BATCH_SIZE=1000
MEMORY_LIMIT=2GB
```

### 2. **Docker é…ç½®**

```yaml
# docker-compose.yml
services:
  interaction-analysis-service:
    build: ./interaction-analysis-service
    environment:
      - INTERACTION_ANALYSIS_ENABLED=true
      - ANALYSIS_INTERVAL=1800
      - MAX_CONCURRENT_ANALYSIS=5
    volumes:
      - ./data/analysis-cache:/app/cache
    depends_on:
      - postgres
      - posting-service
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### 1. **å•Ÿå‹•äº’å‹•åˆ†æ**

```python
# å•Ÿå‹•äº’å‹•åˆ†æç³»çµ±
async def start_interaction_analysis():
    """å•Ÿå‹•äº’å‹•åˆ†æç³»çµ±"""
    
    # å‰µå»ºåˆ†æè™•ç†å™¨
    processor = InteractionAnalysisProcessor(max_concurrent=5)
    
    # ç²å–äº’å‹•æ•¸æ“š
    interaction_data = await collect_interaction_data()
    
    # ä¸¦è¡Œåˆ†æ
    analysis_result = await processor.analyze_interactions_parallel(interaction_data)
    
    # ç”Ÿæˆæ´å¯Ÿ
    insights = generate_insights(analysis_result)
    
    # ä¿å­˜çµæœ
    await save_analysis_result(analysis_result, insights)
    
    logger.info("ğŸ‰ äº’å‹•åˆ†æç³»çµ±å•Ÿå‹•å®Œæˆ")
```

### 2. **ç›£æ§åˆ†æé€²åº¦**

```python
# ç›£æ§åˆ†æé€²åº¦
async def monitor_analysis_progress():
    """ç›£æ§åˆ†æé€²åº¦"""
    
    monitor = AnalysisProgressMonitor()
    
    while True:
        # ç²å–æ‰€æœ‰ä»»å‹™é€²åº¦
        for task_id in monitor.progress_data.keys():
            progress = monitor.get_progress(task_id)
            print(f"ä»»å‹™ {task_id}: {progress['progress']}% - {progress['message']}")
        
        await asyncio.sleep(10)  # æ¯10ç§’æ›´æ–°ä¸€æ¬¡
```

## ğŸ”§ æ•…éšœæ’é™¤

### 1. **å¸¸è¦‹å•é¡Œ**

#### å•é¡Œï¼šæ•¸æ“šæ”¶é›†å¤±æ•—
**ç—‡ç‹€**: ç„¡æ³•ç²å–äº’å‹•æ•¸æ“š
**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æª¢æŸ¥ API é€£æ¥
async def check_api_connection():
    """æª¢æŸ¥ API é€£æ¥"""
    try:
        response = await cmoney_client.health_check()
        if response.status_code == 200:
            logger.info("âœ… API é€£æ¥æ­£å¸¸")
        else:
            logger.error("âŒ API é€£æ¥å¤±æ•—")
    except Exception as e:
        logger.error(f"âŒ API é€£æ¥ç•°å¸¸: {e}")
```

#### å•é¡Œï¼šåˆ†æçµæœä¸æº–ç¢º
**ç—‡ç‹€**: åˆ†æçµæœèˆ‡å¯¦éš›æƒ…æ³ä¸ç¬¦
**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é©—è­‰æ•¸æ“šå“è³ª
def validate_data_quality(data: List[Dict]) -> bool:
    """é©—è­‰æ•¸æ“šå“è³ª"""
    if not data:
        return False
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_fields = ['post_id', 'likes', 'comments', 'shares']
    for post in data:
        for field in required_fields:
            if field not in post:
                return False
    
    return True
```

### 2. **æ€§èƒ½å•é¡Œ**

#### å•é¡Œï¼šåˆ†æéç¨‹å¤ªæ…¢
**è§£æ±ºæ–¹æ¡ˆ**:
- å¢åŠ ä¸¦è¡Œè™•ç†æ•¸é‡
- ä½¿ç”¨æ•¸æ“šç·©å­˜
- å„ªåŒ–ç®—æ³•è¤‡é›œåº¦

#### å•é¡Œï¼šå…§å­˜ä½¿ç”¨éé«˜
**è§£æ±ºæ–¹æ¡ˆ**:
- åˆ†æ‰¹è™•ç†æ•¸æ“š
- ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨
- åŠæ™‚æ¸…ç†ç·©å­˜

## ğŸ“ˆ æœªä¾†æ”¹é€²

### 1. **å¯¦æ™‚åˆ†æ**
- å¯¦æ™‚æ•¸æ“šæµè™•ç†
- å³æ™‚æ´å¯Ÿç”Ÿæˆ
- å‹•æ…‹å‘Šè­¦ç³»çµ±

### 2. **æ©Ÿå™¨å­¸ç¿’é›†æˆ**
- é æ¸¬æ¨¡å‹
- ç•°å¸¸æª¢æ¸¬
- è‡ªå‹•å„ªåŒ–å»ºè­°

### 3. **å¤šå¹³å°æ•´åˆ**
- è·¨å¹³å°æ•¸æ“šæ•´åˆ
- çµ±ä¸€åˆ†æè¦–åœ–
- ç¶œåˆè¡¨ç¾è©•ä¼°

### 4. **å¯è¦–åŒ–å¢å¼·**
- äº’å‹•å¼åœ–è¡¨
- å¯¦æ™‚å„€è¡¨æ¿
- è‡ªå®šç¾©å ±å‘Š

## ğŸ“š ç›¸é—œæ–‡æª”

- [è‡ªæˆ‘å­¸ç¿’ç³»çµ±](./08-self-learning-system.md)
- [è§¸ç™¼å™¨ç³»çµ±](./05-trigger-system.md)
- [å…§å®¹ç”Ÿæˆæµç¨‹](./06-content-generation-flow.md)
- [Bug åˆ†æå’Œå•é¡Œæ¸…å–®](./07-bug-analysis-and-issues.md)


