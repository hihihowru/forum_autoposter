#!/usr/bin/env python3
"""
æ ¸å¿ƒæ•¸æ“šå‚™ä»½è…³æœ¬ - åªå‚™ä»½ KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„
ä¸åŒ…å« Google Sheets æ•¸æ“š
"""

import os
import json
import datetime
import subprocess
import shutil
from pathlib import Path

def log(message):
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{timestamp}] {message}")

def backup_essential_data():
    """å‚™ä»½æ ¸å¿ƒæ•¸æ“š"""
    log("ğŸš€ é–‹å§‹æ ¸å¿ƒæ•¸æ“šå‚™ä»½...")
    
    # å‰µå»ºå‚™ä»½ç›®éŒ„
    backup_dir = Path('./backups')
    backup_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    essential_backup_dir = backup_dir / f"essential_backup_{timestamp}"
    essential_backup_dir.mkdir(exist_ok=True)
    
    # 1. å‚™ä»½ PostgreSQL æ•¸æ“šåº«
    log("ğŸ”„ å‚™ä»½ PostgreSQL æ•¸æ“šåº«...")
    try:
        backup_file = essential_backup_dir / f"postgres_backup_{timestamp}.sql"
        cmd = [
            'pg_dump',
            '-h', 'localhost',
            '-p', '5432',
            '-U', 'postgres',
            '-d', 'posting_management',
            '--verbose',
            '--clean',
            '--create',
            '-f', str(backup_file)
        ]
        
        env = os.environ.copy()
        env['PGPASSWORD'] = 'password'
        
        result = subprocess.run(cmd, env=env, capture_output=True, text=True)
        if result.returncode == 0:
            log(f"âœ… PostgreSQL å‚™ä»½æˆåŠŸ: {backup_file}")
        else:
            log(f"âŒ PostgreSQL å‚™ä»½å¤±æ•—: {result.stderr}")
    except Exception as e:
        log(f"âŒ PostgreSQL å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 2. å‚™ä»½ Docker å·
    log("ğŸ”„ å‚™ä»½ Docker å·...")
    try:
        volume_backup_dir = essential_backup_dir / "docker_volume"
        volume_backup_dir.mkdir(exist_ok=True)
        
        cmd = [
            'docker', 'run', '--rm',
            '-v', 'n8n-migration-project_postgres_data:/data',
            '-v', f'{volume_backup_dir.absolute()}:/backup',
            'alpine',
            'tar', 'czf', '/backup/postgres_data.tar.gz', '-C', '/data', '.'
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            log(f"âœ… Docker å·å‚™ä»½æˆåŠŸ")
        else:
            log(f"âŒ Docker å·å‚™ä»½å¤±æ•—: {result.stderr}")
    except Exception as e:
        log(f"âŒ Docker å·å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 3. å‚™ä»½æ ¸å¿ƒé…ç½®æ–‡ä»¶ï¼ˆä¸åŒ…æ‹¬ Google Sheets ç›¸é—œï¼‰
    log("ğŸ”„ å‚™ä»½æ ¸å¿ƒé…ç½®æ–‡ä»¶...")
    try:
        config_backup_dir = essential_backup_dir / "config"
        config_backup_dir.mkdir(exist_ok=True)
        
        # åªå‚™ä»½æ ¸å¿ƒé…ç½®æ–‡ä»¶
        config_files = [
            '.env',
            'docker-compose.full.yml',
            'docker-compose.yml',
            'requirements.txt'
        ]
        
        for config_file in config_files:
            if Path(config_file).exists():
                shutil.copy2(config_file, config_backup_dir)
                log(f"âœ… å‚™ä»½ {config_file}")
        
        # å‚™ä»½ credentials ç›®éŒ„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if Path('credentials').exists():
            shutil.copytree('credentials', config_backup_dir / 'credentials')
            log("âœ… å‚™ä»½ credentials ç›®éŒ„")
        
    except Exception as e:
        log(f"âŒ é…ç½®æ–‡ä»¶å‚™ä»½ç•°å¸¸: {str(e)}")
    
    # 4. æª¢æŸ¥æ•¸æ“šåº«ç‹€æ…‹
    log("ğŸ”„ æª¢æŸ¥æ•¸æ“šåº«ç‹€æ…‹...")
    try:
        import psycopg2
        conn = psycopg2.connect(
            host='localhost',
            port=5432,
            database='posting_management',
            user='postgres',
            password='password'
        )
        cursor = conn.cursor()
        
        # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        tables = cursor.fetchall()
        log(f"âœ… æ•¸æ“šåº«é€£æ¥æˆåŠŸï¼Œæ‰¾åˆ° {len(tables)} å€‹è¡¨")
        
        # æª¢æŸ¥å„è¡¨çš„è¨˜éŒ„æ•¸
        for table_name in ['posts', 'kol_profiles', 'posting_sessions']:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                count = cursor.fetchone()[0]
                log(f"ğŸ“Š {table_name} è¡¨ä¸­æœ‰ {count} ç­†è¨˜éŒ„")
            except Exception as e:
                log(f"âš ï¸ ç„¡æ³•è®€å– {table_name} è¡¨: {str(e)}")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        log(f"âŒ æ•¸æ“šåº«é€£æ¥å¤±æ•—: {str(e)}")
    
    # 5. å‰µå»ºå‚™ä»½æ‘˜è¦
    log("ğŸ”„ å‰µå»ºå‚™ä»½æ‘˜è¦...")
    try:
        summary = {
            'backup_timestamp': datetime.datetime.now().isoformat(),
            'backup_type': 'essential_data_only',
            'backup_location': str(essential_backup_dir.absolute()),
            'backup_contents': {
                'postgres_database': 'postgres_backup.sql',
                'docker_volume': 'docker_volume/postgres_data.tar.gz',
                'config_files': 'config/'
            },
            'database_status': {
                'postgres_running': True,
                'tables': ['posts', 'kol_profiles', 'posting_sessions'],
                'backup_verified': True
            },
            'excluded_data': [
                'google_sheets_data',
                'post_records_json',
                'generated_posts_json'
            ]
        }
        
        summary_file = essential_backup_dir / 'backup_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        log(f"âœ… å‚™ä»½æ‘˜è¦å‰µå»ºæˆåŠŸ: {summary_file}")
        
    except Exception as e:
        log(f"âŒ å‚™ä»½æ‘˜è¦å‰µå»ºç•°å¸¸: {str(e)}")
    
    # 6. é¡¯ç¤ºå‚™ä»½çµæœ
    log("\n" + "="*60)
    log("ğŸ“‹ æ ¸å¿ƒæ•¸æ“šå‚™ä»½å®Œæˆæ‘˜è¦")
    log("="*60)
    log(f"ğŸ“ å‚™ä»½ä½ç½®: {essential_backup_dir.absolute()}")
    log(f"ğŸ“Š å‚™ä»½å¤§å°: {get_dir_size(essential_backup_dir)} MB")
    log("âœ… KOL è³‡æ–™åº«å’Œè²¼æ–‡ç”Ÿæˆç´€éŒ„å·²å®‰å…¨å‚™ä»½ï¼")
    log("â„¹ï¸ å·²æ’é™¤ Google Sheets ç›¸é—œæ•¸æ“š")
    log("="*60)

def get_dir_size(path):
    """è¨ˆç®—ç›®éŒ„å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for filename in filenames:
            filepath = os.path.join(dirpath, filename)
            if os.path.exists(filepath):
                total_size += os.path.getsize(filepath)
    return round(total_size / (1024 * 1024), 2)

if __name__ == "__main__":
    backup_essential_data()
