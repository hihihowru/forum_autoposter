#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è©³ç´° UGC åˆ†æå ±å‘Šç”Ÿæˆå™¨
åŸºæ–¼æ™ºèƒ½åˆ†æçµæœç”Ÿæˆå¯æ“ä½œçš„å»ºè­°
"""

import json
import os
from datetime import datetime

def generate_detailed_report():
    """ç”Ÿæˆè©³ç´°åˆ†æå ±å‘Š"""
    
    # æ‰¾åˆ°æœ€æ–°çš„åˆ†æçµæœæ–‡ä»¶
    result_files = [f for f in os.listdir('.') if f.startswith('ugc_analysis_results_')]
    if not result_files:
        print("âŒ æœªæ‰¾åˆ°åˆ†æçµæœæ–‡ä»¶")
        return
    
    latest_file = max(result_files)
    print(f"ğŸ“– è®€å–åˆ†æçµæœ: {latest_file}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("\n" + "="*80)
    print("ğŸ“Š è©³ç´° UGC åˆ†æå ±å‘Š")
    print("="*80)
    
    # 1. æ•¸æ“šæ¦‚è¦½
    total_titles = sum(result['title_count'] for result in data['files_analyzed'].values())
    print(f"\nğŸ“ˆ æ•¸æ“šæ¦‚è¦½:")
    print(f"   ç¸½æ¨™é¡Œæ•¸: {total_titles:,}")
    print(f"   åˆ†ææ–‡ä»¶æ•¸: {len(data['files_analyzed'])}")
    print(f"   åˆ†ææ™‚é–“: {data['analysis_timestamp']}")
    
    # 2. æ–‡ä»¶è©³ç´°ä¿¡æ¯
    print(f"\nğŸ“ æ–‡ä»¶è©³ç´°ä¿¡æ¯:")
    for file_key, file_data in data['files_analyzed'].items():
        print(f"   {file_key}: {file_data['title_count']:,} å€‹æ¨™é¡Œ")
    
    # 3. åˆä½µçµ±è¨ˆåˆ†æ
    combined = data['combined_statistics']
    
    print(f"\nğŸ¯ æ ¸å¿ƒç™¼ç¾:")
    
    # é•·åº¦åˆ†å¸ƒ
    if 'length_distribution' in combined:
        length_stats = combined['length_distribution']
        print(f"   å¹³å‡æ¨™é¡Œé•·åº¦: {length_stats['avg_length']:.1f} å­—")
        print(f"   ä¸­ä½æ•¸é•·åº¦: {length_stats['median_length']:.1f} å­—")
        print(f"   é•·åº¦åˆ†å¸ƒ:")
        print(f"     - çŸ­æ¨™é¡Œ (â‰¤10å­—): {length_stats['distribution']['short']/length_stats['total_count']*100:.1f}%")
        print(f"     - ä¸­æ¨™é¡Œ (11-20å­—): {length_stats['distribution']['medium']/length_stats['total_count']*100:.1f}%")
        print(f"     - é•·æ¨™é¡Œ (>20å­—): {length_stats['distribution']['long']/length_stats['total_count']*100:.1f}%")
    
    # è¡¨æƒ…ç¬¦è™Ÿ
    if 'emoji_analysis' in combined:
        emoji_stats = combined['emoji_analysis']
        print(f"   è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡: {emoji_stats['emoji_usage_rate']:.1f}%")
        print(f"   è¡¨æƒ…ç¬¦è™Ÿå¤šæ¨£æ€§: {emoji_stats['emoji_diversity']} ç¨®")
        print(f"   æœ€å¸¸è¦‹è¡¨æƒ…ç¬¦è™Ÿ:")
        for emoji, count in list(emoji_stats['top_emojis'].items())[:5]:
            print(f"     {emoji}: {count} æ¬¡")
    
    # è‚¡ç¥¨ä»£è™Ÿ
    if 'stock_codes' in combined:
        stock_stats = combined['stock_codes']
        print(f"   è‚¡ç¥¨ä»£è™Ÿä½¿ç”¨ç‡: {stock_stats['stock_usage_rate']:.1f}%")
        print(f"   è‚¡ç¥¨ä»£è™Ÿå¤šæ¨£æ€§: {stock_stats['stock_diversity']} ç¨®")
        print(f"   æœ€å¸¸æåˆ°çš„è‚¡ç¥¨:")
        for code, count in list(stock_stats['top_stocks'].items())[:5]:
            print(f"     {code}: {count} æ¬¡")
    
    # 4. è©³ç´°æ¨¡å¼åˆ†æ
    print(f"\nğŸ” è©³ç´°æ¨¡å¼åˆ†æ:")
    
    # æ™‚é–“æ¨¡å¼
    if 'time_patterns' in combined:
        time_stats = combined['time_patterns']
        print(f"   æ™‚é–“å…ƒç´ ä½¿ç”¨:")
        for pattern, stats in time_stats.items():
            print(f"     - {pattern}: {stats['rate']:.1f}%")
    
    # æ•¸å­—æ¨¡å¼
    if 'number_patterns' in combined:
        number_stats = combined['number_patterns']
        print(f"   æ•¸å­—é¡å‹ä½¿ç”¨:")
        for pattern, stats in number_stats.items():
            print(f"     - {pattern}: {stats['rate']:.1f}%")
    
    # ç†±é–€è©±é¡Œ
    if 'topic_analysis' in combined:
        topic_stats = combined['topic_analysis']
        print(f"   ç†±é–€è©±é¡Œåˆ†å¸ƒ:")
        sorted_topics = sorted(topic_stats.items(), key=lambda x: x[1]['rate'], reverse=True)
        for topic, stats in sorted_topics[:5]:
            print(f"     - {topic}: {stats['rate']:.1f}%")
    
    # æƒ…æ„Ÿåˆ†æ
    if 'emotion_analysis' in combined:
        emotion_stats = combined['emotion_analysis']
        print(f"   æƒ…æ„Ÿå¼·åº¦åˆ†å¸ƒ:")
        for emotion, stats in emotion_stats.items():
            print(f"     - {emotion}: {stats['rate']:.1f}%")
    
    # äº’å‹•æ¨¡å¼
    if 'interaction_patterns' in combined:
        interaction_stats = combined['interaction_patterns']
        print(f"   äº’å‹•æ¨¡å¼ä½¿ç”¨:")
        for pattern, stats in interaction_stats.items():
            print(f"     - {pattern}: {stats['rate']:.1f}%")
    
    # å°ˆæ¥­è¡“èª
    if 'professional_terms' in combined:
        professional_stats = combined['professional_terms']
        print(f"   å°ˆæ¥­è¡“èªä½¿ç”¨:")
        for term, stats in professional_stats.items():
            print(f"     - {term}: {stats['rate']:.1f}%")
    
    # 5. AI æ¨™é¡Œç”Ÿæˆå»ºè­°
    print(f"\nğŸ’¡ AI æ¨™é¡Œç”Ÿæˆæ™ºèƒ½å»ºè­°:")
    
    recommendations = []
    
    # åŸºæ–¼é•·åº¦åˆ†å¸ƒ
    if 'length_distribution' in combined:
        length_stats = combined['length_distribution']
        medium_rate = length_stats['distribution']['medium'] / length_stats['total_count'] * 100
        recommendations.append(f"1. é‡é»ç”Ÿæˆ {medium_rate:.1f}% çš„ 11-20 å­—ä¸­ç­‰é•·åº¦æ¨™é¡Œ")
        recommendations.append(f"2. é©åº¦ç”ŸæˆçŸ­æ¨™é¡Œ ({length_stats['distribution']['short']/length_stats['total_count']*100:.1f}%) å’Œé•·æ¨™é¡Œ ({length_stats['distribution']['long']/length_stats['total_count']*100:.1f}%)")
    
    # åŸºæ–¼è¡¨æƒ…ç¬¦è™Ÿ
    if 'emoji_analysis' in combined:
        emoji_stats = combined['emoji_analysis']
        recommendations.append(f"3. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç›®æ¨™ä½¿ç”¨ç‡ {emoji_stats['emoji_usage_rate']:.1f}%")
        top_emojis = list(emoji_stats['top_emojis'].keys())[:3]
        recommendations.append(f"4. å„ªå…ˆä½¿ç”¨å¸¸è¦‹è¡¨æƒ…ç¬¦è™Ÿ: {', '.join(top_emojis)}")
    
    # åŸºæ–¼äº’å‹•æ¨¡å¼
    if 'interaction_patterns' in combined:
        interaction_stats = combined['interaction_patterns']
        exclamation_rate = interaction_stats['exclamation']['rate']
        question_rate = interaction_stats['question']['rate']
        recommendations.append(f"5. å¢åŠ äº’å‹•æ€§ï¼šæ„Ÿå˜†å¥ {exclamation_rate:.1f}%ï¼Œå•å¥ {question_rate:.1f}%")
    
    # åŸºæ–¼å°ˆæ¥­è¡“èª
    if 'professional_terms' in combined:
        professional_stats = combined['professional_terms']
        fundamental_rate = professional_stats['fundamental']['rate']
        technical_rate = professional_stats['technical']['rate']
        sentiment_rate = professional_stats['sentiment']['rate']
        recommendations.append(f"6. èå…¥å°ˆæ¥­è¡“èªï¼šåŸºæœ¬é¢ {fundamental_rate:.1f}%ï¼ŒæŠ€è¡“é¢ {technical_rate:.1f}%ï¼Œç±Œç¢¼é¢ {sentiment_rate:.1f}%")
    
    # åŸºæ–¼ç†±é–€è©±é¡Œ
    if 'topic_analysis' in combined:
        topic_stats = combined['topic_analysis']
        sorted_topics = sorted(topic_stats.items(), key=lambda x: x[1]['rate'], reverse=True)
        top_topic = sorted_topics[0]
        recommendations.append(f"7. é—œæ³¨ç†±é–€è©±é¡Œï¼š{top_topic[0]} ({top_topic[1]['rate']:.1f}%)")
    
    # åŸºæ–¼è‚¡ç¥¨ä»£è™Ÿ
    if 'stock_codes' in combined:
        stock_stats = combined['stock_codes']
        recommendations.append(f"8. é©åº¦ä½¿ç”¨è‚¡ç¥¨ä»£è™Ÿï¼Œç›®æ¨™ä½¿ç”¨ç‡ {stock_stats['stock_usage_rate']:.1f}%")
    
    for rec in recommendations:
        print(f"   {rec}")
    
    # 6. KOL æ“´å±•å»ºè­°
    print(f"\nğŸ‘¥ KOL æ“´å±•å»ºè­°:")
    
    kol_recommendations = [
        "1. åŸºæ–¼æƒ…æ„Ÿå¼·åº¦åˆ†å¸ƒè¨­è¨ˆä¸åŒ KOL æ€§æ ¼ï¼ˆæ­£é¢ã€è² é¢ã€ä¸­æ€§ï¼‰",
        "2. æ ¹æ“šäº’å‹•æ¨¡å¼å·®ç•°åŒ– KOL è¡¨é”æ–¹å¼ï¼ˆå•å¥ã€æ„Ÿå˜†å¥ã€æŒ‡ä»¤å¥ï¼‰",
        "3. åˆ©ç”¨å°ˆæ¥­è¡“èªåå¥½å€åˆ† KOL å°ˆæ¥­é ˜åŸŸï¼ˆåŸºæœ¬é¢ã€æŠ€è¡“é¢ã€ç±Œç¢¼é¢ï¼‰",
        "4. çµåˆç†±é–€è©±é¡Œè¨­è¨ˆ KOL å°ˆé•·é ˜åŸŸï¼ˆAIã€åŠå°é«”ã€èˆªé‹ç­‰ï¼‰",
        "5. åƒè€ƒè¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç¿’æ…£è¨­è¨ˆ KOL é¢¨æ ¼ï¼ˆæ´»æ½‘ã€å°ˆæ¥­ã€å¹½é»˜ï¼‰",
        "6. æ ¹æ“šæ¨™é¡Œé•·åº¦åå¥½è¨­è¨ˆ KOL è¡¨é”ç¿’æ…£ï¼ˆç°¡æ½”ã€è©³ç´°ã€å¹³è¡¡ï¼‰",
        "7. åˆ©ç”¨æ™‚é–“å…ƒç´ ä½¿ç”¨ç¿’æ…£è¨­è¨ˆ KOL æ™‚æ•ˆæ€§åå¥½",
        "8. çµåˆæ•¸å­—ä½¿ç”¨ç¿’æ…£è¨­è¨ˆ KOL æ•¸æ“šåå¥½ï¼ˆåƒ¹æ ¼ã€ç™¾åˆ†æ¯”ã€å¼µæ•¸ï¼‰"
    ]
    
    for rec in kol_recommendations:
        print(f"   {rec}")
    
    # 7. å¯¦éš›æ‡‰ç”¨å»ºè­°
    print(f"\nğŸ¯ å¯¦éš›æ‡‰ç”¨å»ºè­°:")
    
    application_suggestions = [
        "1. èª¿æ•´ AI æ¨™é¡Œç”Ÿæˆç­–ç•¥ï¼Œé‡é»é—œæ³¨ 11-20 å­—ä¸­ç­‰é•·åº¦",
        "2. å¢åŠ æ„Ÿå˜†å¥å’Œå•å¥æ¯”ä¾‹ï¼Œæå‡äº’å‹•æ€§",
        "3. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç‰¹åˆ¥æ˜¯ ğŸ“ˆğŸš€ğŸ”¥ ç­‰æŠ•è³‡ç›¸é—œè¡¨æƒ…",
        "4. èå…¥å°ˆæ¥­è¡“èªï¼Œæå‡å¯ä¿¡åº¦å’Œå°ˆæ¥­åº¦",
        "5. é—œæ³¨ç†±é–€è©±é¡Œï¼Œå¢åŠ æ™‚æ•ˆæ€§å’Œç›¸é—œæ€§",
        "6. é©åº¦ä½¿ç”¨è‚¡ç¥¨ä»£è™Ÿå’Œå…·é«”æ•¸å­—ï¼Œå¢åŠ å…·é«”æ€§",
        "7. å¹³è¡¡æƒ…æ„Ÿè¡¨é”ï¼Œé¿å…éåº¦æ¥µç«¯",
        "8. ç‚ºä¸åŒ KOL è¨­è¨ˆå·®ç•°åŒ–çš„è¡¨é”é¢¨æ ¼"
    ]
    
    for suggestion in application_suggestions:
        print(f"   {suggestion}")
    
    # 8. æ•¸æ“šè³ªé‡è©•ä¼°
    print(f"\nğŸ“Š æ•¸æ“šè³ªé‡è©•ä¼°:")
    
    quality_metrics = []
    
    if 'length_distribution' in combined:
        length_stats = combined['length_distribution']
        if length_stats['avg_length'] > 30:
            quality_metrics.append("âš ï¸ å¹³å‡æ¨™é¡Œé•·åº¦è¼ƒé•·ï¼Œå¯èƒ½éœ€è¦ç°¡åŒ–")
        else:
            quality_metrics.append("âœ… æ¨™é¡Œé•·åº¦åˆ†å¸ƒåˆç†")
    
    if 'emoji_analysis' in combined:
        emoji_stats = combined['emoji_analysis']
        if emoji_stats['emoji_usage_rate'] < 1:
            quality_metrics.append("âš ï¸ è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡è¼ƒä½ï¼Œå¯é©ç•¶å¢åŠ ")
        else:
            quality_metrics.append("âœ… è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨é©åº¦")
    
    if 'stock_codes' in combined:
        stock_stats = combined['stock_codes']
        if stock_stats['stock_usage_rate'] < 2:
            quality_metrics.append("âš ï¸ è‚¡ç¥¨ä»£è™Ÿä½¿ç”¨ç‡è¼ƒä½ï¼Œå¯é©ç•¶å¢åŠ ")
        else:
            quality_metrics.append("âœ… è‚¡ç¥¨ä»£è™Ÿä½¿ç”¨é©åº¦")
    
    for metric in quality_metrics:
        print(f"   {metric}")
    
    print(f"\nâœ… è©³ç´°åˆ†æå ±å‘Šç”Ÿæˆå®Œæˆï¼")
    print(f"ğŸ“ åŸå§‹æ•¸æ“šæ–‡ä»¶: {latest_file}")

if __name__ == "__main__":
    generate_detailed_report()
