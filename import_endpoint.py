# Add this endpoint to the unified API main.py file

@app.post("/admin/import-1788-posts")
async def import_1788_posts():
    """å°å…¥ 1788 ç­† post_records æ•¸æ“šï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        # è®€å– JSON æ•¸æ“šæ–‡ä»¶
        json_file_path = '/app/post_records_1788.json'
        if not os.path.exists(json_file_path):
            return {"error": "post_records_1788.json æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
        
        logger.info(f"ğŸ“Š å¾ JSON æ–‡ä»¶åŠ è¼‰ {len(records)} ç­†è¨˜éŒ„")
        
        with db_connection.cursor() as cursor:
            # æ¸…ç©ºç¾æœ‰æ•¸æ“š
            cursor.execute("DELETE FROM post_records")
            logger.info("ğŸ—‘ï¸ æ¸…ç©ºç¾æœ‰æ•¸æ“š")
            
            # æ‰¹é‡æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            # è½‰æ›è¨˜éŒ„æ ¼å¼
            records_dict = []
            for record in records:
                record_dict = dict(record)
                
                # è™•ç† datetime å­—ç¬¦ä¸²
                for datetime_field in ['created_at', 'updated_at', 'approved_at', 'scheduled_at', 'published_at']:
                    if record_dict.get(datetime_field):
                        if isinstance(record_dict[datetime_field], str):
                            try:
                                record_dict[datetime_field] = datetime.fromisoformat(record_dict[datetime_field].replace('Z', '+00:00'))
                            except:
                                record_dict[datetime_field] = None
                        elif not isinstance(record_dict[datetime_field], datetime):
                            record_dict[datetime_field] = None
                    else:
                        record_dict[datetime_field] = None
                
                # è™•ç† JSON å­—æ®µ - ç¢ºä¿æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                for json_field in ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']:
                    if record_dict.get(json_field):
                        if isinstance(record_dict[json_field], (dict, list)):
                            # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½‰æ›ç‚º JSON å­—ç¬¦ä¸²
                            try:
                                record_dict[json_field] = json.dumps(record_dict[json_field], ensure_ascii=False)
                            except:
                                record_dict[json_field] = None
                        elif isinstance(record_dict[json_field], str):
                            # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸è®Š
                            pass
                        else:
                            record_dict[json_field] = None
                    else:
                        record_dict[json_field] = None
                
                records_dict.append(record_dict)
            
            # æ‰¹é‡æ’å…¥
            logger.info(f"ğŸ“¥ é–‹å§‹æ’å…¥ {len(records_dict)} ç­†è¨˜éŒ„...")
            cursor.executemany(insert_sql, records_dict)
            db_connection.commit()
            
            logger.info(f"âœ… æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„")
            
            # é©—è­‰å°å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"âŒ å°å…¥ 1788 ç­†è¨˜éŒ„å¤±æ•—: {e}")
        return {"error": str(e)}
