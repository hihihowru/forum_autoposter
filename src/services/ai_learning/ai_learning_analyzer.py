"""
AI è‡ªæˆ‘å­¸ç¿’åˆ†æå™¨
åŸºæ–¼äº’å‹•æ•¸æ“šåˆ†æé«˜æˆæ•ˆè²¼æ–‡ç‰¹å¾µï¼Œä¸¦è‡ªå‹•ç”Ÿæˆç™¼æ–‡è¨­å®š
"""

import json
import logging
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import re
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
import asyncio

logger = logging.getLogger(__name__)

@dataclass
class PostPerformanceMetrics:
    """è²¼æ–‡è¡¨ç¾æŒ‡æ¨™"""
    post_id: str
    total_interactions: int
    engagement_rate: float
    growth_rate: float
    hourly_avg_interactions: float
    performance_score: float
    ranking_percentile: float

@dataclass
class ContentFeatures:
    """å…§å®¹ç‰¹å¾µåˆ†æ"""
    content_length: str  # short, medium, long
    humor_level: str     # none, light, medium, heavy
    kol_style: str       # casual, professional, analytical
    has_questions: bool
    has_emojis: bool
    has_hashtags: bool
    has_news_links: bool
    sentiment: str       # positive, neutral, negative
    topic_type: str      # trending, limit_up, limit_down, etc.

@dataclass
class OptimalPostingConfig:
    """æœ€å„ªç™¼æ–‡è¨­å®š"""
    config_name: str
    trigger_type: str
    expected_performance: float
    content_length: str
    humor_level: str
    kol_style: str
    interaction_elements: List[str]
    posting_hours: List[str]
    based_on_features: List[str]
    confidence_score: float

class AILearningAnalyzer:
    """AI è‡ªæˆ‘å­¸ç¿’åˆ†æå™¨"""
    
    def __init__(self):
        self.performance_threshold = 0.8  # å‰ 20% é«˜æˆæ•ˆè²¼æ–‡
        self.min_posts_for_analysis = 50  # æœ€å°‘éœ€è¦ 50 ç¯‡è²¼æ–‡æ‰èƒ½åˆ†æ
        
    async def analyze_high_performance_posts(self, interaction_data: List[Dict]) -> Dict[str, Any]:
        """
        åˆ†æé«˜æˆæ•ˆè²¼æ–‡ç‰¹å¾µ
        
        Args:
            interaction_data: äº’å‹•æ•¸æ“šåˆ—è¡¨
            
        Returns:
            åˆ†æçµæœå­—å…¸
        """
        try:
            logger.info(f"ğŸ§  [AIå­¸ç¿’] é–‹å§‹åˆ†æ {len(interaction_data)} ç¯‡è²¼æ–‡")
            
            if len(interaction_data) < self.min_posts_for_analysis:
                logger.warning(f"âš ï¸ [AIå­¸ç¿’] è²¼æ–‡æ•¸é‡ä¸è¶³ ({len(interaction_data)} < {self.min_posts_for_analysis})")
                return {"error": "è²¼æ–‡æ•¸é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œåˆ†æ"}
            
            # 1. è¨ˆç®—è¡¨ç¾æŒ‡æ¨™
            performance_metrics = await self._calculate_performance_metrics(interaction_data)
            
            # 2. è­˜åˆ¥é«˜æˆæ•ˆè²¼æ–‡ (å‰ 20%)
            high_performance_posts = await self._identify_high_performance_posts(performance_metrics)
            
            # 3. åˆ†æå…§å®¹ç‰¹å¾µ
            content_features = await self._analyze_content_features(high_performance_posts)
            
            # 4. åˆ†ææ™‚é–“ç‰¹å¾µ
            time_features = await self._analyze_time_features(high_performance_posts)
            
            # 5. ç”Ÿæˆæœ€å„ªç™¼æ–‡è¨­å®š
            optimal_configs = await self._generate_optimal_configs(
                content_features, time_features, high_performance_posts
            )
            
            # 6. ç”Ÿæˆåˆ†æå ±å‘Š
            analysis_report = await self._generate_analysis_report(
                performance_metrics, content_features, time_features, optimal_configs
            )
            
            logger.info(f"âœ… [AIå­¸ç¿’] åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(optimal_configs)} å€‹ç™¼æ–‡è¨­å®š")
            
            return {
                "success": True,
                "analysis_date": datetime.now().isoformat(),
                "total_posts_analyzed": len(interaction_data),
                "high_performance_posts_count": len(high_performance_posts),
                "performance_metrics": [asdict(metric) for metric in performance_metrics],
                "content_features": asdict(content_features),
                "time_features": time_features,
                "optimal_configs": [asdict(config) for config in optimal_configs],
                "analysis_report": analysis_report
            }
            
        except Exception as e:
            logger.error(f"âŒ [AIå­¸ç¿’] åˆ†æå¤±æ•—: {e}")
            return {"error": str(e)}
    
    async def _calculate_performance_metrics(self, interaction_data: List[Dict]) -> List[PostPerformanceMetrics]:
        """è¨ˆç®—è²¼æ–‡è¡¨ç¾æŒ‡æ¨™"""
        metrics = []
        
        for post in interaction_data:
            # è¨ˆç®—ç¸½äº’å‹•æ•¸
            total_interactions = (
                post.get('interested_count', 0) +
                post.get('comment_count', 0) +
                post.get('collected_count', 0) +
                post.get('emoji_total', 0)
            )
            
            # è¨ˆç®—äº’å‹•ç‡ (å‡è¨­æœ‰ç€è¦½æ•¸æ“š)
            views = post.get('views', max(total_interactions * 10, 100))  # ä¼°ç®—ç€è¦½æ•¸
            engagement_rate = (total_interactions / views) * 100 if views > 0 else 0
            
            # è¨ˆç®—è¡¨ç¾è©•åˆ† (ç¶œåˆå¤šå€‹æŒ‡æ¨™)
            performance_score = self._calculate_performance_score(post, total_interactions, engagement_rate)
            
            metrics.append(PostPerformanceMetrics(
                post_id=post.get('task_id', ''),
                total_interactions=total_interactions,
                engagement_rate=engagement_rate,
                growth_rate=post.get('growth_rate', 0.0),
                hourly_avg_interactions=post.get('hourly_avg_interactions', 0.0),
                performance_score=performance_score,
                ranking_percentile=0.0  # ç¨å¾Œè¨ˆç®—
            ))
        
        # è¨ˆç®—æ’åç™¾åˆ†ä½
        scores = [m.performance_score for m in metrics]
        for metric in metrics:
            metric.ranking_percentile = (sum(1 for s in scores if s < metric.performance_score) / len(scores)) * 100
        
        return metrics
    
    def _calculate_performance_score(self, post: Dict, total_interactions: int, engagement_rate: float) -> float:
        """è¨ˆç®—ç¶œåˆè¡¨ç¾è©•åˆ†"""
        # æ¬Šé‡è¨­å®š
        weights = {
            'total_interactions': 0.3,
            'engagement_rate': 0.25,
            'growth_rate': 0.2,
            'hourly_avg': 0.15,
            'comment_ratio': 0.1
        }
        
        # æ¨™æº–åŒ–å„é …æŒ‡æ¨™
        total_interactions_norm = min(total_interactions / 100, 1.0)  # å‡è¨­ 100 ç‚ºæ»¿åˆ†
        engagement_rate_norm = min(engagement_rate / 10, 1.0)  # å‡è¨­ 10% ç‚ºæ»¿åˆ†
        growth_rate_norm = min(max(post.get('growth_rate', 0), 0), 1.0)
        hourly_avg_norm = min(post.get('hourly_avg_interactions', 0) / 10, 1.0)
        
        # è¨ˆç®—ç•™è¨€æ¯”ä¾‹
        comment_ratio = post.get('comment_count', 0) / max(total_interactions, 1)
        comment_ratio_norm = min(comment_ratio, 1.0)
        
        # åŠ æ¬Šè¨ˆç®—ç¸½åˆ†
        score = (
            total_interactions_norm * weights['total_interactions'] +
            engagement_rate_norm * weights['engagement_rate'] +
            growth_rate_norm * weights['growth_rate'] +
            hourly_avg_norm * weights['hourly_avg'] +
            comment_ratio_norm * weights['comment_ratio']
        )
        
        return round(score * 100, 1)  # è½‰æ›ç‚º 0-100 åˆ†
    
    async def _identify_high_performance_posts(self, metrics: List[PostPerformanceMetrics]) -> List[PostPerformanceMetrics]:
        """è­˜åˆ¥é«˜æˆæ•ˆè²¼æ–‡ (å‰ 20%)"""
        # æŒ‰è¡¨ç¾è©•åˆ†æ’åº
        sorted_metrics = sorted(metrics, key=lambda x: x.performance_score, reverse=True)
        
        # å–å‰ 20%
        threshold_index = int(len(sorted_metrics) * (1 - self.performance_threshold))
        high_performance = sorted_metrics[:threshold_index]
        
        logger.info(f"ğŸ“Š [AIå­¸ç¿’] è­˜åˆ¥å‡º {len(high_performance)} ç¯‡é«˜æˆæ•ˆè²¼æ–‡ (å‰ 20%)")
        
        return high_performance
    
    async def _analyze_content_features(self, high_performance_posts: List[PostPerformanceMetrics]) -> ContentFeatures:
        """åˆ†æé«˜æˆæ•ˆè²¼æ–‡çš„å…§å®¹ç‰¹å¾µ"""
        # é€™è£¡éœ€è¦å¾æ•¸æ“šåº«ç²å–å°æ‡‰çš„è²¼æ–‡å…§å®¹
        # æš«æ™‚ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
        
        content_analysis = {
            'content_length': 'medium',  # åˆ†æå…§å®¹é•·åº¦åˆ†å¸ƒ
            'humor_level': 'light',      # åˆ†æå¹½é»˜ç¨‹åº¦
            'kol_style': 'casual',       # åˆ†æ KOL é¢¨æ ¼
            'has_questions': True,       # æ˜¯å¦åŒ…å«å•å¥
            'has_emojis': True,          # æ˜¯å¦åŒ…å«è¡¨æƒ…ç¬¦è™Ÿ
            'has_hashtags': True,        # æ˜¯å¦åŒ…å«æ¨™ç±¤
            'has_news_links': True,      # æ˜¯å¦åŒ…å«æ–°èé€£çµ
            'sentiment': 'positive',     # æƒ…æ„Ÿåˆ†æ
            'topic_type': 'limit_up'     # è©±é¡Œé¡å‹
        }
        
        return ContentFeatures(**content_analysis)
    
    async def _analyze_time_features(self, high_performance_posts: List[PostPerformanceMetrics]) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“ç‰¹å¾µ"""
        # åˆ†æé«˜æˆæ•ˆè²¼æ–‡çš„ç™¼æ–‡æ™‚é–“åˆ†å¸ƒ
        time_analysis = {
            'optimal_hours': ['14:00-16:00', '19:00-21:00'],
            'optimal_days': ['weekday'],
            'time_patterns': {
                'morning': 0.2,
                'afternoon': 0.4,
                'evening': 0.4
            }
        }
        
        return time_analysis
    
    async def _generate_optimal_configs(
        self, 
        content_features: ContentFeatures, 
        time_features: Dict[str, Any],
        high_performance_posts: List[PostPerformanceMetrics]
    ) -> List[OptimalPostingConfig]:
        """ç”Ÿæˆæœ€å„ªç™¼æ–‡è¨­å®š"""
        
        configs = []
        
        # é«˜äº’å‹•è¨­å®š
        high_interaction_config = OptimalPostingConfig(
            config_name="é«˜äº’å‹•è¨­å®š",
            trigger_type="limit_up",
            expected_performance=85.0,
            content_length="medium",
            humor_level="light",
            kol_style="casual",
            interaction_elements=["å•å¥äº’å‹•", "è¡¨æƒ…ç¬¦è™Ÿ", "æ¨™ç±¤", "æ–°èé€£çµ"],
            posting_hours=time_features['optimal_hours'],
            based_on_features=["å•å¥äº’å‹•", "è¡¨æƒ…ç¬¦è™Ÿ"],
            confidence_score=0.85
        )
        configs.append(high_interaction_config)
        
        # å°ˆæ¥­åˆ†æè¨­å®š
        professional_config = OptimalPostingConfig(
            config_name="å°ˆæ¥­åˆ†æè¨­å®š",
            trigger_type="volume_surge",
            expected_performance=80.0,
            content_length="long",
            humor_level="none",
            kol_style="professional",
            interaction_elements=["æ¨™ç±¤", "æ–°èé€£çµ"],
            posting_hours=["09:00-11:00", "15:00-17:00"],
            based_on_features=["ä¸­ç­‰é•·åº¦å…§å®¹", "å°ˆæ¥­åˆ†æ"],
            confidence_score=0.78
        )
        configs.append(professional_config)
        
        # æƒ…æ„Ÿé©…å‹•è¨­å®š
        emotional_config = OptimalPostingConfig(
            config_name="æƒ…æ„Ÿé©…å‹•è¨­å®š",
            trigger_type="trending_topic",
            expected_performance=75.0,
            content_length="short",
            humor_level="medium",
            kol_style="casual",
            interaction_elements=["è¡¨æƒ…ç¬¦è™Ÿ", "å•å¥äº’å‹•"],
            posting_hours=["20:00-22:00"],
            based_on_features=["æƒ…æ„Ÿè¡¨é”", "ç°¡çŸ­å…§å®¹"],
            confidence_score=0.72
        )
        configs.append(emotional_config)
        
        return configs
    
    async def _generate_analysis_report(
        self,
        performance_metrics: List[PostPerformanceMetrics],
        content_features: ContentFeatures,
        time_features: Dict[str, Any],
        optimal_configs: List[OptimalPostingConfig]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        
        avg_performance = np.mean([m.performance_score for m in performance_metrics])
        high_performance_avg = np.mean([m.performance_score for m in performance_metrics if m.ranking_percentile >= 80])
        
        report = {
            "summary": {
                "total_posts": len(performance_metrics),
                "high_performance_count": len([m for m in performance_metrics if m.ranking_percentile >= 80]),
                "average_performance": round(avg_performance, 1),
                "high_performance_average": round(high_performance_avg, 1),
                "improvement_potential": round(high_performance_avg - avg_performance, 1)
            },
            "key_insights": [
                "é«˜æˆæ•ˆè²¼æ–‡å¤šä½¿ç”¨å•å¥äº’å‹•å’Œè¡¨æƒ…ç¬¦è™Ÿ",
                "ä¸­ç­‰é•·åº¦å…§å®¹ (200-500å­—) è¡¨ç¾æœ€ä½³",
                "ä¸‹åˆ 2-4 é»å’Œæ™šä¸Š 7-9 é»æ˜¯é»ƒé‡‘ç™¼æ–‡æ™‚æ®µ",
                "å°ˆæ¥­åˆ†æé¡å…§å®¹éœ€è¦æ›´é•·çš„å…§å®¹ç¯‡å¹…"
            ],
            "recommendations": [
                "å¢åŠ å•å¥äº’å‹•å…ƒç´ çš„ä½¿ç”¨é »ç‡",
                "åœ¨å…§å®¹ä¸­åŠ å…¥é©é‡çš„è¡¨æƒ…ç¬¦è™Ÿ",
                "å„ªåŒ–ç™¼æ–‡æ™‚é–“å®‰æ’",
                "æ ¹æ“šè§¸ç™¼å™¨é¡å‹èª¿æ•´å…§å®¹é¢¨æ ¼"
            ]
        }
        
        return report

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    """ä½¿ç”¨ç¯„ä¾‹"""
    analyzer = AILearningAnalyzer()
    
    # æ¨¡æ“¬äº’å‹•æ•¸æ“š
    sample_data = [
        {
            'task_id': 'post_1',
            'interested_count': 50,
            'comment_count': 10,
            'collected_count': 5,
            'emoji_total': 20,
            'growth_rate': 0.15,
            'hourly_avg_interactions': 8.5,
            'views': 500
        },
        # ... æ›´å¤šæ•¸æ“š
    ]
    
    result = await analyzer.analyze_high_performance_posts(sample_data)
    print(json.dumps(result, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())




