#!/usr/bin/env python3
"""
Serper API å®¢æˆ¶ç«¯
ç”¨æ–¼æœç´¢æ–°èå’Œç›¸é—œè³‡è¨Š
"""

import os
import aiohttp
import asyncio
import logging
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class NewsResult:
    """æ–°èçµæœçµæ§‹"""
    title: str
    link: str
    snippet: str
    source: str
    date: str
    relevance_score: float

@dataclass
class SearchResult:
    """æœç´¢çµæœçµæ§‹"""
    query: str
    total_results: int
    news_results: List[NewsResult]
    organic_results: List[Dict[str, Any]]
    search_time: datetime

class SerperAPIClient:
    """Serper API å®¢æˆ¶ç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ– Serper API å®¢æˆ¶ç«¯"""
        self.api_key = os.getenv('SERPER_API_KEY')
        if not self.api_key:
            raise ValueError("ç¼ºå°‘ SERPER_API_KEY ç’°å¢ƒè®Šæ•¸")
        
        self.base_url = "https://google.serper.dev/search"
        self.session = None
        
        # å¯ä¿¡è²¡ç¶“åª’é«”ç¶²ç«™åˆ—è¡¨
        self.trusted_sites = [
            # ğŸ“° ä¸»æµè²¡ç¶“æ–°èåª’é«”
            "udn.com",
            "ctee.com.tw", 
            "money.udn.com",
            "ec.ltn.com.tw",
            "www.chinatimes.com",
            "www.cnyes.com",
            "news.cnyes.com",
            "tw.stock.yahoo.com",
            "www.businesstoday.com.tw",
            "www.businessweekly.com.tw",
            
            # ğŸ“Š è²¡ç¶“æ•¸æ“š / æŠ•è³‡ç ”ç©¶
            "www.cmoney.tw",
            "statementdog.com",
            "www.macromicro.me",
            "histock.tw",
            "www.wantgoo.com",
            
            # ğŸ¦ å®˜æ–¹èˆ‡äº¤æ˜“æ‰€
            "www.twse.com.tw",
            "www.tpex.org.tw",
            "www.taifex.com.tw",
            "mops.twse.com.tw",
            
            # ğŸ“º è²¡ç¶“é›»è¦– / æ–°èå»¶ä¼¸
            "moneydj.com",
            "www.setn.com/money",
            "news.tvbs.com.tw/money",
            "news.ltn.com.tw/list/business",
            "www.storm.mg/business"
        ]
        
        logger.info("Serper API å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    async def __aenter__(self):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ç•°æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def search_news(self, query: str, num_results: int = 10) -> Optional[SearchResult]:
        """æœç´¢æ–°è"""
        try:
            headers = {
                'X-API-KEY': self.api_key,
                'Content-Type': 'application/json'
            }
            
            # æ§‹å»ºæŸ¥è©¢ï¼ˆä½¿ç”¨å¯ä¿¡åª’é«”é™åˆ¶ï¼‰
            site_restrictions = " OR ".join([f"site:{site}" for site in self.trusted_sites])
            enhanced_query = f"({query}) AND ({site_restrictions})"
            
            payload = {
                'q': enhanced_query,
                'num': num_results * 3,  # å¢åŠ æœç´¢æ•¸é‡ä»¥å½Œè£œç¶²ç«™é™åˆ¶å’Œç´¢å¼•å»¶é²
                'gl': 'tw',  # å°ç£åœ°å€
                'hl': 'zh-TW',  # ç¹é«”ä¸­æ–‡
                'type': 'news',  # æ–°èæœç´¢
                'sort': 'date'  # æŒ‰æ—¥æœŸæ’åºï¼Œå„ªå…ˆæœ€æ–°
            }
            
            async with self.session.post(self.base_url, json=payload, headers=headers) as response:
                # èª¿è©¦ï¼šè¨˜éŒ„è«‹æ±‚è©³æƒ…
                logger.debug(f"Serper API è«‹æ±‚: {json.dumps(payload, ensure_ascii=False, indent=2)}")
                logger.debug(f"Serper API éŸ¿æ‡‰ç‹€æ…‹: {response.status}")
                
                if response.status == 200:
                    data = await response.json()
                    
                    # èª¿è©¦ï¼šè¨˜éŒ„ API éŸ¿æ‡‰
                    logger.debug(f"Serper API éŸ¿æ‡‰: {json.dumps(data, ensure_ascii=False, indent=2)}")
                    
                    news_results = []
                    if 'newsResults' in data:
                        for news in data['newsResults']:
                            # æª¢æŸ¥æ˜¯å¦ä¾†è‡ªå¯ä¿¡ç¶²ç«™
                            if self._is_trusted_source(news.get('link', '')):
                                news_results.append(NewsResult(
                                    title=news.get('title', ''),
                                    link=news.get('link', ''),
                                    snippet=news.get('snippet', ''),
                                    source=news.get('source', ''),
                                    date=news.get('date', ''),
                                    relevance_score=news.get('relevance_score', 0.0)
                                ))
                    elif 'news' in data:
                        for news in data['news']:
                            # æª¢æŸ¥æ˜¯å¦ä¾†è‡ªå¯ä¿¡ç¶²ç«™
                            if self._is_trusted_source(news.get('link', '')):
                                news_results.append(NewsResult(
                                    title=news.get('title', ''),
                                    link=news.get('link', ''),
                                    snippet=news.get('snippet', ''),
                                    source=news.get('source', ''),
                                    date=news.get('date', ''),
                                    relevance_score=news.get('relevance_score', 0.0)
                                ))
                    else:
                        logger.warning(f"API éŸ¿æ‡‰ä¸­æ²’æœ‰ newsResults æˆ– news: {list(data.keys())}")
                    
                    # æŒ‰æ—¥æœŸå’Œç›¸é—œæ€§æ’åº
                    news_results = self._sort_news_by_relevance_and_date(news_results)
                    
                    # é™åˆ¶çµæœæ•¸é‡
                    news_results = news_results[:num_results]
                    
                    organic_results = data.get('organic', [])
                    
                    return SearchResult(
                        query=query,
                        total_results=len(news_results) + len(organic_results),
                        news_results=news_results,
                        organic_results=organic_results,
                        search_time=datetime.now()
                    )
                
                # è¨˜éŒ„éŒ¯èª¤éŸ¿æ‡‰è©³æƒ…
                error_text = await response.text()
                logger.error(f"Serper API æœç´¢å¤±æ•—: {response.status}")
                logger.error(f"éŒ¯èª¤éŸ¿æ‡‰: {error_text}")
                return None
                
        except Exception as e:
            logger.error(f"Serper API æœç´¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def _is_trusted_source(self, url: str) -> bool:
        """æª¢æŸ¥ URL æ˜¯å¦ä¾†è‡ªå¯ä¿¡ä¾†æº"""
        if not url:
            return False
        
        url_lower = url.lower()
        return any(site in url_lower for site in self.trusted_sites)
    
    def _sort_news_by_relevance_and_date(self, news_results: List[NewsResult]) -> List[NewsResult]:
        """æŒ‰ç›¸é—œæ€§å’Œæ—¥æœŸæ’åºæ–°è"""
        def sort_key(news):
            # å„ªå…ˆè€ƒæ…®ç›¸é—œæ€§åˆ†æ•¸
            relevance_score = news.relevance_score or 0.0
            
            # æ—¥æœŸåŠ åˆ†ï¼šè¶Šæ–°è¶Šå¥½
            date_bonus = 0.0
            if news.date:
                try:
                    # å˜—è©¦è§£ææ—¥æœŸ
                    if 'å°æ™‚å‰' in news.date or 'åˆ†é˜å‰' in news.date:
                        date_bonus = 1.0
                    elif 'å¤©å‰' in news.date:
                        days_ago = int(news.date.split('å¤©å‰')[0])
                        if days_ago <= 1:
                            date_bonus = 0.8
                        elif days_ago <= 3:
                            date_bonus = 0.6
                        elif days_ago <= 7:
                            date_bonus = 0.4
                        else:
                            date_bonus = 0.2
                    else:
                        # å…¶ä»–æ—¥æœŸæ ¼å¼ï¼Œçµ¦äºˆä¸­ç­‰åˆ†æ•¸
                        date_bonus = 0.5
                except:
                    date_bonus = 0.3
            
            # å…§å®¹ç›¸é—œæ€§åŠ åˆ†
            content_bonus = 0.0
            title_lower = news.title.lower()
            snippet_lower = news.snippet.lower()
            
            # è‚¡åƒ¹è®ŠåŒ–ç›¸é—œé—œéµè©
            price_change_keywords = [
                'æ¼²åœ', 'ä¸Šæ¼²', 'å¤§æ¼²', 'é£†æ¼²', 'çªç ´', 'å‰µé«˜', 'æ–°é«˜',
                'åˆ©å¤š', 'å¥½æ¶ˆæ¯', 'æ­£é¢', 'çœ‹å¥½', 'æˆé•·', 'ç²åˆ©',
                'ç‡Ÿæ”¶', 'è²¡å ±', 'EPS', 'ç²åˆ©', 'æ¥­ç¸¾', 'è¨‚å–®',
                'åˆä½œ', 'ä½µè³¼', 'æ–°ç”¢å“', 'æŠ€è¡“', 'å¸‚å ´'
            ]
            
            for keyword in price_change_keywords:
                if keyword in title_lower or keyword in snippet_lower:
                    content_bonus += 0.1
            
            return -(relevance_score + date_bonus + content_bonus)
        
        return sorted(news_results, key=sort_key)
    
    async def search_stock_news(self, stock_id: str, stock_name: str, num_results: int = 5) -> Optional[SearchResult]:
        """æœç´¢è‚¡ç¥¨ç›¸é—œæ–°è"""
        # å„ªå…ˆæœå°‹è‚¡åƒ¹è®ŠåŒ–åŸå› 
        query = f'"{stock_name}" "{stock_id}" (æ¼²åœ OR ä¸Šæ¼² OR å¤§æ¼² OR é£†æ¼² OR çªç ´ OR å‰µé«˜ OR åˆ©å¤š OR å¥½æ¶ˆæ¯)'
        return await self.search_news(query, num_results)
    
    async def search_revenue_news(self, stock_id: str, stock_name: str) -> Optional[SearchResult]:
        """æœç´¢ç‡Ÿæ”¶ç›¸é—œæ–°è"""
        query = f'"{stock_name}" "{stock_id}" (ç‡Ÿæ”¶ OR æœˆç‡Ÿæ”¶ OR è²¡å ± OR ç²åˆ© OR æˆé•· OR æ¥­ç¸¾)'
        return await self.search_news(query, 3)
    
    async def search_earnings_news(self, stock_id: str, stock_name: str) -> Optional[SearchResult]:
        """æœç´¢è²¡å ±ç›¸é—œæ–°è"""
        query = f'"{stock_name}" "{stock_id}" (è²¡å ± OR EPS OR ç²åˆ© OR ç›ˆé¤˜ OR æ¥­ç¸¾ OR æˆé•·)'
        return await self.search_news(query, 3)
    
    async def search_market_news(self, stock_id: str, stock_name: str) -> Optional[SearchResult]:
        """æœç´¢å¸‚å ´ç›¸é—œæ–°è"""
        query = f'"{stock_name}" "{stock_id}" (è‚¡åƒ¹ OR æŠ€è¡“åˆ†æ OR å¸‚å ´ OR åˆ©å¤š OR å¥½æ¶ˆæ¯ OR çªç ´)'
        return await self.search_news(query, 3)
    
    async def generate_news_summary(self, news_results: List[NewsResult], summary_count: int = 3) -> List[str]:
        """ç”Ÿæˆæ–°èæ‘˜è¦"""
        if not news_results:
            return []
        
        # æŒ‰ç›¸é—œæ€§æ’åº
        sorted_news = sorted(news_results, key=lambda x: x.relevance_score, reverse=True)
        
        summaries = []
        for i, news in enumerate(sorted_news[:summary_count]):
            summary = f"{i+1}. {news.title} - {news.snippet[:100]}..."
            summaries.append(summary)
        
        return summaries
    
    async def get_comprehensive_news_analysis(self, stock_id: str, stock_name: str) -> Dict[str, Any]:
        """ç²å–è‚¡ç¥¨ç¶œåˆæ–°èåˆ†æ"""
        try:
            # ä¸¦è¡Œæœç´¢ä¸åŒé¡å‹çš„æ–°è
            tasks = [
                self.search_revenue_news(stock_id, stock_name),
                self.search_earnings_news(stock_id, stock_name),
                self.search_market_news(stock_id, stock_name)
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            revenue_news, earnings_news, market_news = results
            
            analysis = {
                'stock_id': stock_id,
                'stock_name': stock_name,
                'analysis_time': datetime.now().isoformat(),
                'revenue_news': revenue_news,
                'earnings_news': earnings_news,
                'market_news': market_news,
                'news_summaries': {},
                'total_news_count': 0,
                'news_sentiment': 'neutral'
            }
            
            # ç”Ÿæˆæ–°èæ‘˜è¦
            if revenue_news and revenue_news.news_results:
                analysis['news_summaries']['revenue'] = await self.generate_news_summary(
                    revenue_news.news_results, 2
                )
                analysis['total_news_count'] += len(revenue_news.news_results)
            
            if earnings_news and earnings_news.news_results:
                analysis['news_summaries']['earnings'] = await self.generate_news_summary(
                    earnings_news.news_results, 2
                )
                analysis['total_news_count'] += len(earnings_news.news_results)
            
            if market_news and market_news.news_results:
                analysis['news_summaries']['market'] = await self.generate_news_summary(
                    market_news.news_results, 2
                )
                analysis['total_news_count'] += len(market_news.news_results)
            
            # ç°¡å–®çš„æƒ…æ„Ÿåˆ†æï¼ˆåŸºæ–¼é—œéµè©ï¼‰
            positive_keywords = ['æˆé•·', 'äº®çœ¼', 'å„ªç•°', 'å¼·å‹', 'çœ‹å¥½', 'ä¸Šæ¼²', 'çªç ´']
            negative_keywords = ['ä¸‹æ»‘', 'è¡°é€€', 'è™§æ', 'ä¸‹è·Œ', 'çœ‹å£', 'é¢¨éšª']
            
            all_titles = []
            if revenue_news and revenue_news.news_results:
                all_titles.extend([news.title for news in revenue_news.news_results])
            if earnings_news and earnings_news.news_results:
                all_titles.extend([news.title for news in earnings_news.news_results])
            if market_news and market_news.news_results:
                all_titles.extend([news.title for news in market_news.news_results])
            
            positive_count = sum(1 for title in all_titles if any(keyword in title for keyword in positive_keywords))
            negative_count = sum(1 for title in all_titles if any(keyword in title for keyword in negative_keywords))
            
            if positive_count > negative_count:
                analysis['news_sentiment'] = 'positive'
            elif negative_count > positive_count:
                analysis['news_sentiment'] = 'negative'
            else:
                analysis['news_sentiment'] = 'neutral'
            
            return analysis
            
        except Exception as e:
            logger.error(f"ç²å–è‚¡ç¥¨ {stock_id} æ–°èåˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {
                'stock_id': stock_id,
                'stock_name': stock_name,
                'error': str(e),
                'analysis_time': datetime.now().isoformat()
            }

