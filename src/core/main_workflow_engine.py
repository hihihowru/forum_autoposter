#!/usr/bin/env python3
"""
ä¸»å·¥ä½œæµç¨‹å¼•æ“
çµ±ä¸€çš„ç³»çµ±å…¥å£é»ï¼Œæ•´åˆæ‰€æœ‰çµ„ä»¶ä¸¦æä¾›æ¨™æº–åŒ–çš„æµç¨‹ç®¡ç†
"""

import os
import sys
import asyncio
import logging
import random
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials
from src.clients.google.sheets_client import GoogleSheetsClient
from src.utils.config_manager import ConfigManager
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class WorkflowType(Enum):
    """å·¥ä½œæµç¨‹é¡å‹"""
    TRENDING_TOPICS = "trending_topics"
    LIMIT_UP_STOCKS = "limit_up_stocks"
    HOT_STOCKS = "hot_stocks"
    INDUSTRY_ANALYSIS = "industry_analysis"
    MONTHLY_REVENUE = "monthly_revenue"
    HIGH_VOLUME = "high_volume"
    NEWS_SUMMARY = "news_summary"
    AFTER_HOURS_LIMIT_UP = "after_hours_limit_up"
    INTRADAY_SURGE_STOCKS = "intraday_surge_stocks"

@dataclass
class WorkflowConfig:
    """å·¥ä½œæµç¨‹é…ç½®"""
    workflow_type: WorkflowType
    max_posts_per_topic: int = 3
    enable_content_generation: bool = True
    enable_publishing: bool = False
    enable_learning: bool = True
    enable_quality_check: bool = True
    enable_sheets_recording: bool = True
    retry_on_failure: bool = True
    max_retries: int = 3

@dataclass
class WorkflowResult:
    """å·¥ä½œæµç¨‹åŸ·è¡Œçµæœ"""
    success: bool
    workflow_type: WorkflowType
    total_posts_generated: int
    total_posts_published: int
    generated_posts: List[Dict[str, Any]]
    errors: List[str]
    warnings: List[str]
    execution_time: float
    start_time: datetime
    end_time: datetime

class MainWorkflowEngine:
    """ä¸»å·¥ä½œæµç¨‹å¼•æ“"""
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–ä¸»å·¥ä½œæµç¨‹å¼•æ“"""
        self.config_manager = ConfigManager(config_path)
        self.config = self.config_manager.get_config()
        
        # åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶
        self._initialize_core_components()
        
        # å·¥ä½œæµç¨‹ç‹€æ…‹
        self.current_workflow: Optional[WorkflowType] = None
        self.is_running = False
        
        logger.info("ä¸»å·¥ä½œæµç¨‹å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_core_components(self):
        """åˆå§‹åŒ–æ ¸å¿ƒçµ„ä»¶"""
        try:
            # 1. åˆå§‹åŒ– Google Sheets å®¢æˆ¶ç«¯
            self.sheets_client = GoogleSheetsClient(
                credentials_file=self.config.google.credentials_file,
                spreadsheet_id=self.config.google.spreadsheet_id
            )
            
            # 2. åˆå§‹åŒ– CMoney å®¢æˆ¶ç«¯
            self.cmoney_client = CMoneyClient()
            
            # 3. åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
            self.config_manager = ConfigManager()
            
            # 4. åˆå§‹åŒ– KOL Article ID è¿½è¹¤å™¨
            self.kol_article_tracker = {}
            
            # 5. åˆå§‹åŒ– Finlab ç›¸é—œçµ„ä»¶
            try:
                from src.api_integration.finlab_api_client import FinlabAPIClient
                self.finlab_client = FinlabAPIClient()
                self.finlab_cache = {}  # ç°¡å–®çš„å…§å­˜ç·©å­˜
                logger.info("âœ… Finlab API å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ Finlab API å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
                self.finlab_client = None
                self.finlab_cache = {}
            
            logger.info("æ‰€æœ‰æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ ¸å¿ƒçµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    async def execute_workflow(self, config: WorkflowConfig) -> WorkflowResult:
        """åŸ·è¡ŒæŒ‡å®šçš„å·¥ä½œæµç¨‹"""
        
        if self.is_running:
            raise RuntimeError("å·²æœ‰å·¥ä½œæµç¨‹æ­£åœ¨åŸ·è¡Œä¸­")
        
        self.current_workflow = config.workflow_type
        self.is_running = True
        
        start_time = datetime.now()
        result = WorkflowResult(
            success=False,
            workflow_type=config.workflow_type,
            total_posts_generated=0,
            total_posts_published=0,
            generated_posts=[],
            errors=[],
            warnings=[],
            execution_time=0.0,
            start_time=start_time,
            end_time=start_time
        )
        
        try:
            logger.info(f"é–‹å§‹åŸ·è¡Œå·¥ä½œæµç¨‹: {config.workflow_type}")
            
            # 1. é æª¢æŸ¥
            await self._pre_workflow_check(config)
            
            # 2. åŸ·è¡Œå…·é«”å·¥ä½œæµç¨‹
            if config.workflow_type == WorkflowType.AFTER_HOURS_LIMIT_UP:
                await self._execute_after_hours_limit_up_workflow(config, result)
            elif config.workflow_type == WorkflowType.TRENDING_TOPICS:
                await self._execute_trending_topics_workflow(config, result)
            elif config.workflow_type == WorkflowType.LIMIT_UP_STOCKS:
                await self._execute_limit_up_stocks_workflow(config, result)
            elif config.workflow_type == WorkflowType.HOT_STOCKS:
                await self._execute_hot_stocks_workflow(config, result)
            elif config.workflow_type == WorkflowType.INDUSTRY_ANALYSIS:
                await self._execute_industry_analysis_workflow(config, result)
            elif config.workflow_type == WorkflowType.MONTHLY_REVENUE:
                await self._execute_monthly_revenue_workflow(config, result)
            elif config.workflow_type == WorkflowType.HIGH_VOLUME:
                await self._execute_high_volume_workflow(config, result)
            elif config.workflow_type == WorkflowType.NEWS_SUMMARY:
                await self._execute_news_summary_workflow(config, result)
            elif config.workflow_type == WorkflowType.INTRADAY_SURGE_STOCKS:
                await self._execute_intraday_surge_stocks_workflow(config, result)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„å·¥ä½œæµç¨‹é¡å‹: {config.workflow_type}")
            
            # 3. å¾Œè™•ç†
            await self._post_workflow_processing(config, result)
            
            result.success = True
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            result.errors.append(str(e))
            
        finally:
            end_time = datetime.now()
            result.end_time = end_time
            result.execution_time = (end_time - start_time).total_seconds()
            self.is_running = False
            
            logger.info(f"å·¥ä½œæµç¨‹åŸ·è¡Œå®Œæˆ: {config.workflow_type}")
            logger.info(f"åŸ·è¡Œæ™‚é–“: {result.execution_time:.2f}ç§’")
            logger.info(f"ç”Ÿæˆè²¼æ–‡: {result.total_posts_generated}")
            logger.info(f"ç™¼å¸ƒè²¼æ–‡: {result.total_posts_published}")
            
        return result
    
    async def _pre_workflow_check(self, config: WorkflowConfig):
        """å·¥ä½œæµç¨‹åŸ·è¡Œå‰æª¢æŸ¥"""
        logger.info("åŸ·è¡Œé æª¢æŸ¥...")
        
        # 1. æª¢æŸ¥ API é‡‘é‘°
        await self._check_api_keys()
        
        # 2. æª¢æŸ¥ KOL é…ç½®
        await self._check_kol_configuration()
        
        # 3. æª¢æŸ¥æ•¸æ“šæºå¯ç”¨æ€§
        await self._check_data_sources()
        
        logger.info("é æª¢æŸ¥å®Œæˆ")
    
    async def _check_api_keys(self):
        """æª¢æŸ¥ API é‡‘é‘°"""
        required_keys = [
            'OPENAI_API_KEY',
            'CMONEY_PASSWORD',
            'GOOGLE_SHEETS_ID',
            'FINLAB_API_KEY',
            'SERPER_API_KEY'
        ]
        
        missing_keys = []
        for key in required_keys:
            if not os.getenv(key):
                missing_keys.append(key)
        
        if missing_keys:
            raise ValueError(f"ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {', '.join(missing_keys)}")
    
    async def _check_kol_configuration(self):
        """æª¢æŸ¥ KOL é…ç½®"""
        try:
            # ç°¡åŒ–çš„ KOL é…ç½®æª¢æŸ¥
            kol_settings = self.config_manager.get_kol_personalization_settings()
            if not kol_settings:
                raise ValueError("æ²’æœ‰æ‰¾åˆ° KOL è¨­å®š")
            
            logger.info(f"æ‰¾åˆ° {len(kol_settings)} å€‹ KOL è¨­å®š")
            
        except Exception as e:
            logger.error(f"KOL é…ç½®æª¢æŸ¥å¤±æ•—: {e}")
            raise
    
    async def _check_data_sources(self):
        """æª¢æŸ¥æ•¸æ“šæºå¯ç”¨æ€§"""
        try:
            # æª¢æŸ¥ CMoney API é€£æ¥
            test_credentials = LoginCredentials(
                email='forum_200@cmoney.com.tw',
                password=os.getenv('CMONEY_PASSWORD')
            )
            token = await self.cmoney_client.login(test_credentials)
            if not token:
                raise ValueError("CMoney API é€£æ¥å¤±æ•—")
            
            # ç°¡åŒ–çš„ Google Sheets æª¢æŸ¥
            logger.info("Google Sheets å®¢æˆ¶ç«¯å·²åˆå§‹åŒ–")
            
            logger.info("æ‰€æœ‰æ•¸æ“šæºæª¢æŸ¥é€šé")
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæºæª¢æŸ¥å¤±æ•—: {e}")
            raise
    
    async def _execute_after_hours_limit_up_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œç›¤å¾Œæ¼²åœè‚¡å·¥ä½œæµç¨‹ - æ™ºèƒ½èª¿é…ç³»çµ± (10æœ‰é‡ + 5ç„¡é‡)"""
        logger.info("åŸ·è¡Œç›¤å¾Œæ¼²åœè‚¡æ™ºèƒ½èª¿é…å·¥ä½œæµç¨‹...")
        
        try:
            # å°å…¥æ™ºèƒ½èª¿é…ç³»çµ±
            from smart_api_allocator import SmartAPIAllocator, StockAnalysis
            
            # åˆå§‹åŒ–æ™ºèƒ½èª¿é…å™¨
            allocator = SmartAPIAllocator()
            
            # ç²å–ä»Šæ—¥æ¼²åœè‚¡ç¥¨æ•¸æ“šï¼ˆ10æœ‰é‡ + 5ç„¡é‡ï¼‰
            limit_up_stocks = await self._get_today_limit_up_stocks()
            
            if not limit_up_stocks:
                logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»Šæ—¥æ¼²åœè‚¡ç¥¨ï¼Œä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                limit_up_stocks = self._create_sample_limit_up_stocks()
            
            # æ™ºèƒ½åˆ†é…APIè³‡æº
            allocated_stocks = allocator.allocate_apis_for_stocks(limit_up_stocks)
            
            # ç²å– KOL è¨­å®š
            kol_settings = self.config_manager.get_kol_personalization_settings()
            kol_list = list(kol_settings.keys())
            
            # éš¨æ©Ÿé¸æ“‡15å€‹KOLï¼ˆå…è¨±é‡è¤‡ï¼‰
            import random
            selected_kols = random.choices(kol_list, k=15)
            
            # ç”Ÿæˆè²¼æ–‡
            posts_generated = 0
            
            for i, stock in enumerate(allocated_stocks):
                try:
                    kol_serial = selected_kols[i]
                    kol_nickname = kol_settings[kol_serial]['persona']
                    
                    # ç”Ÿæˆå…§å®¹å¤§ç¶±
                    content_outline = allocator.generate_content_outline(stock)
                    
                    # æ ¹æ“šæœ‰é‡/ç„¡é‡ç”Ÿæˆä¸åŒé¢¨æ ¼çš„å…§å®¹
                    is_high_volume = stock.volume_amount >= 1.0  # 1å„„ä»¥ä¸Šç‚ºæœ‰é‡
                    
                    # ç”Ÿæˆè²¼æ–‡å…§å®¹
                    content = await self._generate_limit_up_post_content(
                        stock, kol_serial, kol_settings[kol_serial], 
                        content_outline, is_high_volume
                    )
                    
                    # è¨˜éŒ„åˆ° Google Sheets
                    await self._record_limit_up_post_to_sheets(
                        stock, kol_serial, kol_nickname, content, 
                        content_outline, is_high_volume
                    )
                    
                    posts_generated += 1
                    logger.info(f"âœ… ç”Ÿæˆç¬¬ {posts_generated} ç¯‡è²¼æ–‡: {stock.stock_name}({stock.stock_id}) - {kol_nickname}")
                    
                except Exception as e:
                    logger.error(f"âŒ ç”Ÿæˆç¬¬ {i+1} ç¯‡è²¼æ–‡å¤±æ•—: {e}")
                    continue
            
            result.total_posts_generated = posts_generated
            logger.info(f"ç›¤å¾Œæ¼²åœè‚¡æ™ºèƒ½èª¿é…å·¥ä½œæµç¨‹å®Œæˆï¼Œå…±ç”Ÿæˆ {posts_generated} ç¯‡è²¼æ–‡")
            
        except Exception as e:
            logger.error(f"ç›¤å¾Œæ¼²åœè‚¡å·¥ä½œæµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            result.errors.append(str(e))
            raise
            
    async def _get_today_limit_up_stocks(self):
        """ç²å–ä»Šæ—¥æ¼²åœè‚¡ç¥¨æ•¸æ“š"""
        try:
            # ä½¿ç”¨ Finlab API ç²å–ä»Šæ—¥æ¼²åœè‚¡ç¥¨
            import finlab
            import finlab.data as fdata
            import pandas as pd
            from datetime import datetime
            
            # ç™»å…¥ Finlab
            finlab_key = os.getenv('FINLAB_API_KEY')
            if not finlab_key:
                logger.warning("âš ï¸ æ²’æœ‰ FINLAB_API_KEYï¼Œä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                return None
            
            finlab.login(finlab_key)
            
            # ç²å–ä»Šæ—¥æ—¥æœŸ
            today = datetime.now().strftime('%Y-%m-%d')
            
            # ç²å–æ”¶ç›¤åƒ¹å’Œæˆäº¤é‡‘é¡æ•¸æ“š
            close_price = fdata.get('price:æ”¶ç›¤åƒ¹')
            volume_amount = fdata.get('price:æˆäº¤é‡‘é¡')
            
            if close_price is None or volume_amount is None:
                logger.warning("âš ï¸ ç„¡æ³•ç²å– Finlab æ•¸æ“šï¼Œä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                return None
            
            # è¨ˆç®—æ¼²åœè‚¡ç¥¨
            limit_up_stocks = []
            
            # ç²å–ä»Šæ—¥å’Œæ˜¨æ—¥çš„æ•¸æ“š
            today_close = close_price.loc[today] if today in close_price.index else None
            yesterday_close = close_price.loc[today - pd.Timedelta(days=1)] if (today - pd.Timedelta(days=1)) in close_price.index else None
            
            if today_close is None or yesterday_close is None:
                logger.warning("âš ï¸ ç„¡æ³•ç²å–ä»Šæ—¥æˆ–æ˜¨æ—¥æ•¸æ“šï¼Œä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                return None
            
            today_volume = volume_amount.loc[today] if today in volume_amount.index else None
            
            # è‚¡ç¥¨åç¨±å°æ‡‰
            stock_names = {
                "6732": "æ˜‡ä½³é›»å­", "4968": "ç«‹ç©", "3491": "æ˜‡é”ç§‘æŠ€",
                "6919": "åº·éœˆç”ŸæŠ€", "5314": "ä¸–ç´€", "4108": "æ‡·ç‰¹",
                "8150": "å—èŒ‚", "3047": "è¨ŠèˆŸ", "8033": "é›·è™",
                "1256": "é®®æ´»æœæ±-KY", "8028": "æ˜‡é™½åŠå°é«”", "8255": "æœ‹ç¨‹", "6753": "é¾å¾·é€ èˆ¹"
            }
            
            # æª¢æŸ¥æ¯éš»è‚¡ç¥¨
            for stock_id in stock_names.keys():
                if stock_id in today_close.index and stock_id in yesterday_close.index:
                    today_price = today_close[stock_id]
                    yesterday_price = yesterday_close[stock_id]
                    
                    if pd.isna(today_price) or pd.isna(yesterday_price):
                        continue
                    
                    # è¨ˆç®—æ¼²å¹…
                    change_percent = ((today_price - yesterday_price) / yesterday_price) * 100
                    
                    # æª¢æŸ¥æ˜¯å¦æ¼²åœï¼ˆ>= 9.5%ï¼‰
                    if change_percent >= 9.5:
                        # ç²å–æˆäº¤é‡‘é¡
                        volume_info = None
                        if today_volume is not None and stock_id in today_volume.index:
                            amount = today_volume[stock_id]
                            if not pd.isna(amount):
                                amount_billion = amount / 100000000
                                volume_info = {
                                    'volume_amount': round(amount, 0),
                                    'volume_amount_billion': round(amount_billion, 4)
                                }
                        
                        # å‰µå»ºè‚¡ç¥¨åˆ†æå°è±¡
                        from smart_api_allocator import StockAnalysis
                        stock_analysis = StockAnalysis(
                            stock_id=stock_id,
                            stock_name=stock_names[stock_id],
                            volume_rank=0,  # ç¨å¾Œæ’åº
                            change_percent=change_percent,
                            volume_amount=volume_info['volume_amount_billion'] if volume_info else 0,
                            rank_type="æˆäº¤é‡‘é¡æ’å"
                        )
                        
                        limit_up_stocks.append(stock_analysis)
            
            # æŒ‰æˆäº¤é‡‘é¡æ’åº
            limit_up_stocks.sort(key=lambda x: x.volume_amount, reverse=True)
            
            # åˆ†é…æ’å
            for i, stock in enumerate(limit_up_stocks, 1):
                stock.volume_rank = i
                if stock.volume_amount < 1.0:
                    stock.rank_type = "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"
            
            # åˆ†é›¢æœ‰é‡å’Œç„¡é‡è‚¡ç¥¨
            high_volume_stocks = [s for s in limit_up_stocks if s.volume_amount >= 1.0][:10]
            low_volume_stocks = [s for s in limit_up_stocks if s.volume_amount < 1.0][:5]
            
            # é‡æ–°åˆ†é…æ’å
            for i, stock in enumerate(high_volume_stocks, 1):
                stock.volume_rank = i
                stock.rank_type = "æˆäº¤é‡‘é¡æ’å"
            
            for i, stock in enumerate(low_volume_stocks, 1):
                stock.volume_rank = i
                stock.rank_type = "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"
            
            result = high_volume_stocks + low_volume_stocks
            logger.info(f"âœ… æ‰¾åˆ° {len(result)} éš»æ¼²åœè‚¡ç¥¨ï¼ˆ{len(high_volume_stocks)}æœ‰é‡ + {len(low_volume_stocks)}ç„¡é‡ï¼‰")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ç²å–ä»Šæ—¥æ¼²åœè‚¡ç¥¨å¤±æ•—: {e}")
            return None
    
    def _create_sample_limit_up_stocks(self):
        """å‰µå»ºæ¨£æœ¬æ¼²åœè‚¡ç¥¨æ•¸æ“š"""
        from smart_api_allocator import StockAnalysis
        
        # æœ‰é‡æ¼²åœè‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡é«˜åˆ°ä½ï¼‰
        high_volume_stocks = [
            StockAnalysis("3665", "è²¿è¯-KY", 1, 9.8, 15.2, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("3653", "å¥ç­–", 2, 9.9, 12.8, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("5314", "ä¸–ç´€", 3, 9.7, 10.5, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("6753", "é¾å¾·é€ èˆ¹", 4, 9.6, 9.2, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("8039", "å°è™¹", 5, 9.8, 8.7, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("3707", "æ¼¢ç£Š", 6, 9.9, 7.3, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("3704", "åˆå‹¤æ§", 7, 9.7, 6.8, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("4303", "ä¿¡ç«‹", 8, 9.6, 5.9, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("1605", "è¯æ–°", 9, 9.8, 4.2, "æˆäº¤é‡‘é¡æ’å"),
            StockAnalysis("2353", "å®ç¢", 10, 9.9, 3.1, "æˆäº¤é‡‘é¡æ’å")
        ]
        
        # ç„¡é‡æ¼²åœè‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡ä½åˆ°é«˜ï¼‰
        low_volume_stocks = [
            StockAnalysis("5345", "å¤©æš", 1, 9.8, 0.0164, "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"),
            StockAnalysis("2724", "å°å˜‰ç¢©", 2, 9.9, 0.0306, "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"),
            StockAnalysis("6264", "ç²¾æ‹“ç§‘", 3, 9.7, 0.0326, "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"),
            StockAnalysis("8906", "é«˜åŠ›", 4, 9.6, 0.0380, "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰"),
            StockAnalysis("2380", "è™¹å…‰", 5, 9.8, 0.0406, "æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰")
        ]
        
        return high_volume_stocks + low_volume_stocks
    
    async def _generate_limit_up_post_content(self, stock, kol_serial, kol_settings, content_outline, is_high_volume):
        """ç”Ÿæˆæ¼²åœè‚¡è²¼æ–‡å…§å®¹ - ä½¿ç”¨å¢å¼·ç‰ˆpromptç³»çµ±"""
        try:
            import random
            from src.services.content.content_generator import ContentGenerator, ContentRequest
            from src.services.content.enhanced_prompt_templates import enhanced_prompt_templates
            
            # åˆå§‹åŒ–å…§å®¹ç”Ÿæˆå™¨
            content_generator = ContentGenerator()
            
            # æ ¼å¼åŒ–æˆäº¤é‡‘é¡
            def format_volume_amount(amount_billion: float) -> str:
                if amount_billion >= 1.0:
                    return f"{amount_billion:.4f}å„„å…ƒ"
                else:
                    amount_million = amount_billion * 100
                    return f"{amount_million:.2f}ç™¾è¬å…ƒ"
            
            # ä½¿ç”¨çœŸå¯¦çš„æˆäº¤é‡‘é¡æ•¸æ“š
            real_stock_data = self._get_real_stock_data(stock.stock_id)
            volume_formatted = format_volume_amount(real_stock_data.get("volume_amount", stock.volume_amount))
            
            # ç²å– Serper API æ–°èé€£çµ
            news_links = await self._get_serper_news_links(stock.stock_id, stock.stock_name)
            
            # ä½¿ç”¨å¢å¼·ç‰ˆpromptæ¨¡æ¿ç³»çµ±
            stock_data = {
                "stock_id": stock.stock_id,
                "stock_name": stock.stock_name,
                "change_percent": stock.change_percent,
                "volume_rank": stock.volume_rank,
                "volume_amount": real_stock_data.get("volume_amount", stock.volume_amount),
                "volume_formatted": volume_formatted,
                "is_high_volume": is_high_volume
            }
            
            # æ ¹æ“šKOLè¨­å®šé¸æ“‡é¢¨æ ¼ï¼Œå¦‚æœæ²’æœ‰è¨­å®šå‰‡éš¨æ©Ÿé¸æ“‡
            style = self._get_kol_style(kol_settings, enhanced_prompt_templates)
            prompt_data = enhanced_prompt_templates.build_limit_up_prompt(stock_data, style)
            
            # ä½¿ç”¨å¢å¼·ç‰ˆpromptç”Ÿæˆå…§å®¹
            try:
                response = content_generator.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": prompt_data["system_prompt"]},
                        {"role": "user", "content": prompt_data["user_prompt"]}
                    ],
                    temperature=prompt_data["temperature"],
                    max_tokens=800,
                    top_p=0.95,
                    frequency_penalty=prompt_data["frequency_penalty"],
                    presence_penalty=prompt_data["presence_penalty"]
                )
                
                content = response.choices[0].message.content.strip()
                
                # ä½¿ç”¨é è¨­çš„æ¨™é¡Œï¼Œè€Œä¸æ˜¯è®“LLMç”Ÿæˆ
                title = prompt_data["title"]
                
                # æ·»åŠ æ–°èé€£çµåˆ°å…§å®¹ä¸­
                if news_links:
                    content += f"\n\n{news_links}"
                
                # ç›´æ¥ä½¿ç”¨å…§å®¹ï¼Œæ¨™é¡Œå·²ç¶“åŒ…å«åœ¨promptä¸­
                full_content = content
                
                logger.info(f"âœ… ä½¿ç”¨{style['name']}é¢¨æ ¼ç”Ÿæˆå…§å®¹æˆåŠŸ")
                return full_content, prompt_data
                
            except Exception as e:
                logger.error(f"âŒ ä½¿ç”¨å¢å¼·ç‰ˆpromptç”Ÿæˆå…§å®¹å¤±æ•—: {e}")
                # å›é€€åˆ°åŸä¾†çš„å€‹äººåŒ–ç”Ÿæˆ
                return await self._generate_fallback_content(stock, kol_settings, is_high_volume)
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå€‹äººåŒ–è²¼æ–‡å…§å®¹å¤±æ•—: {e}")
            # å›é€€åˆ°ç°¡å–®æ¨¡æ¿
            return f"{stock.stock_name}({stock.stock_id}) ä»Šæ—¥æ¼²åœ{stock.change_percent:.1f}%ï¼"
    
    def _get_kol_style(self, kol_settings, enhanced_prompt_templates):
        """æ ¹æ“šKOLè¨­å®šé¸æ“‡é¢¨æ ¼"""
        try:
            # æª¢æŸ¥KOLæ˜¯å¦æœ‰ç‰¹å®šçš„å¯«ä½œé¢¨æ ¼è¨­å®š
            if isinstance(kol_settings, dict) and kol_settings.get('writing_style'):
                style_name = kol_settings['writing_style']
                style = enhanced_prompt_templates.get_style_by_name(style_name)
                if style:
                    logger.info(f"ğŸ­ ä½¿ç”¨KOLè¨­å®šé¢¨æ ¼: {style_name}")
                    return style
            
            # æª¢æŸ¥KOLæš±ç¨±ï¼Œæ ¹æ“šæš±ç¨±æ¨æ¸¬é¢¨æ ¼
            if isinstance(kol_settings, dict) and kol_settings.get('nickname'):
                nickname = kol_settings['nickname'].lower()
                
                # æ ¹æ“šæš±ç¨±é—œéµå­—é¸æ“‡é¢¨æ ¼
                if any(keyword in nickname for keyword in ['ptt', 'é„‰æ°‘', 'è‚¡ç¥']):
                    return enhanced_prompt_templates.get_style_by_name("PTTè‚¡ç¥é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['å…«å¦', 'çˆ†æ–™']):
                    return enhanced_prompt_templates.get_style_by_name("å…«å¦å¥³ç‹é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['å¹½é»˜', 'æç¬‘']):
                    return enhanced_prompt_templates.get_style_by_name("å–œåŠ‡æ¼”å“¡é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['åˆ†æ', 'å°ˆæ¥­']):
                    return enhanced_prompt_templates.get_style_by_name("å–œåŠ‡åˆ†æå¸«é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['æ•…äº‹', 'èªªæ›¸']):
                    return enhanced_prompt_templates.get_style_by_name("æ•…äº‹å¤§ç‹é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['æ°£æ°›', 'å—¨']):
                    return enhanced_prompt_templates.get_style_by_name("æ°£æ°›å¤§å¸«é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['æ´¾å°', 'ä¸»æŒ']):
                    return enhanced_prompt_templates.get_style_by_name("æ´¾å°ä¸»æŒäººé¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['æˆ²åŠ‡', 'è¡¨æ¼”']):
                    return enhanced_prompt_templates.get_style_by_name("æˆ²åŠ‡ä¹‹ç‹é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['è«·åˆº', 'é…¸']):
                    return enhanced_prompt_templates.get_style_by_name("è«·åˆºè©•è«–å®¶é¢¨æ ¼")
                elif any(keyword in nickname for keyword in ['è¿·å› ', 'æ¢—']):
                    return enhanced_prompt_templates.get_style_by_name("è¿·å› å¤§å¸«é¢¨æ ¼")
            
            # å¦‚æœæ²’æœ‰åŒ¹é…çš„é¢¨æ ¼ï¼Œéš¨æ©Ÿé¸æ“‡
            style = enhanced_prompt_templates.get_random_style()
            logger.info(f"ğŸ² éš¨æ©Ÿé¸æ“‡é¢¨æ ¼: {style['name']}")
            return style
            
        except Exception as e:
            logger.error(f"âŒ é¸æ“‡KOLé¢¨æ ¼å¤±æ•—: {e}")
            return enhanced_prompt_templates.get_random_style()

    async def _generate_fallback_content(self, stock, kol_settings, is_high_volume):
        """å›é€€åˆ°åŸä¾†çš„å€‹äººåŒ–å…§å®¹ç”Ÿæˆ"""
        try:
            from src.services.content.content_generator import ContentGenerator, ContentRequest
            
            # åˆå§‹åŒ–å…§å®¹ç”Ÿæˆå™¨
            content_generator = ContentGenerator()
            
            # æ ¼å¼åŒ–æˆäº¤é‡‘é¡
            def format_volume_amount(amount_billion: float) -> str:
                if amount_billion >= 1.0:
                    return f"{amount_billion:.4f}å„„å…ƒ"
                else:
                    amount_million = amount_billion * 100
                    return f"{amount_million:.2f}ç™¾è¬å…ƒ"
            
            volume_formatted = format_volume_amount(stock.volume_amount)
            
            # ç²å– Serper API æ–°èé€£çµ
            news_links = await self._get_serper_news_links(stock.stock_id, stock.stock_name)
            
            # æ ¹æ“šæœ‰é‡/ç„¡é‡é¸æ“‡ä¸åŒçš„ä¸»é¡Œ
            if is_high_volume:
                # æœ‰é‡æ¼²åœçš„å¤šæ¨£åŒ–æ¨™é¡Œæ¨¡æ¿
                title_templates = [
                    f"{stock.stock_name}å¼·å‹¢æ¼²åœ{stock.change_percent:.1f}%ï¼æˆäº¤é‡‘é¡{volume_formatted}èƒŒå¾Œçš„ç§˜å¯†",
                    f"å¸‚å ´ç„¦é»ï¼š{stock.stock_name}çˆ†é‡{stock.change_percent:.1f}%æ¼²åœï¼Œæˆäº¤é‡‘é¡æ’åç¬¬{stock.volume_rank}å",
                    f"{stock.stock_name}é£†å‡{stock.change_percent:.1f}%ï¼{volume_formatted}æˆäº¤é‡çš„æŠ•è³‡å•Ÿç¤º",
                    f"ä»Šæ—¥äº®é»ï¼š{stock.stock_name}æ¼²åœ{stock.change_percent:.1f}%ï¼Œæˆäº¤é‡‘é¡{volume_formatted}çš„å¸‚å ´æ„ç¾©",
                    f"{stock.stock_name}å¤§æ¼²{stock.change_percent:.1f}%ï¼å¾{volume_formatted}æˆäº¤é‡çœ‹å¾Œå¸‚",
                    f"å¸‚å ´ç†±é»ï¼š{stock.stock_name}å¼·å‹¢{stock.change_percent:.1f}%æ¼²åœï¼Œæˆäº¤é‡‘é¡æ’åç¬¬{stock.volume_rank}å",
                    f"{stock.stock_name}é€†å‹¢æ¼²åœ{stock.change_percent:.1f}%ï¼{volume_formatted}æˆäº¤é‡çš„æŠ•è³‡æ©Ÿæœƒ",
                    f"ä»Šæ—¥ç„¦é»ï¼š{stock.stock_name}é£†å‡{stock.change_percent:.1f}%ï¼Œæˆäº¤é‡‘é¡{volume_formatted}çš„èƒŒå¾Œ",
                    f"{stock.stock_name}æš´æ¼²{stock.change_percent:.1f}%ï¼å¾æˆäº¤é‡‘é¡æ’åç¬¬{stock.volume_rank}åçœ‹è¶¨å‹¢",
                    f"å¸‚å ´äº®é»ï¼š{stock.stock_name}å¼·å‹¢æ¼²åœ{stock.change_percent:.1f}%ï¼Œ{volume_formatted}æˆäº¤é‡çš„å•Ÿç¤º"
                ]
                topic_title = random.choice(title_templates)
                topic_keywords = f"æ¼²åœè‚¡,å¸‚å ´ç†±é»,{stock.stock_name},æˆäº¤é‡å¤§,æŠ•è³‡æ©Ÿæœƒ"
                market_context = f"""
å¸‚å ´ç„¦é»ï¼š
â€¢ æˆäº¤é‡‘é¡æ’åï¼šç¬¬{stock.volume_rank}å
â€¢ æˆäº¤é‡‘é¡ï¼š{volume_formatted}
â€¢ æ¼²å¹…ï¼š{stock.change_percent:.1f}%

æŠ•è³‡äº®é»ï¼š
â€¢ å¸‚å ´è³‡é‡‘ç©æ¥µé€²å ´ï¼Œé¡¯ç¤ºå¼·çƒˆè²·ç›¤
â€¢ æˆäº¤é‡æ”¾å¤§ï¼Œæ”¯æ’è‚¡åƒ¹çºŒæ¼²å‹•èƒ½
â€¢ æŠ€è¡“é¢çªç ´ï¼Œå¾Œå¸‚å¯æœŸ

é—œæ³¨é‡é»ï¼š
â€¢ æ˜æ—¥é–‹ç›¤è¡¨ç¾
â€¢ æˆäº¤é‡æ˜¯å¦æŒçºŒæ”¾å¤§
â€¢ ç›¸é—œç”¢æ¥­å‹•æ…‹

{news_links}
"""
            else:
                # ç„¡é‡æ¼²åœçš„å¤šæ¨£åŒ–æ¨™é¡Œæ¨¡æ¿
                title_templates = [
                    f"{stock.stock_name}ç„¡é‡æ¼²åœ{stock.change_percent:.1f}%ï¼ç±Œç¢¼é›†ä¸­çš„æŠ•è³‡æ™ºæ…§",
                    f"ç±Œç¢¼åˆ†æï¼š{stock.stock_name}ç„¡é‡{stock.change_percent:.1f}%æ¼²åœï¼Œæˆäº¤é‡‘é¡{volume_formatted}",
                    f"{stock.stock_name}å¼·å‹¢ç„¡é‡{stock.change_percent:.1f}%ï¼å¾{volume_formatted}çœ‹ç±Œç¢¼é›†ä¸­åº¦",
                    f"ä»Šæ—¥äº®é»ï¼š{stock.stock_name}ç„¡é‡æ¼²åœ{stock.change_percent:.1f}%ï¼Œç±Œç¢¼é›†ä¸­æ’åç¬¬{stock.volume_rank}å",
                    f"{stock.stock_name}é£†å‡{stock.change_percent:.1f}%ï¼ç„¡é‡ä¸Šæ¼²çš„ç±Œç¢¼ç§˜å¯†",
                    f"ç±Œç¢¼ç„¦é»ï¼š{stock.stock_name}ç„¡é‡{stock.change_percent:.1f}%æ¼²åœï¼Œæˆäº¤é‡‘é¡{volume_formatted}çš„æ„ç¾©",
                    f"{stock.stock_name}é€†å‹¢ç„¡é‡{stock.change_percent:.1f}%ï¼å¾ç±Œç¢¼é›†ä¸­åº¦çœ‹å¾Œå¸‚",
                    f"ä»Šæ—¥åˆ†æï¼š{stock.stock_name}ç„¡é‡æ¼²åœ{stock.change_percent:.1f}%ï¼Œ{volume_formatted}çš„æŠ•è³‡å•Ÿç¤º",
                    f"{stock.stock_name}å¼·å‹¢{stock.change_percent:.1f}%ï¼ç„¡é‡ä¸Šæ¼²èƒŒå¾Œçš„ç±Œç¢¼é‚è¼¯",
                    f"ç±Œç¢¼äº®é»ï¼š{stock.stock_name}ç„¡é‡æ¼²åœ{stock.change_percent:.1f}%ï¼Œæ’åç¬¬{stock.volume_rank}åçš„ç§˜å¯†"
                ]
                topic_title = random.choice(title_templates)
                topic_keywords = f"æ¼²åœè‚¡,ç±Œç¢¼åˆ†æ,{stock.stock_name},ç„¡é‡ä¸Šæ¼²,ç±Œç¢¼é›†ä¸­"
                market_context = f"""
ç±Œç¢¼åˆ†æï¼š
â€¢ æˆäº¤é‡‘é¡æ’åï¼ˆç„¡é‡ï¼‰ï¼šç¬¬{stock.volume_rank}å
â€¢ æˆäº¤é‡‘é¡ï¼š{volume_formatted}
â€¢ æ¼²å¹…ï¼š{stock.change_percent:.1f}%

æŠ•è³‡äº®é»ï¼š
â€¢ ç±Œç¢¼é«˜åº¦é›†ä¸­ï¼Œè³£å£“è¼•å¾®
â€¢ ç„¡é‡ä¸Šæ¼²ï¼Œé¡¯ç¤ºå¼·çƒˆçºŒæ¼²æ„é¡˜
â€¢ æŠ€è¡“é¢å¼·å‹¢ï¼Œçªç ´é—œéµåƒ¹ä½

é—œæ³¨é‡é»ï¼š
â€¢ æ˜æ—¥æˆäº¤é‡è®ŠåŒ–
â€¢ ç±Œç¢¼é›†ä¸­åº¦ç¶­æŒ
â€¢ ç›¸é—œæ¶ˆæ¯é¢ç™¼å±•

{news_links}
"""
            
            # å»ºç«‹ ContentRequest
            content_request = ContentRequest(
                topic_title=topic_title,
                topic_keywords=topic_keywords,
                kol_persona=kol_settings.get('persona', 'æŠ€è¡“æ´¾'),
                kol_nickname=kol_settings.get('nickname', 'æœªçŸ¥KOL'),
                content_type="analysis",
                target_audience="æŠ•è³‡äºº",
                market_data={
                    "stock_id": stock.stock_id,
                    "stock_name": stock.stock_name,
                    "volume_rank": stock.volume_rank,
                    "volume_amount": stock.volume_amount,
                    "change_percent": stock.change_percent,
                    "is_high_volume": is_high_volume,
                    "market_context": market_context
                }
            )
            
            # ä½¿ç”¨ä¸»æµç¨‹çš„å…§å®¹ç”Ÿæˆå™¨ç”Ÿæˆå€‹äººåŒ–å…§å®¹
            title = content_generator.generate_title(content_request)
            content = content_generator.generate_content(content_request, title)
            
            # å‰µå»º prompt_data
            prompt_data = {
                "title": title,
                "selected_title": title
            }
            
            return content, prompt_data
            
        except Exception as e:
            logger.error(f"âŒ å›é€€å…§å®¹ç”Ÿæˆå¤±æ•—: {e}")
            fallback_content = f"{stock.stock_name}({stock.stock_id}) ä»Šæ—¥æ¼²åœ{stock.change_percent:.1f}%ï¼"
            fallback_prompt_data = {
                "title": f"{stock.stock_name} æ¼²åœåˆ†æ",
                "selected_title": f"{stock.stock_name} æ¼²åœåˆ†æ"
            }
            return fallback_content, fallback_prompt_data
    
    async def _get_serper_news_links(self, stock_id: str, stock_name: str) -> str:
        """ç²å– Serper API æ–°èé€£çµå’Œæ¼²åœåŸå› åˆ†æ"""
        try:
            from src.api_integration.serper_api_client import SerperAPIClient
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ SERPER_API_KEY
            if not os.getenv('SERPER_API_KEY'):
                logger.warning("âš ï¸ æœªè¨­å®š SERPER_API_KEYï¼Œè·³éæ–°èé€£çµç²å–")
                return self._get_fallback_analysis(stock_name, stock_id)
            
            # åˆå§‹åŒ– Serper å®¢æˆ¶ç«¯
            async with SerperAPIClient() as serper_client:
                # æœå°‹æ¼²åœåŸå› ç›¸é—œæ–°è
                search_query = f"{stock_name} {stock_id} æ¼²åœ åŸå›  æ–°è 2025"
                search_result = await serper_client.search_news(search_query, num_results=5)
                
                if not search_result or not search_result.news_results:
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ° {stock_name} ç›¸é—œæ–°èï¼Œä½¿ç”¨å‚™ç”¨åˆ†æ")
                    return self._get_fallback_analysis(stock_name, stock_id)
                
                # åˆ†ææ¼²åœåŸå› 
                reasons = []
                news_links = []
                
                for result in search_result.news_results[:3]:
                    # æå–å¯èƒ½çš„åŸå› é—œéµè©
                    snippet = result.snippet.lower()
                    title = result.title.lower()
                    
                    # å¸¸è¦‹æ¼²åœåŸå› é—œéµè©
                    reason_keywords = {
                        "æ¥­ç¸¾": ["æ¥­ç¸¾", "ç‡Ÿæ”¶", "ç²åˆ©", "eps", "è²¡å ±"],
                        "è¨‚å–®": ["è¨‚å–®", "æ¥å–®", "å®¢æˆ¶", "åˆä½œ"],
                        "æŠ€è¡“": ["æŠ€è¡“", "å°ˆåˆ©", "ç ”ç™¼", "å‰µæ–°"],
                        "æ”¿ç­–": ["æ”¿ç­–", "è£œåŠ©", "æ”¿åºœ", "æ³•è¦"],
                        "å¸‚å ´": ["å¸‚å ´", "éœ€æ±‚", "æˆé•·", "æ“´å¼µ"],
                        "ä½µè³¼": ["ä½µè³¼", "æ”¶è³¼", "åˆä½µ", "æŠ•è³‡"],
                        "ç”¢å“": ["ç”¢å“", "æ–°å“", "æ¨å‡º", "ä¸Šå¸‚"]
                    }
                    
                    # åˆ†ææ–°èå…§å®¹æ‰¾å‡ºå¯èƒ½åŸå› 
                    for reason_type, keywords in reason_keywords.items():
                        if any(keyword in snippet or keyword in title for keyword in keywords):
                            if reason_type not in reasons:
                                reasons.append(reason_type)
                    
                    # æ”¶é›†æ–°èé€£çµ
                    news_links.append(f"â€¢ [{result.title}]({result.link})")
                
                # æ·±åº¦åˆ†ææ–°èå…§å®¹
                analysis_result = await self._analyze_news_content(search_result.news_results, stock_name)
                
                # ç”Ÿæˆæ·±åº¦æ´å¯Ÿå…§å®¹ï¼ˆç°¡æ½”ç‰ˆï¼‰
                insight_content = f"""

ğŸ“ˆ **æ¼²åœåŸå› **: {analysis_result['reason_summary']}

ğŸ¢ **é¡è‚¡é€£å‹•**: {analysis_result['sector_analysis']}

ğŸ’¡ **æ·±åº¦æ´å¯Ÿ**: {analysis_result['insight_analysis']}

ğŸ“° **ç›¸é—œæ–°è**:
{chr(10).join(news_links)}
"""
                
                return insight_content
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ–°èé€£çµå¤±æ•—: {e}")
            return self._get_fallback_analysis(stock_name, stock_id)
    
    async def _analyze_news_content(self, news_results, stock_name: str) -> dict:
        """æ·±åº¦åˆ†ææ–°èå…§å®¹ï¼Œæä¾›æœ‰æ–™çš„æ´å¯Ÿ"""
        try:
            # åˆ†ææ–°èå…§å®¹
            all_text = ""
            sector_keywords = []
            reason_keywords = []
            
            for result in news_results:
                all_text += f"{result.title} {result.snippet} "
            
            all_text = all_text.lower()
            
            # é¡è‚¡åˆ†æé—œéµè©
            sector_mapping = {
                "è¨˜æ†¶é«”": ["è¨˜æ†¶é«”", "dram", "nand", "flash", "è¯é‚¦é›»", "å—äºç§‘", "ç¾¤è¯"],
                "ç‡Ÿå»º": ["ç‡Ÿå»º", "å»ºè¨­", "æˆ¿åœ°ç”¢", "æ°¸ä¿¡å»º", "å…¨å¤å»º"],
                "åŠå°é«”": ["åŠå°é«”", "æ™¶ç‰‡", "å°è£", "æ¸¬è©¦", "é‡‡éˆº", "ç²¾æ¸¬", "ç©å´´"],
                "é›»å­": ["é›»å­", "pcb", "é€£æ¥å™¨", "ç³»çµ±é›»", "åŠ ç™¾è£•"],
                "ç”ŸæŠ€": ["ç”ŸæŠ€", "é†«ç™‚", "è—¥å“", "å‹éœ–"],
                "å‚³ç”¢": ["å‚³ç”¢", "é‹¼éµ", "ä¸–ç´€", "ä¸­ç’°"]
            }
            
            # æ‰¾å‡ºæ‰€å±¬é¡è‚¡
            for sector, keywords in sector_mapping.items():
                if any(keyword in all_text for keyword in keywords):
                    sector_keywords.append(sector)
            
            # æ¼²åœåŸå› åˆ†æ
            reason_mapping = {
                "æ¥­ç¸¾æˆé•·": ["æ¥­ç¸¾", "ç‡Ÿæ”¶", "ç²åˆ©", "eps", "è²¡å ±", "æˆé•·"],
                "è¨‚å–®å¢åŠ ": ["è¨‚å–®", "æ¥å–®", "å®¢æˆ¶", "åˆä½œ", "åˆç´„"],
                "æŠ€è¡“çªç ´": ["æŠ€è¡“", "å°ˆåˆ©", "ç ”ç™¼", "å‰µæ–°", "çªç ´"],
                "æ”¿ç­–åˆ©å¤š": ["æ”¿ç­–", "è£œåŠ©", "æ”¿åºœ", "æ³•è¦", "åˆ©å¤š"],
                "å¸‚å ´éœ€æ±‚": ["å¸‚å ´", "éœ€æ±‚", "ä¾›ä¸æ‡‰æ±‚", "ç¼ºè²¨"],
                "ä½µè³¼é¡Œæ": ["ä½µè³¼", "æ”¶è³¼", "åˆä½µ", "æŠ•è³‡", "å…¥è‚¡"],
                "ç”¢å“æ¨å‡º": ["ç”¢å“", "æ–°å“", "æ¨å‡º", "ä¸Šå¸‚", "ç™¼è¡¨"]
            }
            
            for reason, keywords in reason_mapping.items():
                if any(keyword in all_text for keyword in keywords):
                    reason_keywords.append(reason)
            
            # ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
            insight_analysis = self._generate_insight_analysis(stock_name, sector_keywords, reason_keywords)
            
            return {
                "reason_summary": f"æ ¹æ“šæœ€æ–°æ¶ˆæ¯åˆ†æï¼Œå¯èƒ½åŸå› åŒ…æ‹¬ï¼š{', '.join(reason_keywords[:3]) if reason_keywords else 'å¸‚å ´é—œæ³¨åº¦æå‡'}",
                "sector_analysis": f"æ‰€å±¬é¡è‚¡ï¼š{', '.join(sector_keywords) if sector_keywords else 'ç¶œåˆé¡è‚¡'}",
                "insight_analysis": insight_analysis
            }
            
        except Exception as e:
            logger.error(f"âŒ åˆ†ææ–°èå…§å®¹å¤±æ•—: {e}")
            return {
                "reason_summary": "å¸‚å ´é—œæ³¨åº¦æå‡ï¼Œè³‡é‡‘æµå…¥æ˜é¡¯",
                "sector_analysis": "ç¶œåˆé¡è‚¡",
                "insight_analysis": "å»ºè­°æŒçºŒé—œæ³¨å¾ŒçºŒç™¼å±•"
            }
    
    def _generate_insight_analysis(self, stock_name: str, sectors: list, reasons: list) -> str:
        """ç”Ÿæˆæ·±åº¦æ´å¯Ÿåˆ†æï¼ˆæ ¹æ“šå…§å®¹é•·åº¦èª¿æ•´ï¼‰"""
        insights = []
        
        # é¡è‚¡é€£å‹•åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        if "è¨˜æ†¶é«”" in sectors:
            insights.append("è¨˜æ†¶é«”é¡è‚¡å—æƒ AIéœ€æ±‚çˆ†ç™¼")
        elif "ç‡Ÿå»º" in sectors:
            insights.append("ç‡Ÿå»ºé¡è‚¡å—æƒ æ”¿ç­–åˆ©å¤š")
        elif "åŠå°é«”" in sectors:
            insights.append("åŠå°é«”é¡è‚¡AIã€5Géœ€æ±‚æˆé•·")
        elif "é›»å­" in sectors:
            insights.append("é›»å­é¡è‚¡çµ‚ç«¯éœ€æ±‚å›æº«")
        
        # æ¼²åœåŸå› æ·±åº¦åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        if "æ¥­ç¸¾æˆé•·" in reasons:
            insights.append("åŸºæœ¬é¢æ”¹å–„æ”¯æ’è‚¡åƒ¹")
        elif "è¨‚å–®å¢åŠ " in reasons:
            insights.append("è¨‚å–®èƒ½è¦‹åº¦æå‡")
        elif "æŠ€è¡“çªç ´" in reasons:
            insights.append("æŠ€è¡“å‰µæ–°æ˜¯ç«¶çˆ­å„ªå‹¢")
        
        # å¸‚å ´æƒ…ç·’åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        insights.append("å¸‚å ´å‰æ™¯æ¨‚è§€ï¼Œæ³¨æ„è¿½é«˜é¢¨éšª")
        
        return "ï¼›".join(insights) if insights else "å»ºè­°æŒçºŒé—œæ³¨å¾ŒçºŒç™¼å±•"
    
    def _generate_fallback_insight(self, stock_name: str, sector: str, reasons: list) -> str:
        """ç”Ÿæˆå‚™ç”¨æ·±åº¦æ´å¯Ÿåˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰"""
        insights = []
        
        # é¡è‚¡é€£å‹•åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        if sector == "è¨˜æ†¶é«”":
            insights.append("è¨˜æ†¶é«”é¡è‚¡å—æƒ AIéœ€æ±‚çˆ†ç™¼ï¼Œåƒ¹æ ¼æ­¢è·Œå›å‡")
        elif sector == "åŠå°é«”":
            insights.append("åŠå°é«”é¡è‚¡åœ¨AIã€5Gå¸¶å‹•ä¸‹ï¼Œå…ˆé€²è£½ç¨‹éœ€æ±‚æˆé•·")
        elif sector == "é›»å­":
            insights.append("é›»å­é¡è‚¡å—æƒ çµ‚ç«¯éœ€æ±‚å›æº«ï¼Œæ–°èˆˆæ‡‰ç”¨å¸¶å‹•æˆé•·")
        elif sector == "æ–°èƒ½æº":
            insights.append("æ–°èƒ½æºé¡è‚¡æ”¿ç­–æ”¯æŒï¼Œé•·æœŸæˆé•·è¶¨å‹¢æ˜ç¢º")
        elif sector == "é‹¼éµ":
            insights.append("é‹¼éµé¡è‚¡å—æƒ åŸºç¤å»ºè¨­éœ€æ±‚ï¼ŒåŸç‰©æ–™åƒ¹æ ¼ä¸Šæ¼²")
        
        # æ¼²åœåŸå› æ·±åº¦åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        if "æ¥­ç¸¾æˆé•·" in reasons or "æ¥­ç¸¾" in str(reasons):
            insights.append("åŸºæœ¬é¢æ”¹å–„æ”¯æ’è‚¡åƒ¹ï¼Œé—œæ³¨å¾ŒçºŒè²¡å ±")
        elif "æŠ€è¡“çªç ´" in reasons or "æŠ€è¡“" in str(reasons):
            insights.append("æŠ€è¡“å‰µæ–°æ˜¯é•·æœŸç«¶çˆ­å„ªå‹¢ï¼Œå€¼å¾—è¿½è¹¤")
        elif "å®¢æˆ¶éœ€æ±‚" in reasons or "éœ€æ±‚" in str(reasons):
            insights.append("çµ‚ç«¯éœ€æ±‚å›æº«ï¼Œæœªä¾†ç‡Ÿæ”¶æˆé•·å¯æœŸ")
        
        # å¸‚å ´æƒ…ç·’åˆ†æï¼ˆç°¡æ½”ç‰ˆï¼‰
        insights.append("å¸‚å ´å‰æ™¯æ¨‚è§€ï¼Œä½†éœ€æ³¨æ„è¿½é«˜é¢¨éšª")
        
        return "ï¼›".join(insights) if insights else "å»ºè­°æŒçºŒé—œæ³¨å¾ŒçºŒç™¼å±•"
    
    def _get_fallback_analysis(self, stock_name: str, stock_id: str) -> str:
        """ç•¶Serper APIå¤±æ•—æ™‚çš„å‚™ç”¨åˆ†æ"""
        # åŸºæ–¼è‚¡ç¥¨åç¨±å’Œè¡Œæ¥­çš„å¸¸è¦‹æ¼²åœåŸå› 
        fallback_reasons = {
            "è¯é‚¦é›»": ["è¨˜æ†¶é«”éœ€æ±‚", "AIä¼ºæœå™¨", "æ¥­ç¸¾æˆé•·"],
            "å—äºç§‘": ["è¨˜æ†¶é«”å¾©ç”¦", "åƒ¹æ ¼ä¸Šæ¼²", "ç”¢èƒ½æ“´å……"],
            "é‡‡éˆº": ["å…ˆé€²å°è£", "AIæ™¶ç‰‡", "æŠ€è¡“çªç ´"],
            "ç²¾æ¸¬": ["åŠå°é«”æª¢æ¸¬", "å…ˆé€²è£½ç¨‹", "å®¢æˆ¶éœ€æ±‚"],
            "AES-KY": ["é›»å‹•è»Šé›»æ± ", "å„²èƒ½ç³»çµ±", "æ–°èƒ½æº"],
            "æ—ºçŸ½": ["åŠå°é«”è¨­å‚™", "å…ˆé€²è£½ç¨‹", "å®¢æˆ¶æ“´ç”¢"],
            "ç©å´´": ["æ¸¬è©¦ä»‹é¢", "å…ˆé€²å°è£", "AIéœ€æ±‚"],
            "ä¸–ç´€": ["é‹¼éµéœ€æ±‚", "åŸºç¤å»ºè¨­", "åƒ¹æ ¼ä¸Šæ¼²"],
            "é †é”": ["é›»æ± æ¨¡çµ„", "é›»å‹•è»Š", "å„²èƒ½ç³»çµ±"],
            "å“å®‰": ["è¨˜æ†¶é«”æ¨¡çµ„", "é›»å­å…ƒä»¶", "å®¢æˆ¶éœ€æ±‚"],
            "åŠ ç™¾è£•": ["æ•£ç†±æ¨¡çµ„", "é›»å­æ•£ç†±", "å®¢æˆ¶æ“´ç”¢"],
            "é”èˆˆææ–™": ["é›»å­ææ–™", "åŠå°é«”ææ–™", "æŠ€è¡“çªç ´"],
            "å®ç¢©ç³»çµ±": ["ç³»çµ±æ•´åˆ", "é›»å­ç³»çµ±", "å®¢æˆ¶åˆä½œ"],
            "é¦¥é´»": ["é›»å­å…ƒä»¶", "é€£æ¥å™¨", "å®¢æˆ¶éœ€æ±‚"],
            "æ¦®ç¾¤": ["é›»å­å…ƒä»¶", "é€£æ¥å™¨", "å®¢æˆ¶åˆä½œ"],
            "æ™¶è±ªç§‘": ["è¨˜æ†¶é«”IC", "è¨˜æ†¶é«”æ§åˆ¶", "å®¢æˆ¶éœ€æ±‚"],
            "é‡‘å±…": ["éŠ…ç®”åŸºæ¿", "PCBææ–™", "å®¢æˆ¶æ“´ç”¢"],
            "ç³»çµ±é›»": ["é›»æºç³»çµ±", "é›»å­ç³»çµ±", "å®¢æˆ¶éœ€æ±‚"],
            "ç¾¤è¯": ["NANDæ§åˆ¶IC", "è¨˜æ†¶é«”æ§åˆ¶", "å®¢æˆ¶éœ€æ±‚"],
            "ä¸­ç’°": ["å…‰ç¢Ÿç‰‡", "å„²å­˜åª’é«”", "è½‰å‹é¡Œæ"]
        }
        
        # é¡è‚¡åˆ†æ
        sector_mapping = {
            "è¯é‚¦é›»": "è¨˜æ†¶é«”",
            "å—äºç§‘": "è¨˜æ†¶é«”", 
            "ç¾¤è¯": "è¨˜æ†¶é«”",
            "é‡‡éˆº": "åŠå°é«”",
            "ç²¾æ¸¬": "åŠå°é«”",
            "ç©å´´": "åŠå°é«”",
            "æ—ºçŸ½": "åŠå°é«”",
            "é”èˆˆææ–™": "é›»å­ææ–™",
            "é‡‘å±…": "é›»å­ææ–™",
            "ç³»çµ±é›»": "é›»å­",
            "åŠ ç™¾è£•": "é›»å­",
            "å“å®‰": "é›»å­",
            "å®ç¢©ç³»çµ±": "é›»å­",
            "é¦¥é´»": "é›»å­",
            "æ¦®ç¾¤": "é›»å­",
            "æ™¶è±ªç§‘": "é›»å­",
            "AES-KY": "æ–°èƒ½æº",
            "é †é”": "æ–°èƒ½æº",
            "ä¸–ç´€": "é‹¼éµ",
            "ä¸­ç’°": "å…‰é›»"
        }
        
        reasons = fallback_reasons.get(stock_name, ["å¸‚å ´é—œæ³¨", "è³‡é‡‘æµå…¥", "é¡Œæç™¼é…µ"])
        sector = sector_mapping.get(stock_name, "ç¶œåˆé¡è‚¡")
        
        # ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
        insight = self._generate_fallback_insight(stock_name, sector, reasons)
        
        return f"""

ğŸ“ˆ **æ¼²åœåŸå› **: æ ¹æ“šå¸‚å ´è§€å¯Ÿï¼Œå¯èƒ½åŸå› åŒ…æ‹¬ï¼š{', '.join(reasons[:3])}

ğŸ¢ **é¡è‚¡é€£å‹•**: {sector}

ğŸ’¡ **æ·±åº¦æ´å¯Ÿ**: {insight}

ğŸ“° **ç›¸é—œæ–°è**: å»ºè­°é—œæ³¨å…¬å¸å®˜ç¶²åŠè²¡ç¶“åª’é«”æœ€æ–°æ¶ˆæ¯
"""
    
    async def _record_limit_up_post_to_sheets(self, stock, kol_serial, kol_nickname, content, content_outline, is_high_volume, prompt_data=None):
        """è¨˜éŒ„æ¼²åœè‚¡è²¼æ–‡åˆ° Google Sheets"""
        try:
            from datetime import datetime
            
            # è™•ç†å…§å®¹é•·åº¦å•é¡Œ - æˆªæ–·éé•·çš„å…§å®¹
            def truncate_content(text, max_length=50000):
                """æˆªæ–·éé•·çš„å…§å®¹ï¼Œä¿ç•™å‰éƒ¨åˆ†"""
                if len(text) > max_length:
                    return text[:max_length] + "...[å…§å®¹å·²æˆªæ–·]"
                return text
            
            # è™•ç†å…§å®¹ï¼Œç§»é™¤éé•·çš„æ–°èé€£çµéƒ¨åˆ†
            def clean_content(text):
                """æ¸…ç†å…§å®¹ï¼Œç§»é™¤éé•·çš„æ–°èé€£çµ"""
                # å¦‚æœå…§å®¹åŒ…å«å¤§é‡æ–°èé€£çµï¼Œåªä¿ç•™ä¸»è¦å…§å®¹
                if "ğŸ“° **ç›¸é—œæ–°è**:" in text:
                    parts = text.split("ğŸ“° **ç›¸é—œæ–°è**:")
                    main_content = parts[0].strip()
                    # åªä¿ç•™å‰2å€‹æ–°èé€£çµ
                    if len(parts) > 1:
                        news_part = parts[1]
                        lines = news_part.split('\n')
                        news_lines = [line for line in lines if line.strip().startswith('â€¢')]
                        if len(news_lines) > 2:
                            news_lines = news_lines[:2]
                            news_part = '\n'.join(news_lines)
                            return main_content + "\n\nğŸ“° **ç›¸é—œæ–°è**:\n" + news_part
                    return main_content
                return text
            
            # æ¸…ç†å…§å®¹
            cleaned_content = clean_content(content)
            cleaned_content = truncate_content(cleaned_content, 30000)  # é™åˆ¶åœ¨30Kå­—ç¬¦å…§
            
            # æº–å‚™è¨˜éŒ„æ•¸æ“š
            record_data = {
                # åŸºç¤æ¬„ä½
                'test_post_id': f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{kol_serial}",
                'test_time': datetime.now().isoformat(),
                'test_status': 'ready_to_post',
                'trigger_type': 'limit_up_stock_smart',
                'status': 'ready_to_post',
                'priority_level': 'high',
                'batch_id': f"batch_{datetime.now().strftime('%Y%m%d')}",
                
                # KOL ç›¸é—œæ¬„ä½
                'kol_serial': kol_serial,
                'kol_nickname': kol_nickname,
                'kol_id': kol_serial,
                'persona': kol_nickname,
                'writing_style': 'smart_allocated',
                'tone': 'professional',
                'key_phrases': 'æ¼²åœè‚¡,ç›¤å¾Œå›é¡§',
                'avoid_topics': '',
                'preferred_data_sources': 'finlab_api,serper_api',
                'kol_assignment_method': 'smart_allocated',
                'kol_weight': 8,
                'kol_version': 'v4.0',
                'kol_learning_score': 8.5,
                
                # è‚¡ç¥¨/è©±é¡Œç›¸é—œæ¬„ä½
                'stock_id': stock.stock_id,
                'stock_name': stock.stock_name,
                'topic_category': 'limit_up_stock_smart',
                'analysis_type': 'high_volume' if is_high_volume else 'low_volume',
                'analysis_type_detail': f"{'high' if is_high_volume else 'low'}_volume_analysis",
                'topic_priority': 'high',
                'topic_heat_score': 9.0,
                'topic_id': f"limit_up_smart_{stock.stock_id}_{datetime.now().strftime('%Y%m%d')}",
                'topic_title': f"{stock.stock_name} æ™ºèƒ½èª¿é…æ¼²åœåˆ†æ",
                'topic_keywords': f"æ¼²åœè‚¡,{stock.stock_name},æ™ºèƒ½èª¿é…",
                'is_stock_trigger': True,
                'stock_trigger_type': 'limit_up_smart',
                
                # å…§å®¹ç›¸é—œæ¬„ä½
                'title': prompt_data.get("title", f"{stock.stock_name} æ™ºèƒ½èª¿é…æ¼²åœåˆ†æ") if prompt_data else f"{stock.stock_name} æ™ºèƒ½èª¿é…æ¼²åœåˆ†æ",
                'content': cleaned_content,  # ä½¿ç”¨æ¸…ç†å¾Œçš„å…§å®¹
                'content_length': len(cleaned_content),
                'content_style': 'smart_allocated',
                'target_length': len(cleaned_content),
                'weight': 8,
                'random_seed': f"{stock.stock_id}_{kol_serial}_{datetime.now().strftime('%Y%m%d%H%M%S')}",
                'content_quality_score': 9.0,
                'content_type': 'limit_up_smart_analysis',
                'article_length_type': 'medium',
                'content_length_vector': 0.8,
                'tone_vector': 0.7,
                'temperature_setting': 0.8,
                
                # API èª¿ç”¨ç›¸é—œæ¬„ä½
                'apis_to_use': ','.join(content_outline['apis_to_use']) if content_outline else 'serper,volume_analysis',
                'serper_api_called': True,
                'finlab_api_called': True,
                'openai_api_called': True,
                'data_sources_used': 'finlab_api,serper_api,openai_api',
                
                # æˆäº¤é‡ç›¸é—œæ¬„ä½
                'volume_rank': stock.volume_rank,
                'volume_amount': stock.volume_amount,
                'volume_type': 'high_volume' if is_high_volume else 'low_volume',
                'change_percent': stock.change_percent,
                'rank_type': stock.rank_type
            }
            
            # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
            record_list = list(record_data.values())
            
            # å¯«å…¥ Google Sheets
            self.sheets_client.append_sheet("è²¼æ–‡ç´€éŒ„è¡¨", [record_list])
            
            logger.info(f"âœ… è¨˜éŒ„åˆ° Google Sheets: {record_data['test_post_id']}")
            
        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„åˆ° Google Sheets å¤±æ•—: {e}")
            # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œè®“æµç¨‹ç¹¼çºŒ
            logger.warning("âš ï¸ è·³éGoogle Sheetsè¨˜éŒ„ï¼Œç¹¼çºŒåŸ·è¡Œ")
    
    async def _ensure_sheet_headers(self, sheets_client: GoogleSheetsClient, sheet_name: str):
        """ç¢ºä¿ Google Sheets æœ‰æ­£ç¢ºçš„æ¨™é¡Œè¡Œ"""
        try:
            # å®šç¾©è²¼æ–‡ç´€éŒ„è¡¨çš„å®Œæ•´æ¬„ä½ï¼ˆ109å€‹æ¬„ä½ï¼‰
            headers = [
                'test_post_id', 'test_time', 'test_status', 'trigger_type', 'status',
                'priority_level', 'batch_id', 'kol_serial', 'kol_nickname', 'kol_id',
                'persona', 'writing_style', 'tone', 'key_phrases', 'avoid_topics',
                'preferred_data_sources', 'kol_assignment_method', 'kol_weight', 'kol_version',
                'kol_learning_score', 'stock_id', 'stock_name', 'topic_category', 'analysis_type',
                'analysis_type_detail', 'topic_priority', 'topic_heat_score', 'topic_id',
                'topic_title', 'topic_keywords', 'is_stock_trigger', 'stock_trigger_type',
                'title', 'content', 'content_length', 'content_style', 'target_length',
                'weight', 'random_seed', 'content_quality_score', 'content_type',
                'article_length_type', 'content_length_vector', 'tone_vector', 'temperature_setting',
                'openai_model', 'openai_tokens_used', 'prompt_template', 'sentiment_score',
                'ai_detection_risk_score', 'personalization_level', 'creativity_score',
                'coherence_score', 'data_sources_used', 'serper_api_called', 'serper_api_results',
                'serper_api_summary_count', 'finlab_api_called', 'finlab_api_results',
                'cmoney_api_called', 'cmoney_api_results', 'data_quality_score', 'data_freshness',
                'data_manager_dispatch', 'trending_topics_summarized', 'data_interpretability_score',
                'news_count', 'news_summaries', 'news_sentiment', 'news_sources',
                'news_relevance_score', 'news_freshness_score', 'revenue_yoy_growth',
                'revenue_mom_growth', 'eps_value', 'eps_growth', 'gross_margin', 'net_margin',
                'financial_analysis_score', 'price_change_percent', 'volume_ratio', 'rsi_value',
                'macd_signal', 'ma_trend', 'technical_analysis_score', 'technical_confidence',
                'publish_time', 'publish_platform', 'publish_status', 'interaction_count',
                'engagement_rate', 'platform_post_id', 'platform_post_url', 'articleid',
                'learning_insights', 'strategy_adjustments', 'performance_improvement',
                'risk_alerts', 'next_optimization_targets', 'learning_cycle_count',
                'adaptive_score', 'quality_check_rounds', 'quality_issues_record',
                'regeneration_count', 'quality_improvement_record', 'body_parameter',
                'commodity_tags', 'post_id', 'agent_decision_record'
            ]
            
            # æª¢æŸ¥ç¾æœ‰æ¨™é¡Œ
            try:
                existing_data = sheets_client.read_sheet(sheet_name, 'A1:ZZ1')
                if existing_data and len(existing_data) > 0:
                    existing_headers = existing_data[0]
                    if len(existing_headers) != len(headers):
                        logger.warning(f"âš ï¸ Google Sheets æ¨™é¡Œæ¬„ä½æ•¸é‡ä¸åŒ¹é…: æœŸæœ› {len(headers)}ï¼Œå¯¦éš› {len(existing_headers)}")
                        
                        # æª¢æŸ¥æ˜¯å¦åªæ˜¯ç¼ºå°‘æ¬„ä½ï¼Œå¦‚æœæ˜¯çš„è©±ï¼Œåªè¿½åŠ ç¼ºå°‘çš„æ¬„ä½
                        if len(existing_headers) < len(headers):
                            # æ‰¾åˆ°ç¼ºå°‘çš„æ¬„ä½
                            missing_fields = []
                            for field in headers:
                                if field not in existing_headers:
                                    missing_fields.append(field)
                            
                            if missing_fields:
                                # è¿½åŠ ç¼ºå°‘çš„æ¬„ä½åˆ°æœ€å¾Œä¸€åˆ—
                                for i, field in enumerate(missing_fields):
                                    col_letter = self._get_column_letter(len(existing_headers) + i)
                                    sheets_client.update_cell(sheet_name, f"{col_letter}1", field)
                                logger.info(f"âœ… å·²è¿½åŠ ç¼ºå°‘çš„æ¬„ä½: {missing_fields}")
                            else:
                                # å¦‚æœæ‰¾ä¸åˆ°ç¼ºå°‘çš„æ¬„ä½ï¼Œé‡æ–°å¯«å…¥æ¨™é¡Œ
                                sheets_client.write_sheet(sheet_name, [headers], 'A1:ZZ1')
                                logger.info(f"âœ… å·²é‡æ–°å¯«å…¥ Google Sheets æ¨™é¡Œè¡Œ")
                        else:
                            # å¦‚æœå¯¦éš›æ¬„ä½æ¯”æœŸæœ›çš„å¤šï¼Œé‡æ–°å¯«å…¥æ¨™é¡Œ
                            sheets_client.write_sheet(sheet_name, [headers], 'A1:ZZ1')
                            logger.info(f"âœ… å·²é‡æ–°å¯«å…¥ Google Sheets æ¨™é¡Œè¡Œ")
                    else:
                        logger.info(f"âœ… Google Sheets æ¨™é¡Œæ¬„ä½æ•¸é‡æ­£ç¢º: {len(headers)}")
                else:
                    # å¦‚æœæ²’æœ‰æ•¸æ“šï¼Œå¯«å…¥æ¨™é¡Œ
                    sheets_client.write_sheet(sheet_name, [headers], 'A1:ZZ1')
                    logger.info(f"âœ… å·²å‰µå»º Google Sheets æ¨™é¡Œè¡Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ æª¢æŸ¥æ¨™é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œé‡æ–°å‰µå»º: {e}")
                sheets_client.write_sheet(sheet_name, [headers], 'A1:ZZ1')
                logger.info(f"âœ… å·²é‡æ–°å‰µå»º Google Sheets æ¨™é¡Œè¡Œ")
                
        except Exception as e:
            logger.error(f"ç¢ºä¿æ¨™é¡Œè¡Œå¤±æ•—: {e}")
            raise
    
    async def _convert_to_ordered_row(self, record_data: Dict[str, Any]) -> List[str]:
        """å°‡è¨˜éŒ„æ•¸æ“šè½‰æ›ç‚ºæœ‰åºçš„è¡Œæ•¸æ“š"""
        # å®šç¾©æ¬„ä½é †åºï¼ˆèˆ‡æ¨™é¡Œä¸€è‡´ï¼Œ109å€‹æ¬„ä½ï¼‰
        field_order = [
            'test_post_id', 'test_time', 'test_status', 'trigger_type', 'status',
            'priority_level', 'batch_id', 'kol_serial', 'kol_nickname', 'kol_id',
            'persona', 'writing_style', 'tone', 'key_phrases', 'avoid_topics',
            'preferred_data_sources', 'kol_assignment_method', 'kol_weight', 'kol_version',
            'kol_learning_score', 'stock_id', 'stock_name', 'topic_category', 'analysis_type',
            'analysis_type_detail', 'topic_priority', 'topic_heat_score', 'topic_id',
            'topic_title', 'topic_keywords', 'is_stock_trigger', 'stock_trigger_type',
            'title', 'content', 'content_length', 'content_style', 'target_length',
            'weight', 'random_seed', 'content_quality_score', 'content_type',
            'article_length_type', 'content_length_vector', 'tone_vector', 'temperature_setting',
            'openai_model', 'openai_tokens_used', 'prompt_template', 'sentiment_score',
            'ai_detection_risk_score', 'personalization_level', 'creativity_score',
            'coherence_score', 'data_sources_used', 'serper_api_called', 'serper_api_results',
            'serper_api_summary_count', 'finlab_api_called', 'finlab_api_results',
            'cmoney_api_called', 'cmoney_api_results', 'data_quality_score', 'data_freshness',
            'data_manager_dispatch', 'trending_topics_summarized', 'data_interpretability_score',
            'news_count', 'news_summaries', 'news_sentiment', 'news_sources',
            'news_relevance_score', 'news_freshness_score', 'revenue_yoy_growth',
            'revenue_mom_growth', 'eps_value', 'eps_growth', 'gross_margin', 'net_margin',
            'financial_analysis_score', 'price_change_percent', 'volume_ratio', 'rsi_value',
            'macd_signal', 'ma_trend', 'technical_analysis_score', 'technical_confidence',
            'publish_time', 'publish_platform', 'publish_status', 'interaction_count',
            'engagement_rate', 'platform_post_id', 'platform_post_url', 'articleid',
            'learning_insights', 'strategy_adjustments', 'performance_improvement',
            'risk_alerts', 'next_optimization_targets', 'learning_cycle_count',
            'adaptive_score', 'quality_check_rounds', 'quality_issues_record',
            'regeneration_count', 'quality_improvement_record', 'body_parameter',
            'commodity_tags', 'post_id', 'agent_decision_record'
        ]
        
        # æŒ‰ç…§é †åºè½‰æ›æ•¸æ“š
        row_data = []
        for field in field_order:
            value = record_data.get(field, '')
            # è™•ç†ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å… Google Sheets è§£æéŒ¯èª¤
            if isinstance(value, str):
                value = value.replace('\n', ' ').replace('\r', ' ')
            row_data.append(str(value))
        
        return row_data
    
    def _get_column_letter(self, column_index: int) -> str:
        """å°‡æ•¸å­—è½‰æ›ç‚º Google Sheets åˆ—å­—æ¯"""
        result = ""
        while column_index > 0:
            column_index -= 1
            result = chr(ord('A') + (column_index % 26)) + result
            column_index //= 26
        return result
    
    async def _verify_sheet_update(self, sheets_client: GoogleSheetsClient, sheet_name: str, post_id: str) -> bool:
        """é©—è­‰ Google Sheets æ›´æ–°æ˜¯å¦æˆåŠŸ"""
        try:
            # ç­‰å¾…ä¸€ä¸‹è®“ Google Sheets æ›´æ–°
            await asyncio.sleep(3)  # å¢åŠ ç­‰å¾…æ™‚é–“
            
            # è®€å–æœ€å¾Œå¹¾è¡Œæ•¸æ“š
            data = sheets_client.read_sheet(sheet_name)
            if not data or len(data) < 2:  # è‡³å°‘è¦æœ‰æ¨™é¡Œè¡Œå’Œä¸€è¡Œæ•¸æ“š
                logger.error(f"âŒ Google Sheets æ•¸æ“šé©—è­‰å¤±æ•—: æ²’æœ‰æ•¸æ“š")
                return False
            
            # æª¢æŸ¥æœ€å¾Œå¹¾è¡Œæ˜¯å¦åŒ…å«æˆ‘å€‘çš„ post_id
            for i in range(min(10, len(data))):  # æª¢æŸ¥æœ€å¾Œ10è¡Œ
                row = data[-(i+1)]
                if len(row) > 0 and post_id in str(row[0]):
                    logger.info(f"âœ… Google Sheets æ›´æ–°é©—è­‰æˆåŠŸ: åœ¨ç¬¬ {len(data)-i} è¡Œæ‰¾åˆ° post_id {post_id}")
                    return True
            
            # å¦‚æœæ²’æ‰¾åˆ°ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•éç©ºè¡ŒåŒ…å«æˆ‘å€‘çš„ post_id
            for i, row in enumerate(data):
                if len(row) > 0 and post_id in str(row[0]):
                    logger.info(f"âœ… Google Sheets æ›´æ–°é©—è­‰æˆåŠŸ: åœ¨ç¬¬ {i+1} è¡Œæ‰¾åˆ° post_id {post_id}")
                    return True
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œé¡¯ç¤ºæœ€å¾Œå¹¾è¡Œçš„å…§å®¹ä»¥ä¾¿èª¿è©¦
            logger.error(f"âŒ Google Sheets æ›´æ–°é©—è­‰å¤±æ•—: æœªæ‰¾åˆ° post_id {post_id}")
            logger.error(f"   æª¢æŸ¥äº†æœ€å¾Œ {min(10, len(data))} è¡Œ:")
            for i in range(min(10, len(data))):
                row = data[-(i+1)]
                logger.error(f"   ç¬¬ {len(data)-i} è¡Œ: {row[:3]}...")  # åªé¡¯ç¤ºå‰3å€‹æ¬„ä½
            return False
                
        except Exception as e:
            logger.error(f"âŒ Google Sheets æ›´æ–°é©—è­‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    async def _post_workflow_processing(self, config: WorkflowConfig, result: WorkflowResult):
        """å·¥ä½œæµç¨‹å¾Œè™•ç†"""
        logger.info("åŸ·è¡Œå¾Œè™•ç†...")
        
        # 1. å­¸ç¿’æ©Ÿåˆ¶ï¼ˆæš«æ™‚è·³éï¼‰
        if config.enable_learning:
            logger.info("å­¸ç¿’æ©Ÿåˆ¶æš«æ™‚è·³éï¼Œç­‰å¾…ä¸‹ä¸€éšæ®µå¯¦ç¾")
        
        # 2. æ¸…ç†è‡¨æ™‚æ•¸æ“š
        await self._cleanup_temp_data()
        
        # 3. ç”Ÿæˆå ±å‘Š
        await self._generate_workflow_report(result)
        
        logger.info("å¾Œè™•ç†å®Œæˆ")
    
    async def _cleanup_temp_data(self):
        """æ¸…ç†è‡¨æ™‚æ•¸æ“š"""
        # å¯¦ç¾æ¸…ç†é‚è¼¯
        pass
    
    async def _generate_workflow_report(self, result: WorkflowResult):
        """ç”Ÿæˆå·¥ä½œæµç¨‹å ±å‘Š"""
        # å¯¦ç¾å ±å‘Šç”Ÿæˆé‚è¼¯
        pass
    
    async def get_workflow_status(self) -> Dict[str, Any]:
        """ç²å–å·¥ä½œæµç¨‹ç‹€æ…‹"""
        return {
            'is_running': self.is_running,
            'current_workflow': self.current_workflow.value if self.current_workflow else None,
            'start_time': getattr(self, '_start_time', None),
            'uptime': getattr(self, '_uptime', 0)
        }
    
    async def stop_workflow(self):
        """åœæ­¢ç•¶å‰å·¥ä½œæµç¨‹"""
        if self.is_running:
            self.is_running = False
            logger.info("å·¥ä½œæµç¨‹å·²åœæ­¢")
        else:
            logger.info("æ²’æœ‰æ­£åœ¨é‹è¡Œçš„å·¥ä½œæµç¨‹")
    
    async def _get_finlab_revenue_data(self, stock_id: str) -> Optional[Dict[str, Any]]:
        """ç²å– Finlab æœˆç‡Ÿæ”¶æ•¸æ“š"""
        try:
            # ä½¿ç”¨ FinLabDataCache ä¾†æ¸›å°‘ API èª¿ç”¨
            cache_key = f"monthly_revenue_{stock_id}"
            cached_data = self.finlab_cache.get(cache_key)
            if cached_data:
                logger.info(f"âœ… ä½¿ç”¨ç·©å­˜çš„æœˆç‡Ÿæ”¶æ•¸æ“š: {stock_id}")
                return cached_data
            
            # å¦‚æœæ²’æœ‰ç·©å­˜ï¼Œå¾ Finlab API ç²å–æœˆç‡Ÿæ”¶æ•¸æ“š
            revenue_data = await self.finlab_client.get_revenue_data(stock_id)
            if revenue_data:
                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä¸¦æ·»åŠ æœˆç‡Ÿæ”¶æ¨™è­˜
                revenue_dict = {
                    'stock_id': revenue_data.stock_id,
                    'stock_name': revenue_data.stock_name,
                    'period': revenue_data.period,
                    # ç•¶æœˆç‡Ÿæ”¶
                    'current_month_revenue': revenue_data.current_month_revenue,
                    'current_month_revenue_formatted': f"{revenue_data.current_month_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.current_month_revenue < 100000000 else f"{revenue_data.current_month_revenue/100000000:.2f}å„„å…ƒ",
                    # ä¸Šæœˆç‡Ÿæ”¶
                    'last_month_revenue': revenue_data.last_month_revenue,
                    'last_month_revenue_formatted': f"{revenue_data.last_month_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.last_month_revenue < 100000000 else f"{revenue_data.last_month_revenue/100000000:.2f}å„„å…ƒ",
                    # å»å¹´ç•¶æœˆç‡Ÿæ”¶
                    'last_year_same_month_revenue': revenue_data.last_year_same_month_revenue,
                    'last_year_same_month_revenue_formatted': f"{revenue_data.last_year_same_month_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.last_year_same_month_revenue < 100000000 else f"{revenue_data.last_year_same_month_revenue/100000000:.2f}å„„å…ƒ",
                    # æˆé•·ç‡
                    'mom_growth_pct': revenue_data.mom_growth_pct,
                    'yoy_growth_pct': revenue_data.yoy_growth_pct,
                    'ytd_growth_pct': revenue_data.ytd_growth_pct,
                    # ç´¯è¨ˆç‡Ÿæ”¶
                    'ytd_revenue': revenue_data.ytd_revenue,
                    'ytd_revenue_formatted': f"{revenue_data.ytd_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.ytd_revenue < 100000000 else f"{revenue_data.ytd_revenue/100000000:.2f}å„„å…ƒ",
                    'last_year_ytd_revenue': revenue_data.last_year_ytd_revenue,
                    'last_year_ytd_revenue_formatted': f"{revenue_data.last_year_ytd_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.last_year_ytd_revenue < 100000000 else f"{revenue_data.last_year_ytd_revenue/100000000:.2f}å„„å…ƒ",
                    'data_type': 'monthly_revenue',  # æ˜ç¢ºæ¨™ç¤ºç‚ºæœˆç‡Ÿæ”¶
                    # ä¸»è¦ç‡Ÿæ”¶æ•¸æ“šï¼ˆå‘å¾Œå…¼å®¹ï¼‰
                    'revenue': revenue_data.current_month_revenue,
                    'revenue_formatted': f"{revenue_data.current_month_revenue/1000000:.2f}ç™¾è¬å…ƒ" if revenue_data.current_month_revenue < 100000000 else f"{revenue_data.current_month_revenue/100000000:.2f}å„„å…ƒ",
                    'yoy_growth': revenue_data.yoy_growth_pct,
                    'mom_growth': revenue_data.mom_growth_pct,
                    'ytd_growth': revenue_data.ytd_growth_pct
                }
                
                # ç·©å­˜æ•¸æ“š
                self.finlab_cache.set(cache_key, revenue_dict)
                logger.info(f"âœ… ç²å–ä¸¦ç·©å­˜æœˆç‡Ÿæ”¶æ•¸æ“š: {stock_id} - {revenue_dict['revenue_formatted']}")
                return revenue_dict
            else:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å–æœˆç‡Ÿæ”¶æ•¸æ“š: {stock_id}")
                return None
        except Exception as e:
            logger.error(f"âŒ ç²å–æœˆç‡Ÿæ”¶æ•¸æ“šå¤±æ•—: {stock_id}, éŒ¯èª¤: {e}")
            return None

    async def _get_finlab_earnings_data(self, stock_id: str) -> Optional[Dict[str, Any]]:
        """ç²å– Finlab è²¡å ±æ•¸æ“š"""
        try:
            # ä½¿ç”¨ FinLabDataCache ä¾†æ¸›å°‘ API èª¿ç”¨
            cache_key = f"earnings_{stock_id}"
            cached_data = self.finlab_cache.get(cache_key)
            if cached_data:
                logger.info(f"âœ… ä½¿ç”¨ç·©å­˜çš„è²¡å ±æ•¸æ“š: {stock_id}")
                return cached_data
            
            # å¦‚æœæ²’æœ‰ç·©å­˜ï¼Œå¾ Finlab API ç²å–
            earnings_data = await self.finlab_client.get_earnings_data(stock_id)
            if earnings_data:
                # ç·©å­˜æ•¸æ“š
                self.finlab_cache.set(cache_key, earnings_data)
                logger.info(f"âœ… ç²å–ä¸¦ç·©å­˜è²¡å ±æ•¸æ“š: {stock_id}")
                return earnings_data
            else:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å–è²¡å ±æ•¸æ“š: {stock_id}")
                return None
        except Exception as e:
            logger.error(f"âŒ ç²å–è²¡å ±æ•¸æ“šå¤±æ•—: {stock_id}, éŒ¯èª¤: {e}")
            return None

    async def _get_finlab_stock_data(self, stock_id: str) -> Optional[Dict[str, Any]]:
        """ç²å– Finlab è‚¡ç¥¨åƒ¹æ ¼æ•¸æ“š"""
        try:
            # ä½¿ç”¨ FinLabDataCache ä¾†æ¸›å°‘ API èª¿ç”¨
            cache_key = f"stock_{stock_id}"
            cached_data = self.finlab_cache.get(cache_key)
            if cached_data:
                logger.info(f"âœ… ä½¿ç”¨ç·©å­˜çš„è‚¡ç¥¨æ•¸æ“š: {stock_id}")
                return cached_data
            
            # å¦‚æœæ²’æœ‰ç·©å­˜ï¼Œå¾ Finlab API ç²å–
            stock_data = await self.finlab_client.get_stock_data(stock_id)
            if stock_data:
                # ç·©å­˜æ•¸æ“š
                self.finlab_cache.set(cache_key, stock_data)
                logger.info(f"âœ… ç²å–ä¸¦ç·©å­˜è‚¡ç¥¨æ•¸æ“š: {stock_id}")
                return stock_data
            else:
                logger.warning(f"âš ï¸ ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š: {stock_id}")
                return None
        except Exception as e:
            logger.error(f"âŒ ç²å–è‚¡ç¥¨æ•¸æ“šå¤±æ•—: {stock_id}, éŒ¯èª¤: {e}")
            return None
    
    async def _get_serper_news_data(self, stock_id: str, stock_name: str, topic: str) -> Dict[str, Any]:
        """ç²å– Serper æ–°èæ•¸æ“š"""
        try:
            from src.api_integration.serper_api_client import SerperAPIClient
            
            async with SerperAPIClient() as serper_client:
                # å„ªå…ˆæœå°‹è‚¡åƒ¹è®ŠåŒ–åŸå› ï¼ˆä½¿ç”¨å¯ä¿¡åª’é«”ï¼‰
                primary_queries = [
                    f'"{stock_name}" "{stock_id}" æ¼²åœåŸå› ',
                    f'"{stock_name}" "{stock_id}" è‚¡åƒ¹ä¸Šæ¼²åŸå› ',
                    f'"{stock_name}" "{stock_id}" å¤§æ¼²åŸå› ',
                    f'"{stock_name}" "{stock_id}" åˆ©å¤šæ¶ˆæ¯',
                    f'"{stock_name}" "{stock_id}" çªç ´åŸå› '
                ]
                
                # æ ¹æ“šåˆ†æé¡å‹æ·»åŠ ç‰¹å®šæœå°‹
                if topic == "ç‡Ÿæ”¶":
                    primary_queries.extend([
                        f'"{stock_name}" "{stock_id}" æœˆç‡Ÿæ”¶',
                        f'"{stock_name}" "{stock_id}" ç‡Ÿæ”¶',
                        f'"{stock_name}" "{stock_id}" ç‡Ÿæ”¶æˆé•·',
                        f'"{stock_name}" "{stock_id}" æœˆç‡Ÿæ”¶å…¬å‘Š',
                        f'"{stock_name}" "{stock_id}" ç‡Ÿæ”¶äº®çœ¼'
                    ])
                elif topic == "è²¡å ±":
                    primary_queries.extend([
                        f'"{stock_name}" "{stock_id}" è²¡å ±',
                        f'"{stock_name}" "{stock_id}" EPS',
                        f'"{stock_name}" "{stock_id}" ç²åˆ©'
                    ])
                elif topic == "æ–°è":
                    primary_queries.extend([
                        f'"{stock_name}" "{stock_id}" æ–°è',
                        f'"{stock_name}" "{stock_id}" æœ€æ–°æ¶ˆæ¯',
                        f'"{stock_name}" "{stock_id}" ç”¢æ¥­å‹•æ…‹'
                    ])
                elif topic == "è‚¡åƒ¹":
                    primary_queries.extend([
                        f'"{stock_name}" "{stock_id}" è‚¡åƒ¹',
                        f'"{stock_name}" "{stock_id}" æŠ€è¡“åˆ†æ',
                        f'"{stock_name}" "{stock_id}" æˆäº¤é‡'
                    ])
                
                all_news = []
                for query in primary_queries:
                    try:
                        news_data = await serper_client.search_news(query, 3)  # æ¯å€‹æŸ¥è©¢å–3å‰‡
                        if news_data and news_data.news_results:
                            logger.debug(f"æŸ¥è©¢ '{query}' æ‰¾åˆ° {len(news_data.news_results)} å‰‡æ–°è")
                            all_news.extend(news_data.news_results)
                        else:
                            logger.debug(f"æŸ¥è©¢ '{query}' æ²’æœ‰æ‰¾åˆ°æ–°è")
                    except Exception as e:
                        logger.warning(f"æœå°‹ '{query}' å¤±æ•—: {e}")
                
                # å»é‡ä¸¦æ’åº
                unique_news = []
                seen_titles = set()
                seen_links = set()  # åŒæ™‚æª¢æŸ¥é€£çµå»é‡
                for news in all_news:
                    # æª¢æŸ¥æ¨™é¡Œå’Œé€£çµæ˜¯å¦é‡è¤‡
                    if news.title not in seen_titles and news.link not in seen_links:
                        unique_news.append(news)
                        seen_titles.add(news.title)
                        seen_links.add(news.link)
                
                if unique_news:
                    logger.info(f"âœ… ç²å–åˆ° {stock_name} ç›¸é—œæ–°è {len(unique_news)} å‰‡")
                    
                    # è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä»¥ä¾¿å¾ŒçºŒè™•ç†
                    news_dict_list = []
                    for news in unique_news:
                        news_dict = {
                            'title': news.title,
                            'link': news.link,
                            'snippet': news.snippet,
                            'source': getattr(news, 'source', ''),
                            'date': getattr(news, 'date', '')
                        }
                        news_dict_list.append(news_dict)
                    
                    return {
                        'news': news_dict_list,
                        'news_count': len(unique_news),
                        'news_summaries': [news.snippet for news in unique_news[:3]],
                        'news_titles': [news.title for news in unique_news[:3]],
                        'news_sentiment': 'positive',
                        'has_news': True,
                        'query': f"{stock_name} {stock_id} {topic}"
                    }
                else:
                    logger.warning(f"âš ï¸ ç„¡æ³•ç²å– {stock_name} æ–°èæ•¸æ“š")
                    return {
                        'news': [],
                        'news_count': 0,
                        'news_summaries': [],
                        'news_titles': [],
                        'news_sentiment': 'neutral',
                        'has_news': False,
                        'query': f"{stock_name} {stock_id} {topic}"
                    }
        except Exception as e:
            logger.error(f"ç²å– {stock_name} æ–°èæ•¸æ“šå¤±æ•—: {e}")
            return {
                'news': [],
                'news_count': 0,
                'news_summaries': [],
                'news_titles': [],
                'news_sentiment': 'neutral',
                'has_news': False,
                'query': f"{stock_name} {stock_id} {topic}"
            }
    
    async def _generate_openai_content(self, stock_id: str, stock_name: str, kol_serial: str, 
                                     kol_settings: Dict[str, Any], analysis_type: str, 
                                     finlab_data: Optional[Dict[str, Any]], serper_data: Optional[Dict[str, Any]], 
                                     target_length: int) -> Dict[str, str]:
        """ä½¿ç”¨ OpenAI API ç”Ÿæˆå…§å®¹"""
        try:
            # æª¢æŸ¥æ•¸æ“šå¯ç”¨æ€§
            if finlab_data is None:
                logger.warning(f"âš ï¸ Finlab æ•¸æ“šä¸å¯ç”¨ï¼Œèª¿æ•´åˆ†æé¡å‹ç‚ºæ–°èåˆ†æ")
                analysis_type = "news_analysis"
            
            if serper_data is None:
                logger.warning(f"âš ï¸ Serper æ•¸æ“šä¸å¯ç”¨")
            
            # ä½¿ç”¨å®‰å…¨çš„æ•¸æ“šè¨ªå•
            finlab_data = finlab_data or {}
            serper_data = serper_data or {}
            
            from src.api_integration.openai_api_client import OpenAIAPIClient, ContentGenerationRequest
            
            async with OpenAIAPIClient() as openai_client:
                request = ContentGenerationRequest(
                    stock_id=stock_id,
                    stock_name=stock_name,
                    analysis_type=analysis_type,
                    kol_profile=kol_settings,
                    stock_data=finlab_data,
                    news_data=serper_data,
                    target_length=target_length,
                    content_style="åˆ†æ"
                )
                
                result = await openai_client.generate_content(request)
                if result:
                    logger.info(f"âœ… OpenAI ç”Ÿæˆå…§å®¹æˆåŠŸ: {result.title[:50]}...")
                    
                    # æ·»åŠ æ–°èé€£çµåˆ°å…§å®¹æœ«å°¾
                    content_with_links = await self._extract_and_add_news_links(result.content, serper_data)
                    
                    return {
                        'title': result.title,
                        'content': content_with_links,
                        'tokens_used': result.tokens_used,
                        'quality_score': result.quality_score,
                        'data_availability': {
                            'finlab_available': finlab_data is not None,
                            'serper_available': serper_data is not None
                        }
                    }
                else:
                    logger.warning(f"âš ï¸ OpenAI ç”Ÿæˆå…§å®¹å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨å…§å®¹")
                    return {
                        'title': f"{stock_name} {analysis_type} åˆ†æ",
                        'content': f"{stock_name}({stock_id}) ä»Šæ—¥è¡¨ç¾äº®çœ¼ï¼\n\nğŸ“Š åˆ†æé‡é»ï¼š\nâ€¢ åŸºæœ¬é¢å¼·å‹\nâ€¢ æŠ€è¡“é¢çªç ´\nâ€¢ æŠ•è³‡åƒ¹å€¼é¡¯ç¾\n\n#æ¼²åœè‚¡ #{analysis_type} #{stock_id}",
                        'tokens_used': 0,
                        'quality_score': 7.0,
                        'data_availability': {
                            'finlab_available': finlab_data is not None,
                            'serper_available': serper_data is not None
                        }
                    }
        except Exception as e:
            logger.error(f"OpenAI ç”Ÿæˆå…§å®¹å¤±æ•—: {e}")
            return {
                'title': f"{stock_name} {analysis_type} åˆ†æ",
                'content': f"{stock_name}({stock_id}) ä»Šæ—¥è¡¨ç¾äº®çœ¼ï¼\n\nğŸ“Š åˆ†æé‡é»ï¼š\nâ€¢ åŸºæœ¬é¢å¼·å‹\nâ€¢ æŠ€è¡“é¢çªç ´\nâ€¢ æŠ•è³‡åƒ¹å€¼é¡¯ç¾\n\n#æ¼²åœè‚¡ #{analysis_type} #{stock_id}",
                'tokens_used': 0,
                'quality_score': 7.0,
                'data_availability': {
                    'finlab_available': finlab_data is not None,
                    'serper_available': serper_data is not None
                }
            }
    
    async def _save_to_local_backup(self, record_data: Dict[str, Any]):
        """ä¿å­˜åˆ°æœ¬åœ°å‚™ä»½æ–‡ä»¶"""
        try:
            import json
            import os
            from datetime import datetime
            
            # å‰µå»ºå‚™ä»½ç›®éŒ„
            backup_dir = "data/backup"
            os.makedirs(backup_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"post_record_{timestamp}_{record_data.get('post_id', 'unknown')}.json"
            filepath = os.path.join(backup_dir, filename)
            
            # ä¿å­˜æ•¸æ“š
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(record_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æœ¬åœ°å‚™ä»½å·²ä¿å­˜: {filepath}")
            
            # åŒæ™‚ä¿å­˜åˆ° CSV æ ¼å¼ï¼ˆæ–¹ä¾¿å°å…¥ Google Sheetsï¼‰
            csv_filename = f"post_record_{timestamp}_{record_data.get('post_id', 'unknown')}.csv"
            csv_filepath = os.path.join(backup_dir, csv_filename)
            
            # å°‡æ•¸æ“šè½‰æ›ç‚º CSV æ ¼å¼
            csv_data = []
            for key, value in record_data.items():
                csv_data.append(f'"{key}","{str(value).replace(chr(34), chr(34)+chr(34))}"')
            
            with open(csv_filepath, 'w', encoding='utf-8') as f:
                f.write("æ¬„ä½åç¨±,æ¬„ä½å€¼\n")  # CSV æ¨™é¡Œ
                f.write("\n".join(csv_data))
            
            logger.info(f"ğŸ“Š CSV æ ¼å¼å·²ä¿å­˜: {csv_filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æœ¬åœ°å‚™ä»½å¤±æ•—: {e}")
    
    def get_kol_article_tracker(self) -> Dict[str, List[str]]:
        """ç²å– KOL Article ID è¿½è¹¤å™¨"""
        return self.kol_article_tracker
    
    async def add_article_to_tracker(self, kol_serial: str, article_id: str):
        """æ·»åŠ  Article ID åˆ°è¿½è¹¤å™¨"""
        if kol_serial not in self.kol_article_tracker:
            self.kol_article_tracker[kol_serial] = []
        self.kol_article_tracker[kol_serial].append(article_id)
        logger.info(f"âœ… æ·»åŠ  Article ID {article_id} åˆ° KOL {kol_serial} è¿½è¹¤å™¨")

    async def _extract_and_add_news_links(self, content: str, serper_data: Optional[Dict[str, Any]], max_links: int = 3) -> str:
        """æå–ä¸¦æ·»åŠ æ–°èé€£çµåˆ°å…§å®¹æœ«å°¾"""
        try:
            if not serper_data or 'news' not in serper_data:
                return content
            
            news_list = serper_data['news']
            if not news_list:
                return content
            
            # å¾æŸ¥è©¢ä¸­æå–è‚¡ç¥¨ä¿¡æ¯
            query = serper_data.get('query', '')
            query_parts = query.lower().split()
            stock_id = ""
            stock_name = ""
            
            # æå–è‚¡ç¥¨ä»£è™Ÿ
            for part in query_parts:
                if len(part) == 4 and part.isdigit():
                    stock_id = part
                    break
            
            # æå–è‚¡ç¥¨åç¨±
            if stock_id:
                try:
                    stock_name_index = query_parts.index(stock_id) - 1
                    if stock_name_index >= 0:
                        stock_name = query_parts[stock_name_index]
                except:
                    pass
            
            # æ ¹æ“šç›¸é—œæ€§å’Œè‚¡ç¥¨åŒ¹é…åº¦æ’åºæ–°è
            scored_news = []
            for news in news_list:
                score = self._calculate_news_relevance_score(news, query)
                
                # é¡å¤–æª¢æŸ¥ï¼šç¢ºä¿æ–°èèˆ‡è‚¡ç¥¨ç›¸é—œ
                title = news.get('title', '').lower()
                snippet = news.get('snippet', '').lower()
                
                # å¦‚æœæ–°èèˆ‡è‚¡ç¥¨å®Œå…¨ç„¡é—œï¼Œå¤§å¹…æ‰£åˆ†
                if stock_id and stock_id not in title and stock_id not in snippet:
                    if stock_name and stock_name not in title and stock_name not in snippet:
                        score -= 20.0  # åš´é‡æ‰£åˆ†ï¼Œç¢ºä¿ä¸æœƒè¢«é¸ä¸­
                
                scored_news.append((score, news))
            
            # æŒ‰åˆ†æ•¸æ’åºï¼Œå–å‰ max_links å€‹
            scored_news.sort(key=lambda x: x[0], reverse=True)
            top_news = scored_news[:max_links]
            
            # åªé¡¯ç¤ºåˆ†æ•¸ç‚ºæ­£çš„æ–°èï¼ˆç¢ºä¿ç›¸é—œæ€§ï¼‰
            relevant_news = [(score, news) for score, news in top_news if score > 0]
            
            if not relevant_news:
                logger.warning(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°èˆ‡ {stock_name}({stock_id}) ç›¸é—œçš„æ–°è")
                return content
            
            # æ·»åŠ æ–°èé€£çµåˆ°å…§å®¹æœ«å°¾
            links_section = "\n\nğŸ“° ç›¸é—œæ–°èé€£çµï¼š\n"
            for i, (score, news) in enumerate(relevant_news, 1):
                title = news.get('title', 'ç„¡æ¨™é¡Œ')
                link = news.get('link', '')
                if link:
                    # ä½¿ç”¨ CMoney çš„è¶…é€£çµæ ¼å¼ï¼š[æ–‡å­—](URL)
                    links_section += f"{i}. [{title}]({link})\n\n"
            
            logger.info(f"âœ… ç‚º {stock_name}({stock_id}) æ·»åŠ äº† {len(relevant_news)} å€‹ç›¸é—œæ–°èé€£çµ")
            return content + links_section
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–°èé€£çµå¤±æ•—: {e}")
            return content
    
    def _calculate_news_relevance_score(self, news: Dict[str, Any], query: str) -> float:
        """è¨ˆç®—æ–°èç›¸é—œæ€§åˆ†æ•¸"""
        try:
            score = 0.0
            title = news.get('title', '').lower()
            snippet = news.get('snippet', '').lower()
            
            # å¾æŸ¥è©¢ä¸­æå–è‚¡ç¥¨åç¨±å’Œä»£è™Ÿ
            query_parts = query.lower().split()
            stock_name = ""
            stock_id = ""
            
            # æå–è‚¡ç¥¨ä»£è™Ÿï¼ˆ4ä½æ•¸å­—ï¼‰
            for part in query_parts:
                if len(part) == 4 and part.isdigit():
                    stock_id = part
                    break
            
            # æå–è‚¡ç¥¨åç¨±ï¼ˆé€šå¸¸åœ¨è‚¡ç¥¨ä»£è™Ÿå‰é¢ï¼‰
            if stock_id:
                try:
                    stock_name_index = query_parts.index(stock_id) - 1
                    if stock_name_index >= 0:
                        stock_name = query_parts[stock_name_index]
                except:
                    pass
            
            # è‚¡ç¥¨ç›¸é—œæ€§æª¢æŸ¥ï¼ˆæœ€é‡è¦ï¼‰
            if stock_id and stock_id in title:
                score += 10.0  # è‚¡ç¥¨ä»£è™ŸåŒ¹é…çµ¦äºˆæœ€é«˜åˆ†
            elif stock_id and stock_id in snippet:
                score += 8.0
            
            if stock_name and stock_name in title:
                score += 8.0  # è‚¡ç¥¨åç¨±åŒ¹é…çµ¦äºˆé«˜åˆ†
            elif stock_name and stock_name in snippet:
                score += 6.0
            
            # å¦‚æœæ–°èèˆ‡è‚¡ç¥¨å®Œå…¨ç„¡é—œï¼Œçµ¦äºˆè² åˆ†
            if stock_id and stock_id not in title and stock_id not in snippet:
                if stock_name and stock_name not in title and stock_name not in snippet:
                    score -= 5.0  # åš´é‡æ‰£åˆ†
            
            # é—œéµè©åŒ¹é…åˆ†æ•¸
            keywords = ['æ¼²åœ', 'å¤§æ¼²', 'é£†æ¼²', 'çªç ´', 'åˆ©å¤š', 'å¥½æ¶ˆæ¯', 'ç‡Ÿæ”¶', 'è²¡å ±', 'ç²åˆ©', 'æˆé•·']
            for keyword in keywords:
                if keyword in title:
                    score += 2.0
                if keyword in snippet:
                    score += 1.0
            
            # æ™‚é–“ç›¸é—œæ€§ï¼ˆè¶Šæ–°è¶Šå¥½ï¼‰
            if 'date' in news:
                try:
                    from datetime import datetime
                    news_date = datetime.fromisoformat(news['date'].replace('Z', '+00:00'))
                    days_old = (datetime.now() - news_date).days
                    if days_old <= 1:
                        score += 3.0
                    elif days_old <= 3:
                        score += 2.0
                    elif days_old <= 7:
                        score += 1.0
                except:
                    pass
            
            # ä¾†æºå¯ä¿¡åº¦
            trusted_sources = ['udn.com', 'ctee.com.tw', 'money.udn.com', 'ec.ltn.com.tw', 
                             'www.chinatimes.com', 'www.cnyes.com', 'news.cnyes.com']
            source = news.get('source', '').lower()
            for trusted in trusted_sources:
                if trusted in source:
                    score += 1.5
                    break
            
            # å…§å®¹é•·åº¦ï¼ˆé©ä¸­çš„å…§å®¹æ›´å¥½ï¼‰
            content_length = len(title) + len(snippet)
            if 50 <= content_length <= 200:
                score += 0.5
            
            return score
            
        except Exception as e:
            logger.error(f"è¨ˆç®—æ–°èç›¸é—œæ€§åˆ†æ•¸å¤±æ•—: {e}")
            return 0.0

    async def _execute_trending_topics_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œç†±é–€è©±é¡Œå·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œç†±é–€è©±é¡Œå·¥ä½œæµç¨‹...")
        
        try:
            # ç°¡åŒ–çš„ç†±é–€è©±é¡Œæµç¨‹å¯¦ç¾
            logger.info("ç†±é–€è©±é¡Œå·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç†±é–€è©±é¡Œå·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_limit_up_stocks_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œæ¼²åœè‚¡ç¥¨å·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œæ¼²åœè‚¡ç¥¨å·¥ä½œæµç¨‹...")
        
        try:
            # ç°¡åŒ–çš„æ¼²åœè‚¡ç¥¨æµç¨‹å¯¦ç¾
            logger.info("æ¼²åœè‚¡ç¥¨å·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¼²åœè‚¡ç¥¨å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_hot_stocks_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œç†±é–€è‚¡ç¥¨å·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œç†±é–€è‚¡ç¥¨å·¥ä½œæµç¨‹...")
        
        try:
            # å¯¦ç¾ç†±é–€è‚¡ç¥¨åˆ†æé‚è¼¯
            # é€™è£¡å¯ä»¥æ•´åˆæ‚¨ä¹‹å‰çš„ç†±é–€è‚¡ç¥¨åˆ†æè…³æœ¬
            logger.info("ç†±é–€è‚¡ç¥¨å·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç†±é–€è‚¡ç¥¨å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_industry_analysis_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œç”¢æ¥­åˆ†æå·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œç”¢æ¥­åˆ†æå·¥ä½œæµç¨‹...")
        
        try:
            # å¯¦ç¾ç”¢æ¥­åˆ†æé‚è¼¯
            # é€™è£¡å¯ä»¥æ•´åˆæ‚¨ä¹‹å‰çš„ç”¢æ¥­åˆ†æè…³æœ¬
            logger.info("ç”¢æ¥­åˆ†æå·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ç”¢æ¥­åˆ†æå·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_monthly_revenue_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œæœˆç‡Ÿæ”¶å·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œæœˆç‡Ÿæ”¶å·¥ä½œæµç¨‹...")
        
        try:
            # å¯¦ç¾æœˆç‡Ÿæ”¶åˆ†æé‚è¼¯
            # é€™è£¡å¯ä»¥æ•´åˆæ‚¨ä¹‹å‰çš„æœˆç‡Ÿæ”¶åˆ†æè…³æœ¬
            logger.info("æœˆç‡Ÿæ”¶å·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æœˆç‡Ÿæ”¶å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_high_volume_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œé«˜æˆäº¤é‡å·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œé«˜æˆäº¤é‡å·¥ä½œæµç¨‹...")
        
        try:
            # å¯¦ç¾é«˜æˆäº¤é‡åˆ†æé‚è¼¯
            # é€™è£¡å¯ä»¥æ•´åˆæ‚¨ä¹‹å‰çš„é«˜æˆäº¤é‡åˆ†æè…³æœ¬
            logger.info("é«˜æˆäº¤é‡å·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"é«˜æˆäº¤é‡å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_news_summary_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œæ–°èæ‘˜è¦å·¥ä½œæµç¨‹"""
        logger.info("åŸ·è¡Œæ–°èæ‘˜è¦å·¥ä½œæµç¨‹...")
        
        try:
            # å¯¦ç¾æ–°èæ‘˜è¦é‚è¼¯
            # é€™è£¡å¯ä»¥æ•´åˆæ‚¨ä¹‹å‰çš„æ–°èæ‘˜è¦è…³æœ¬
            logger.info("æ–°èæ‘˜è¦å·¥ä½œæµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ–°èæ‘˜è¦å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _execute_intraday_surge_stocks_workflow(self, config: WorkflowConfig, result: WorkflowResult):
        """åŸ·è¡Œç›¤ä¸­æ€¥æ¼²è‚¡å·¥ä½œæµç¨‹ - åŸºæ–¼æ‰‹å‹•è¼¸å…¥çš„è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨"""
        logger.info("åŸ·è¡Œç›¤ä¸­æ€¥æ¼²è‚¡å·¥ä½œæµç¨‹...")
        
        try:
            # å°å…¥æ™ºèƒ½èª¿é…ç³»çµ±
            from smart_api_allocator import SmartAPIAllocator, StockAnalysis
            
            # åˆå§‹åŒ–æ™ºèƒ½èª¿é…å™¨
            allocator = SmartAPIAllocator()
            
            # ç²å–æ‰‹å‹•è¼¸å…¥çš„è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨
            stock_ids = await self._get_manual_stock_list()
            
            if not stock_ids:
                logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ‰‹å‹•è¼¸å…¥çš„è‚¡ç¥¨ä»£è™Ÿï¼Œä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                stock_ids = self._create_sample_surge_stocks()
            
            # ç‚ºæ¯éš»è‚¡ç¥¨å‰µå»ºåˆ†æå°è±¡ï¼Œä½¿ç”¨çœŸå¯¦æ•¸æ“š
            surge_stocks = []
            for stock_id in stock_ids:
                # å¾ç’°å¢ƒè®Šæ•¸æˆ–é…ç½®ä¸­ç²å–çœŸå¯¦è‚¡ç¥¨æ•¸æ“š
                stock_data = self._get_real_stock_data(stock_id)
                stock_analysis = StockAnalysis(
                    stock_id=stock_id,
                    stock_name=stock_data.get("name", f"è‚¡ç¥¨{stock_id}"),
                    change_percent=stock_data.get("change_percent", 5.0),
                    volume_amount=stock_data.get("volume_amount", 1.0),  # ä½¿ç”¨æˆäº¤é‡‘é¡(å„„)
                    volume_rank=stock_data.get("volume_rank", 1),
                    rank_type="surge_stock"
                )
                surge_stocks.append(stock_analysis)
            
            # æ™ºèƒ½åˆ†é…APIè³‡æº
            allocated_stocks = allocator.allocate_apis_for_stocks(surge_stocks)
            
            # ç²å– KOL è¨­å®š
            kol_settings = self.config_manager.get_kol_personalization_settings()
            kol_list = list(kol_settings.keys())
            
            # å¹³å‡åˆ†é…KOLï¼ˆç¢ºä¿æ¯å€‹KOLéƒ½è¢«ä½¿ç”¨ï¼‰
            kol_count = len(kol_list)
            stock_count = len(allocated_stocks)
            
            # è¨ˆç®—æ¯å€‹KOLæ‡‰è©²åˆ†é…å¤šå°‘è‚¡ç¥¨
            base_assignment = stock_count // kol_count
            extra_stocks = stock_count % kol_count
            
            selected_kols = []
            for i, kol_id in enumerate(kol_list):
                # å‰å¹¾å€‹KOLå¤šåˆ†é…ä¸€å€‹è‚¡ç¥¨
                assignments = base_assignment + (1 if i < extra_stocks else 0)
                selected_kols.extend([kol_id] * assignments)
            
            # éš¨æ©Ÿæ‰“äº‚é †åº
            import random
            random.shuffle(selected_kols)
            
            # ç”Ÿæˆè²¼æ–‡
            generated_posts = []
            for i, (stock_analysis, kol_id) in enumerate(zip(allocated_stocks, selected_kols)):
                try:
                    logger.info(f"ğŸ“ ç”Ÿæˆç¬¬{i+1}ç¯‡è²¼æ–‡: {stock_analysis.stock_id}")
                    
                    # ç”Ÿæˆå…§å®¹å¤§ç¶±ï¼ˆèˆ‡ç›¤å¾Œæ¼²åœè‚¡é‚è¼¯ä¸€è‡´ï¼‰
                    content_outline = allocator.generate_content_outline(stock_analysis)
                    
                    # æ ¹æ“šæœ‰é‡/ç„¡é‡ç”Ÿæˆä¸åŒé¢¨æ ¼çš„å…§å®¹ï¼ˆèˆ‡ç›¤å¾Œæ¼²åœè‚¡é‚è¼¯ä¸€è‡´ï¼‰
                    is_high_volume = stock_analysis.volume_amount >= 1.0  # 1å„„ä»¥ä¸Šç‚ºæœ‰é‡
                    
                    # ç”Ÿæˆè²¼æ–‡å…§å®¹ï¼ˆä½¿ç”¨èˆ‡ç›¤å¾Œæ¼²åœè‚¡ç›¸åŒçš„é‚è¼¯ï¼‰
                    content, prompt_data = await self._generate_limit_up_post_content(
                        stock_analysis, kol_id, kol_settings[kol_id], 
                        content_outline, is_high_volume
                    )
                    
                    # è¨˜éŒ„åˆ°Google Sheetsï¼ˆä½¿ç”¨æ­£ç¢ºçš„nicknameï¼‰
                    kol_nickname = kol_settings[kol_id].get('nickname', kol_settings[kol_id].get('persona', 'æœªçŸ¥KOL'))
                    post_id = await self._record_limit_up_post_to_sheets(
                        stock_analysis, kol_id, kol_nickname, 
                        content, content_outline, is_high_volume, prompt_data
                    )
                    
                    generated_posts.append({
                        "post_id": post_id,
                        "stock_id": stock_analysis.stock_id,
                        "stock_name": stock_analysis.stock_name,
                        "kol_id": kol_id,
                        "content": content
                    })
                    
                    logger.info(f"âœ… ç¬¬{i+1}ç¯‡è²¼æ–‡ç”Ÿæˆå®Œæˆ: {post_id}")
                    
                except Exception as e:
                    logger.error(f"âŒ ç¬¬{i+1}ç¯‡è²¼æ–‡ç”Ÿæˆå¤±æ•—: {e}")
                    continue
            
            # æ›´æ–°çµæœ
            result.generated_posts = generated_posts
            result.total_posts_generated = len(generated_posts)
            result.total_posts_published = len(generated_posts)
            
            logger.info(f"ğŸ‰ ç›¤ä¸­æ€¥æ¼²è‚¡å·¥ä½œæµç¨‹å®Œæˆï¼å…±ç”Ÿæˆ {len(generated_posts)} ç¯‡è²¼æ–‡")
            
        except Exception as e:
            logger.error(f"ç›¤ä¸­æ€¥æ¼²è‚¡å·¥ä½œæµç¨‹å¤±æ•—: {e}")
            raise

    async def _get_manual_stock_list(self) -> List[str]:
        """ç²å–æ‰‹å‹•è¼¸å…¥çš„è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨"""
        # ç¬¬ä¸€éšæ®µï¼šæ‰‹å‹•è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ
        # æœªä¾†ç¬¬äºŒéšæ®µæœƒæ”¹ç‚ºå¾å³æ™‚APIç²å–
        try:
            # é€™è£¡å¯ä»¥å¾é…ç½®æ–‡ä»¶ã€ç’°å¢ƒè®Šæ•¸æˆ–æ•¸æ“šåº«è®€å–è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨
            # æš«æ™‚è¿”å›ç©ºåˆ—è¡¨ï¼Œè®“ç”¨æˆ¶å¯ä»¥é€šéé…ç½®æ–‡ä»¶è¨­å®š
            stock_ids = os.getenv('MANUAL_STOCK_IDS', '').split(',')
            stock_ids = [stock_id.strip() for stock_id in stock_ids if stock_id.strip()]
            
            if stock_ids:
                logger.info(f"ğŸ“‹ ç²å–åˆ°æ‰‹å‹•è¼¸å…¥çš„è‚¡ç¥¨ä»£è™Ÿ: {stock_ids}")
                return stock_ids
            else:
                logger.info("ğŸ“‹ æœªè¨­å®šæ‰‹å‹•è‚¡ç¥¨ä»£è™Ÿï¼Œå°‡ä½¿ç”¨æ¨£æœ¬æ•¸æ“š")
                return []
                
        except Exception as e:
            logger.error(f"âŒ ç²å–æ‰‹å‹•è‚¡ç¥¨ä»£è™Ÿå¤±æ•—: {e}")
            return []

    def _create_sample_surge_stocks(self) -> List[str]:
        """å‰µå»ºæ¨£æœ¬æ€¥æ¼²è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨"""
        return ["2330", "2317", "2454", "3008", "3711"]

    async def _generate_surge_stock_post_content(self, stock_analysis: 'StockAnalysis', kol_id: str) -> str:
        """ç”Ÿæˆç›¤ä¸­æ€¥æ¼²è‚¡è²¼æ–‡å…§å®¹"""
        try:
            from src.services.content.content_generator import ContentGenerator, ContentRequest
            from src.services.content.enhanced_prompt_templates import enhanced_prompt_templates
            
            # ç²å–KOLé¢¨æ ¼
            from src.services.content.enhanced_prompt_templates import enhanced_prompt_templates
            kol_settings = self.config_manager.get_kol_personalization_settings().get(kol_id, {})
            kol_style = self._get_kol_style(kol_settings, enhanced_prompt_templates)
            
            # æ ¼å¼åŒ–æˆäº¤é‡
            volume_formatted = self._format_volume_amount(stock_analysis.volume_amount)
            
            # æ§‹å»ºæç¤ºè©
            prompt_data = enhanced_prompt_templates.build_surge_stock_prompt(
                stock_data={
                    'stock_id': stock_analysis.stock_id,
                    'stock_name': stock_analysis.stock_name,
                    'change_percent': stock_analysis.change_percent,
                    'volume_amount': stock_analysis.volume_amount,
                    'volume_rank': stock_analysis.volume_rank,
                    'volume_formatted': volume_formatted
                },
                style=kol_style
            )
            
            # ç²å–æ–°èé€£çµ
            news_links = await self._get_serper_news_links(stock_analysis.stock_id, stock_analysis.stock_name)
            
            # ç”Ÿæˆå…§å®¹
            content_generator = ContentGenerator()
            
            # ç²å–KOLè¨­å®š
            kol_settings = self.config_manager.get_kol_personalization_settings().get(kol_id, {})
            
            # å»ºç«‹ ContentRequest
            content_request = ContentRequest(
                topic_title=prompt_data.get("selected_title", f"{stock_analysis.stock_name}ç›¤ä¸­æ€¥æ¼²åˆ†æ"),
                topic_keywords=f"{stock_analysis.stock_name} ç›¤ä¸­æ€¥æ¼² {stock_analysis.change_percent}%",
                kol_persona=kol_settings.get('persona', 'æŠ€è¡“æ´¾'),
                kol_nickname=kol_settings.get('nickname', 'æœªçŸ¥KOL'),
                content_type="analysis",
                target_audience="æŠ•è³‡äºº",
                market_data={
                    "stock_id": stock_analysis.stock_id,
                    "stock_name": stock_analysis.stock_name,
                    "change_percent": stock_analysis.change_percent,
                    "volume_amount": stock_analysis.volume_amount,
                    "volume_rank": stock_analysis.volume_rank,
                    "volume_formatted": volume_formatted,
                    "news_links": news_links,
                    "prompt_data": prompt_data
                }
            )
            
            # ä½¿ç”¨é¸ä¸­çš„æ¨™é¡Œ
            selected_title = prompt_data.get("selected_title", f"{stock_analysis.stock_name}ç›¤ä¸­æ€¥æ¼²åˆ†æ")
            content = content_generator.generate_content(content_request, selected_title)
            
            # ä¸é‡è¤‡æ·»åŠ æ¨™é¡Œï¼Œå› ç‚ºLLMå·²ç¶“åœ¨å…§å®¹ä¸­åŒ…å«äº†æ¨™é¡Œ
            return content
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç›¤ä¸­æ€¥æ¼²è‚¡è²¼æ–‡å…§å®¹å¤±æ•—: {e}")
            # è¿”å›åŸºæœ¬å…§å®¹ä½œç‚ºå‚™ç”¨
            return f"ğŸ“ˆ {stock_analysis.stock_name}({stock_analysis.stock_id}) ç›¤ä¸­æ€¥æ¼² {stock_analysis.change_percent}%ï¼"

    async def _record_surge_stock_post_to_sheets(self, stock_analysis: 'StockAnalysis', kol_id: str, content: str) -> str:
        """è¨˜éŒ„ç›¤ä¸­æ€¥æ¼²è‚¡è²¼æ–‡åˆ°Google Sheets"""
        try:
            import uuid
            from datetime import datetime
            
            # ç”Ÿæˆå”¯ä¸€çš„è²¼æ–‡ID
            post_id = f"surge_{stock_analysis.stock_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"
            
            # æ ¼å¼åŒ–æˆäº¤é‡
            volume_formatted = self._format_volume_amount(stock_analysis.volume_amount)
            
            # æº–å‚™è²¼æ–‡æ•¸æ“š - å°æ‡‰Google Sheetsçš„æ¬„ä½çµæ§‹
            post_data = [
                post_id,  # test_post_id
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),  # test_time
                "å·²ç”Ÿæˆ",  # test_status
                "ç›¤ä¸­æ€¥æ¼²è‚¡",  # trigger_type
                "ready_to_post",  # status
                "high",  # priority_level
                f"surge_batch_{datetime.now().strftime('%Y%m%d')}",  # batch_id
                kol_id,  # kol_serial
                f"KOL_{kol_id}",  # kol_nickname
                kol_id,  # kol_id
                "æŠ€è¡“æ´¾",  # persona
                "å¹½é»˜é¢¨è¶£",  # writing_style
                "confident",  # tone
                f"{stock_analysis.stock_name},ç›¤ä¸­æ€¥æ¼²",  # key_phrases
                "",  # avoid_topics
                "finlab,serper",  # preferred_data_sources
                "random",  # kol_assignment_method
                1.0,  # kol_weight
                "1.0",  # kol_version
                0.0,  # kol_learning_score
                stock_analysis.stock_id,  # stock_id
                stock_analysis.stock_name,  # stock_name
                "stock_analysis",  # topic_category
                "surge_analysis",  # analysis_type
                "ç›¤ä¸­æ€¥æ¼²åˆ†æ",  # analysis_type_detail
                "high",  # topic_priority
                0.9,  # topic_heat_score
                f"surge_{stock_analysis.stock_id}",  # topic_id
                f"{stock_analysis.stock_name}ç›¤ä¸­æ€¥æ¼²åˆ†æ",  # topic_title
                f"{stock_analysis.stock_name} ç›¤ä¸­æ€¥æ¼² {stock_analysis.change_percent}%",  # topic_keywords
                True,  # is_stock_trigger
                "ç›¤ä¸­æ€¥æ¼²è‚¡",  # stock_trigger_type
                f"{stock_analysis.stock_name}ç›¤ä¸­æ€¥æ¼²åˆ†æ",  # title
                content,  # content
                len(content),  # content_length
                "å¹½é»˜é¢¨è¶£",  # content_style
                300,  # target_length
                1.0,  # weight
                12345,  # random_seed
                0.8,  # content_quality_score
                "analysis",  # content_type
                "medium",  # article_length_type
                "0.5,0.3,0.2",  # content_length_vector
                "0.7,0.6,0.8",  # tone_vector
                0.8,  # temperature_setting
                "gpt-4o",  # openai_model
                0,  # openai_tokens_used
                "surge_stock_template",  # prompt_template
                0.7,  # sentiment_score
                0.2,  # ai_detection_risk_score
                0.8,  # personalization_level
                0.7,  # creativity_score
                0.8,  # coherence_score
                "finlab,serper",  # data_sources_used
                True,  # serper_api_called
                "",  # serper_api_results
                0,  # serper_api_summary_count
                False,  # finlab_api_called
                "",  # finlab_api_results
                False,  # cmoney_api_called
                "",  # cmoney_api_results
                0.8,  # data_quality_score
                "fresh",  # data_freshness
                "auto",  # data_manager_dispatch
                False,  # trending_topics_summarized
                0.8,  # data_interpretability_score
                0,  # news_count
                "",  # news_summaries
                0.0,  # news_sentiment
                "",  # news_sources
                0.0,  # news_relevance_score
                0.0,  # news_freshness_score
                0.0,  # revenue_yoy_growth
                0.0,  # revenue_mom_growth
                0.0,  # eps_value
                0.0,  # eps_growth
                0.0,  # gross_margin
                0.0,  # net_margin
                0.0,  # financial_analysis_score
                stock_analysis.change_percent,  # price_change_percent
                1.0,  # volume_ratio
                0.0,  # rsi_value
                "neutral",  # macd_signal
                "up",  # ma_trend
                0.8,  # technical_analysis_score
                0.7,  # technical_confidence
                "",  # publish_time
                "",  # publish_platform
                "ready_to_post",  # publish_status
                0,  # interaction_count
                0.0,  # engagement_rate
                "",  # platform_post_id
                "",  # platform_post_url
                "",  # articleid
                "",  # learning_insights
                "",  # strategy_adjustments
                "",  # performance_improvement
                "",  # risk_alerts
                "",  # next_optimization_targets
                0,  # learning_cycle_count
                0.0,  # adaptive_score
                0,  # quality_check_rounds
                "",  # quality_issues_record
                0,  # regeneration_count
                "",  # quality_improvement_record
                "",  # body_parameter
                "",  # commodity_tags
                post_id,  # post_id
                ""  # agent_decision_record
            ]
            
            # è¨˜éŒ„åˆ°Google Sheets
            self.sheets_client.append_sheet("è²¼æ–‡ç´€éŒ„è¡¨", [post_data])
            
            logger.info(f"âœ… ç›¤ä¸­æ€¥æ¼²è‚¡è²¼æ–‡å·²è¨˜éŒ„åˆ°Google Sheets: {post_id}")
            return post_id
            
        except Exception as e:
            logger.error(f"âŒ è¨˜éŒ„ç›¤ä¸­æ€¥æ¼²è‚¡è²¼æ–‡åˆ°Google Sheetså¤±æ•—: {e}")
            raise

    def _format_volume_amount(self, volume_amount: float) -> str:
        """æ ¼å¼åŒ–æˆäº¤é‡ç‚ºæ›´æ˜“è®€çš„æ ¼å¼"""
        if volume_amount >= 10000:  # 1è¬å¼µä»¥ä¸Š
            return f"{volume_amount / 10000:.1f}è¬å¼µ"
        elif volume_amount >= 1000:  # 1åƒå¼µä»¥ä¸Š
            return f"{volume_amount / 1000:.1f}åƒå¼µ"
        else:
            return f"{volume_amount:.0f}å¼µ"

    def _get_real_stock_data(self, stock_id: str) -> dict:
        """ç²å–çœŸå¯¦è‚¡ç¥¨æ•¸æ“š - 2025/09/05 æ¼²å¹…æ’è¡Œæ•¸æ“šï¼ˆ17æª”æœ€æ–°ç‰ˆï¼‰"""
        # çœŸå¯¦ç›¤ä¸­æ€¥æ¼²è‚¡ç¥¨æ•¸æ“šæ˜ å°„ (2025/09/05) - ä½¿ç”¨æœ€æ–°çš„17æª”æ•¸æ“š
        real_stock_data = {
            "2344": {"name": "è¯é‚¦é›»", "change_percent": 10.00, "volume_shares": 221286, "volume_amount": 48.5779},      # æˆäº¤é‡: 221,286å¼µ, æˆäº¤é‡‘é¡: 48.5779å„„
            "2642": {"name": "å®…é…é€š", "change_percent": 10.00, "volume_shares": 371, "volume_amount": 0.1072},         # æˆäº¤é‡: 371å¼µ, æˆäº¤é‡‘é¡: 0.1072å„„
            "3211": {"name": "é †é”", "change_percent": 9.97, "volume_shares": 12869, "volume_amount": 50.4413},        # æˆäº¤é‡: 12,869å¼µ, æˆäº¤é‡‘é¡: 50.4413å„„
            "2408": {"name": "å—äºç§‘", "change_percent": 9.96, "volume_shares": 159865, "volume_amount": 82.8822},      # æˆäº¤é‡: 159,865å¼µ, æˆäº¤é‡‘é¡: 82.8822å„„
            "6789": {"name": "é‡‡éˆº", "change_percent": 9.96, "volume_shares": 9700, "volume_amount": 28.2953},          # æˆäº¤é‡: 9,700å¼µ, æˆäº¤é‡‘é¡: 28.2953å„„
            "4989": {"name": "æ¦®ç§‘", "change_percent": 9.95, "volume_shares": 6431, "volume_amount": 1.9565},          # æˆäº¤é‡: 6,431å¼µ, æˆäº¤é‡‘é¡: 1.9565å„„
            "2323": {"name": "ä¸­ç’°", "change_percent": 9.93, "volume_shares": 9531, "volume_amount": 0.8923},          # æˆäº¤é‡: 9,531å¼µ, æˆäº¤é‡‘é¡: 0.8923å„„
            "8088": {"name": "å“å®‰", "change_percent": 9.93, "volume_shares": 8576, "volume_amount": 2.7728},          # æˆäº¤é‡: 8,576å¼µ, æˆäº¤é‡‘é¡: 2.7728å„„
            "3323": {"name": "åŠ ç™¾è£•", "change_percent": 9.92, "volume_shares": 22334, "volume_amount": 9.2602},      # æˆäº¤é‡: 22,334å¼µ, æˆäº¤é‡‘é¡: 9.2602å„„
            "5234": {"name": "é”èˆˆææ–™", "change_percent": 9.92, "volume_shares": 3159, "volume_amount": 13.0698},     # æˆäº¤é‡: 3,159å¼µ, æˆäº¤é‡‘é¡: 13.0698å„„
            "6895": {"name": "å®ç¢©ç³»çµ±", "change_percent": 9.92, "volume_shares": 1266, "volume_amount": 4.1333},     # æˆäº¤é‡: 1,266å¼µ, æˆäº¤é‡‘é¡: 4.1333å„„
            "5345": {"name": "é¦¥é´»", "change_percent": 9.91, "volume_shares": 113, "volume_amount": 0.0286},           # æˆäº¤é‡: 113å¼µ, æˆäº¤é‡‘é¡: 0.0286å„„
            "8034": {"name": "æ¦®ç¾¤", "change_percent": 9.91, "volume_shares": 2536, "volume_amount": 0.6185},         # æˆäº¤é‡: 2,536å¼µ, æˆäº¤é‡‘é¡: 0.6185å„„
            "3006": {"name": "æ™¶è±ªç§‘", "change_percent": 9.90, "volume_shares": 8770, "volume_amount": 5.3913},     # æˆäº¤é‡: 8,770å¼µ, æˆäº¤é‡‘é¡: 5.3913å„„
            "8358": {"name": "é‡‘å±…", "change_percent": 9.90, "volume_shares": 64182, "volume_amount": 142.3081},        # æˆäº¤é‡: 64,182å¼µ, æˆäº¤é‡‘é¡: 142.3081å„„
            "5309": {"name": "ç³»çµ±é›»", "change_percent": 9.89, "volume_shares": 44122, "volume_amount": 26.4867},      # æˆäº¤é‡: 44,122å¼µ, æˆäº¤é‡‘é¡: 26.4867å„„
            "8299": {"name": "ç¾¤è¯", "change_percent": 9.89, "volume_shares": 5393, "volume_amount": 27.5933}         # æˆäº¤é‡: 5,393å¼µ, æˆäº¤é‡‘é¡: 27.5933å„„
        }
        
        return real_stock_data.get(stock_id, {
            "name": f"è‚¡ç¥¨{stock_id}",
            "change_percent": 5.0,
            "volume_shares": 1000,
            "volume_rank": 1
        })
