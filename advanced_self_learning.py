#!/usr/bin/env python3
"""
æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶ - å®Œæ•´ç‰ˆ
åŸºæ–¼å¯¦éš› CMoney äº’å‹•æ•¸æ“šçš„å­¸ç¿’ç³»çµ±
"""

import os
import sys
import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import numpy as np
from collections import defaultdict
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class InteractionData:
    """äº’å‹•æ•¸æ“šçµæ§‹"""
    article_id: str
    kol_id: str
    kol_nickname: str
    likes: int
    comments: int
    shares: int
    emoji_total: int
    total_interactions: int
    engagement_rate: float
    post_timestamp: str
    content: str
    topic_id: str
    views: Optional[int] = None
    sentiment_score: Optional[float] = None

@dataclass
class LearningInsight:
    """å­¸ç¿’æ´å¯Ÿ"""
    insight_type: str
    priority: str  # 'low', 'medium', 'high', 'critical'
    description: str
    suggested_actions: List[str]
    confidence: float
    timestamp: datetime
    impact_score: float
    implementation_difficulty: str

@dataclass
class KOLStrategy:
    """KOL ç­–ç•¥"""
    kol_id: str
    content_type_weights: Dict[str, float]
    persona_adjustments: Dict[str, float]
    timing_preferences: Dict[str, Any]
    interaction_style: Dict[str, Any]
    last_updated: datetime

class RealTimeAnalyzer:
    """å¯¦æ™‚åˆ†æå™¨"""
    
    def __init__(self):
        self.interaction_history = []
        self.kol_performance = defaultdict(list)
        
    def analyze_interaction_performance(self, interaction_data: InteractionData) -> Dict[str, Any]:
        """åˆ†æäº’å‹•è¡¨ç¾"""
        # è¨ˆç®—ç¶œåˆäº’å‹•åˆ†æ•¸
        engagement_score = self._calculate_engagement_score(interaction_data)
        
        # åˆ†æè¡¨æƒ…æƒ…æ„Ÿ
        emoji_sentiment = self._analyze_emoji_sentiment(interaction_data)
        
        # åˆ†ææ™‚é–“æ¨¡å¼
        timing_analysis = self._analyze_timing_pattern(interaction_data)
        
        # åˆ†æå…§å®¹ç‰¹å¾µ
        content_analysis = self._analyze_content_features(interaction_data)
        
        # è¨ˆç®—ç—…æ¯’å‚³æ’­æ½›åŠ›
        viral_potential = self._calculate_viral_potential(interaction_data)
        
        # è¨ˆç®—å“ç‰Œå½±éŸ¿åŠ›
        brand_impact = self._calculate_brand_impact(interaction_data)
        
        return {
            'engagement_score': engagement_score,
            'emoji_sentiment': emoji_sentiment,
            'timing_analysis': timing_analysis,
            'content_analysis': content_analysis,
            'viral_potential': viral_potential,
            'brand_impact': brand_impact,
            'performance_trend': self._analyze_performance_trend(interaction_data),
            'overall_score': self._calculate_overall_score(engagement_score, viral_potential, brand_impact)
        }
    
    def _calculate_engagement_score(self, data: InteractionData) -> float:
        """è¨ˆç®—ç¶œåˆäº’å‹•åˆ†æ•¸"""
        # åŠ æ¬Šè¨ˆç®—
        likes_weight = 0.3
        comments_weight = 0.4
        shares_weight = 0.2
        emoji_weight = 0.1
        
        weighted_score = (
            data.likes * likes_weight +
            data.comments * comments_weight +
            data.shares * shares_weight +
            data.emoji_total * emoji_weight
        )
        
        # æ¨™æº–åŒ–åˆ° 0-100 åˆ†
        normalized_score = min(weighted_score * 5, 100)
        
        return normalized_score
    
    def _analyze_emoji_sentiment(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æè¡¨æƒ…æƒ…æ„Ÿ"""
        return {
            'positive_ratio': 0.8,  # å‡è¨­ 80% æ­£é¢è¡¨æƒ…
            'engagement_level': 'high' if data.emoji_total > 5 else 'medium',
            'sentiment_score': 0.7  # 0-1 çš„æƒ…æ„Ÿåˆ†æ•¸
        }
    
    def _analyze_timing_pattern(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        try:
            post_time = datetime.fromisoformat(data.post_timestamp.replace('Z', '+00:00'))
            hour = post_time.hour
            
            return {
                'post_hour': hour,
                'is_peak_hour': 9 <= hour <= 11 or 19 <= hour <= 21,
                'timezone_effect': 'positive' if 9 <= hour <= 21 else 'negative',
                'optimal_timing': self._find_optimal_timing(hour),
                'timing_score': self._calculate_timing_score(hour)
            }
        except:
            return {
                'post_hour': 0,
                'is_peak_hour': False,
                'timezone_effect': 'unknown',
                'optimal_timing': 'unknown',
                'timing_score': 0.5
            }
    
    def _analyze_content_features(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æå…§å®¹ç‰¹å¾µ"""
        content = data.content
        if not content:
            content = "è²¼æ–‡å…§å®¹..."
        
        return {
            'content_length': len(content),
            'readability_score': self._calculate_readability(content),
            'personalization_level': self._analyze_personalization(content),
            'ai_detection_risk': self._assess_ai_detection_risk(content),
            'engagement_potential': self._assess_engagement_potential(content),
            'content_quality_score': self._calculate_content_quality_score(content)
        }
    
    def _calculate_readability(self, content: str) -> float:
        """è¨ˆç®—å¯è®€æ€§åˆ†æ•¸"""
        sentences = content.split('ã€‚')
        avg_sentence_length = sum(len(s) for s in sentences) / max(len(sentences), 1)
        
        if avg_sentence_length < 20:
            return 90
        elif avg_sentence_length < 30:
            return 70
        else:
            return 50
    
    def _analyze_personalization(self, content: str) -> float:
        """åˆ†æå€‹äººåŒ–ç¨‹åº¦"""
        personal_indicators = [
            'æˆ‘è¦ºå¾—', 'æˆ‘èªç‚º', 'æˆ‘çš„çœ‹æ³•', 'å€‹äººè¦ºå¾—', 'æˆ‘', 'æˆ‘çš„',
            'å“ˆå“ˆ', 'å‘µå‘µ', 'é ', 'å¹¹', 'çœŸçš„å‡çš„', '...', '!!!'
        ]
        
        personal_count = sum(1 for indicator in personal_indicators if indicator in content)
        return min(personal_count / 5, 1.0)
    
    def _assess_ai_detection_risk(self, content: str) -> float:
        """è©•ä¼° AI åµæ¸¬é¢¨éšª"""
        risk_factors = 0
        
        # æª¢æŸ¥éæ–¼æ­£å¼çš„èªè¨€
        formal_indicators = ['å› æ­¤', 'ç„¶è€Œ', 'æ­¤å¤–', 'ç¸½è€Œè¨€ä¹‹', 'ç¶œä¸Šæ‰€è¿°']
        risk_factors += sum(1 for indicator in formal_indicators if indicator in content)
        
        # æª¢æŸ¥ç¼ºä¹å€‹äººåŒ–
        personal_score = self._analyze_personalization(content)
        risk_factors += (1 - personal_score) * 3
        
        # æª¢æŸ¥çµæ§‹éæ–¼è¦æ•´
        sentences = content.split('ã€‚')
        if len(sentences) > 2:
            sentence_lengths = [len(s) for s in sentences if s.strip()]
            if sentence_lengths:
                length_variance = np.var(sentence_lengths)
                if length_variance < 50:
                    risk_factors += 2
        
        return min(risk_factors / 10, 1.0)
    
    def _assess_engagement_potential(self, content: str) -> float:
        """è©•ä¼°äº’å‹•æ½›åŠ›"""
        engagement_indicators = [
            'ä½ è¦ºå¾—', 'å¤§å®¶æ€éº¼çœ‹', 'æ­¡è¿è¨è«–', 'åˆ†äº«ä½ çš„çœ‹æ³•',
            '?', 'ï¼Ÿ', 'ï¼', '!!!', 'å“ˆå“ˆ', 'å‘µå‘µ'
        ]
        
        engagement_count = sum(1 for indicator in engagement_indicators if indicator in content)
        return min(engagement_count / 3, 1.0)
    
    def _calculate_content_quality_score(self, content: str) -> float:
        """è¨ˆç®—å…§å®¹å“è³ªåˆ†æ•¸"""
        readability = self._calculate_readability(content)
        personalization = self._analyze_personalization(content) * 100
        engagement_potential = self._assess_engagement_potential(content) * 100
        
        return (readability + personalization + engagement_potential) / 3
    
    def _find_optimal_timing(self, hour: int) -> str:
        """æ‰¾åˆ°æœ€ä½³ç™¼æ–‡æ™‚æ©Ÿ"""
        if 9 <= hour <= 11:
            return "morning_peak"
        elif 12 <= hour <= 14:
            return "lunch_time"
        elif 19 <= hour <= 21:
            return "evening_peak"
        else:
            return "off_peak"
    
    def _calculate_timing_score(self, hour: int) -> float:
        """è¨ˆç®—æ™‚æ©Ÿåˆ†æ•¸"""
        if 9 <= hour <= 11 or 19 <= hour <= 21:
            return 0.9
        elif 12 <= hour <= 14:
            return 0.7
        else:
            return 0.3
    
    def _calculate_viral_potential(self, data: InteractionData) -> float:
        """è¨ˆç®—ç—…æ¯’å‚³æ’­æ½›åŠ›"""
        share_ratio = data.shares / max(data.total_interactions, 1)
        comment_ratio = data.comments / max(data.total_interactions, 1)
        emoji_ratio = data.emoji_total / max(data.total_interactions, 1)
        
        viral_score = (
            share_ratio * 0.5 +
            comment_ratio * 0.3 +
            emoji_ratio * 0.2
        ) * 100
        
        return min(viral_score, 100)
    
    def _calculate_brand_impact(self, data: InteractionData) -> float:
        """è¨ˆç®—å“ç‰Œå½±éŸ¿åŠ›"""
        engagement_quality = data.total_interactions / max(data.views or 1000, 1)
        sentiment_impact = data.sentiment_score or 0.5
        
        brand_score = (engagement_quality * 0.6 + sentiment_impact * 0.4) * 100
        return min(brand_score, 100)
    
    def _calculate_overall_score(self, engagement_score: float, viral_potential: float, brand_impact: float) -> float:
        """è¨ˆç®—æ•´é«”åˆ†æ•¸"""
        return (engagement_score * 0.5 + viral_potential * 0.3 + brand_impact * 0.2)
    
    def _analyze_performance_trend(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æè¡¨ç¾è¶¨å‹¢"""
        self.kol_performance[data.kol_id].append(data)
        
        if len(self.kol_performance[data.kol_id]) < 2:
            return {'trend': 'insufficient_data', 'change_rate': 0}
        
        recent_data = self.kol_performance[data.kol_id][-5:]
        if len(recent_data) >= 2:
            recent_avg = sum(d.total_interactions for d in recent_data) / len(recent_data)
            previous_avg = sum(d.total_interactions for d in self.kol_performance[data.kol_id][:-5]) / max(len(self.kol_performance[data.kol_id]) - 5, 1)
            
            change_rate = (recent_avg - previous_avg) / max(previous_avg, 1)
            
            if change_rate > 0.1:
                trend = 'improving'
            elif change_rate < -0.1:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
            change_rate = 0
        
        return {'trend': trend, 'change_rate': change_rate}

class PatternDetector:
    """æ¨¡å¼åµæ¸¬å™¨"""
    
    def __init__(self):
        self.success_patterns = defaultdict(list)
        self.failure_patterns = defaultdict(list)
    
    def detect_patterns(self, interaction_data: InteractionData, performance_analysis: Dict) -> Dict[str, Any]:
        """åµæ¸¬æ¨¡å¼"""
        patterns = {
            'success_patterns': self._detect_success_patterns(interaction_data, performance_analysis),
            'failure_patterns': self._detect_failure_patterns(interaction_data, performance_analysis),
            'audience_patterns': self._detect_audience_patterns(interaction_data, performance_analysis)
        }
        
        return patterns
    
    def _detect_success_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬æˆåŠŸæ¨¡å¼"""
        patterns = []
        
        # é«˜äº’å‹•æ¨¡å¼
        if analysis['overall_score'] > 70:
            patterns.append({
                'type': 'high_engagement',
                'description': 'é«˜äº’å‹•è¡¨ç¾',
                'confidence': 0.8,
                'features': {
                    'engagement_score': analysis['engagement_score'],
                    'emoji_sentiment': analysis['emoji_sentiment']['sentiment_score'],
                    'timing': analysis['timing_analysis']['optimal_timing']
                }
            })
        
        # ç—…æ¯’å‚³æ’­æ¨¡å¼
        if analysis['viral_potential'] > 60:
            patterns.append({
                'type': 'viral_content',
                'description': 'ç—…æ¯’å‚³æ’­æ½›åŠ›',
                'confidence': 0.8,
                'features': {
                    'viral_potential': analysis['viral_potential'],
                    'share_rate': data.shares / max(data.total_interactions, 1),
                    'comment_rate': data.comments / max(data.total_interactions, 1)
                }
            })
        
        # æœ€ä½³æ™‚æ©Ÿæ¨¡å¼
        if analysis['timing_analysis']['is_peak_hour']:
            patterns.append({
                'type': 'optimal_timing',
                'description': 'æœ€ä½³ç™¼æ–‡æ™‚æ©Ÿ',
                'confidence': 0.6,
                'features': {
                    'post_hour': analysis['timing_analysis']['post_hour'],
                    'timezone_effect': analysis['timing_analysis']['timezone_effect']
                }
            })
        
        return patterns
    
    def _detect_failure_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬å¤±æ•—æ¨¡å¼"""
        patterns = []
        
        # ä½äº’å‹•æ¨¡å¼
        if analysis['overall_score'] < 30:
            patterns.append({
                'type': 'low_engagement',
                'description': 'äº’å‹•ç‡åä½',
                'confidence': 0.8,
                'features': {
                    'engagement_score': analysis['engagement_score'],
                    'content_length': analysis['content_analysis']['content_length']
                }
            })
        
        # AI åµæ¸¬é¢¨éšªæ¨¡å¼
        if analysis['content_analysis']['ai_detection_risk'] > 0.7:
            patterns.append({
                'type': 'ai_detection_risk',
                'description': 'AI åµæ¸¬é¢¨éšªè¼ƒé«˜',
                'confidence': 0.9,
                'features': {
                    'ai_detection_risk': analysis['content_analysis']['ai_detection_risk'],
                    'personalization_level': analysis['content_analysis']['personalization_level']
                }
            })
        
        # éæœ€ä½³æ™‚æ©Ÿæ¨¡å¼
        if not analysis['timing_analysis']['is_peak_hour']:
            patterns.append({
                'type': 'suboptimal_timing',
                'description': 'ç™¼æ–‡æ™‚æ©Ÿä¸ä½³',
                'confidence': 0.5,
                'features': {
                    'post_hour': analysis['timing_analysis']['post_hour'],
                    'timezone_effect': analysis['timing_analysis']['timezone_effect']
                }
            })
        
        return patterns
    
    def _detect_audience_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬å—çœ¾æ¨¡å¼"""
        patterns = []
        
        # å—çœ¾æ´»èºåº¦æ¨¡å¼
        if data.comments > 0:
            patterns.append({
                'type': 'active_audience',
                'description': 'å—çœ¾äº’å‹•æ´»èº',
                'confidence': 0.7,
                'features': {
                    'comment_count': data.comments,
                    'engagement_rate': data.engagement_rate
                }
            })
        
        # è¡¨æƒ…å›æ‡‰æ¨¡å¼
        if data.emoji_total > 5:
            patterns.append({
                'type': 'emoji_response',
                'description': 'å—çœ¾è¡¨æƒ…å›æ‡‰ç©æ¥µ',
                'confidence': 0.6,
                'features': {
                    'emoji_total': data.emoji_total,
                    'sentiment_score': analysis['emoji_sentiment']['sentiment_score']
                }
            })
        
        return patterns

class RiskAssessor:
    """é¢¨éšªè©•ä¼°å™¨"""
    
    def __init__(self):
        self.risk_thresholds = {
            'ai_detection': 0.7,
            'low_engagement': 0.3,
            'negative_sentiment': 0.4,
            'suboptimal_timing': 0.5
        }
    
    def assess_risks(self, interaction_data: InteractionData, performance_analysis: Dict, patterns: Dict) -> Dict[str, Any]:
        """è©•ä¼°é¢¨éšª"""
        risks = {
            'ai_detection_risk': self._assess_ai_detection_risk(performance_analysis, patterns),
            'engagement_risk': self._assess_engagement_risk(performance_analysis, patterns),
            'timing_risk': self._assess_timing_risk(performance_analysis, patterns),
            'content_quality_risk': self._assess_content_quality_risk(performance_analysis, patterns),
            'overall_risk': 0.0
        }
        
        # è¨ˆç®—æ•´é«”é¢¨éšª
        risk_scores = [risks[k] for k in risks.keys() if k != 'overall_risk']
        risks['overall_risk'] = sum(risk_scores) / len(risk_scores)
        
        # ç”Ÿæˆé¢¨éšªé è­¦
        risks['risk_alerts'] = self._generate_risk_alerts(risks)
        
        return risks
    
    def _assess_ai_detection_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼° AI åµæ¸¬é¢¨éšª"""
        base_risk = analysis['content_analysis']['ai_detection_risk']
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´é¢¨éšª
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'ai_detection_risk':
                base_risk += 0.2
        
        return min(base_risk, 1.0)
    
    def _assess_engagement_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°äº’å‹•é¢¨éšª"""
        base_risk = 1.0 - (analysis['overall_score'] / 100)
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´é¢¨éšª
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'low_engagement':
                base_risk += 0.3
        
        return min(base_risk, 1.0)
    
    def _assess_timing_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°æ™‚æ©Ÿé¢¨éšª"""
        if analysis['timing_analysis']['is_peak_hour']:
            return 0.2
        else:
            return 0.6
    
    def _assess_content_quality_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°å…§å®¹å“è³ªé¢¨éšª"""
        risks = []
        
        # å¯è®€æ€§é¢¨éšª
        if analysis['content_analysis']['readability_score'] < 50:
            risks.append(0.5)
        
        # å€‹äººåŒ–é¢¨éšª
        if analysis['content_analysis']['personalization_level'] < 0.3:
            risks.append(0.4)
        
        # äº’å‹•æ½›åŠ›é¢¨éšª
        if analysis['content_analysis']['engagement_potential'] < 0.3:
            risks.append(0.3)
        
        return sum(risks) / max(len(risks), 1)
    
    def _generate_risk_alerts(self, risks: Dict) -> List[Dict]:
        """ç”Ÿæˆé¢¨éšªé è­¦"""
        alerts = []
        
        for risk_type, risk_score in risks.items():
            if risk_type in ['overall_risk', 'risk_alerts']:
                continue
                
            if isinstance(risk_score, (int, float)) and risk_score > 0.7:
                alerts.append({
                    'type': risk_type,
                    'level': 'critical',
                    'description': f'{risk_type} é¢¨éšªéé«˜',
                    'immediate_action_required': True
                })
            elif isinstance(risk_score, (int, float)) and risk_score > 0.5:
                alerts.append({
                    'type': risk_type,
                    'level': 'warning',
                    'description': f'{risk_type} é¢¨éšªéœ€è¦æ³¨æ„',
                    'immediate_action_required': False
                })
        
        return alerts

class LearningInsightGenerator:
    """å­¸ç¿’æ´å¯Ÿç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.insight_templates = {
            'content_optimization': {
                'low_engagement': 'äº’å‹•ç‡åä½ï¼Œå»ºè­°å¢åŠ å€‹äººåŒ–å…ƒç´ å’Œæƒ…æ„Ÿè¡¨é”',
                'ai_detection_risk': 'AI åµæ¸¬é¢¨éšªè¼ƒé«˜ï¼Œéœ€è¦å¢åŠ äººæ€§åŒ–è¡¨é”',
                'low_personalization': 'å€‹äººåŒ–ç¨‹åº¦ä¸è¶³ï¼Œå»ºè­°å¢åŠ å€‹äººè§€é»å’Œç¶“é©—'
            },
            'timing_optimization': {
                'suboptimal_timing': 'ç™¼æ–‡æ™‚æ©Ÿä¸ä½³ï¼Œå»ºè­°åœ¨é«˜å³°æ™‚æ®µç™¼æ–‡',
                'off_peak': 'éé«˜å³°æ™‚æ®µç™¼æ–‡ï¼Œå¯èƒ½å½±éŸ¿äº’å‹•æ•ˆæœ'
            },
            'content_quality': {
                'low_readability': 'å¯è®€æ€§è¼ƒä½ï¼Œå»ºè­°ç°¡åŒ–èªè¨€çµæ§‹',
                'low_engagement_potential': 'äº’å‹•æ½›åŠ›ä¸è¶³ï¼Œå»ºè­°å¢åŠ äº’å‹•å…ƒç´ '
            }
        }
    
    def generate_insights(self, interaction_data: InteractionData, performance_analysis: Dict, patterns: Dict, risks: Dict) -> List[LearningInsight]:
        """ç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"""
        insights = []
        
        # åŸºæ–¼é¢¨éšªç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_risk_based_insights(risks, performance_analysis))
        
        # åŸºæ–¼æ¨¡å¼ç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_pattern_based_insights(patterns, performance_analysis))
        
        # åŸºæ–¼è¡¨ç¾è¶¨å‹¢ç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_trend_based_insights(performance_analysis))
        
        return insights
    
    def _generate_risk_based_insights(self, risks: Dict, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼é¢¨éšªç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # AI åµæ¸¬é¢¨éšªæ´å¯Ÿ
        if risks['ai_detection_risk'] > 0.7:
            insights.append(LearningInsight(
                insight_type='ai_detection_risk',
                priority='critical',
                description='AI åµæ¸¬é¢¨éšªè¼ƒé«˜ï¼Œéœ€è¦ç«‹å³èª¿æ•´',
                suggested_actions=[
                    'å¢åŠ å£èªåŒ–è¡¨é”',
                    'æ·»åŠ ä¸å®Œæ•´å¥å­',
                    'ä½¿ç”¨æ›´å¤šè¡¨æƒ…ç¬¦è™Ÿ',
                    'åŠ å…¥å€‹äººåŒ–ç´°ç¯€',
                    'é¿å…éæ–¼æ­£å¼çš„èªè¨€çµæ§‹'
                ],
                confidence=0.9,
                timestamp=datetime.now(),
                impact_score=0.8,
                implementation_difficulty='medium'
            ))
        
        # äº’å‹•é¢¨éšªæ´å¯Ÿ
        if risks['engagement_risk'] > 0.6:
            insights.append(LearningInsight(
                insight_type='content_optimization',
                priority='high',
                description='äº’å‹•ç‡åä½ï¼Œå»ºè­°å¢åŠ å€‹äººåŒ–å…ƒç´ å’Œæƒ…æ„Ÿè¡¨é”',
                suggested_actions=[
                    'å¢åŠ å€‹äººè§€é»å’Œç¶“é©—åˆ†äº«',
                    'ä½¿ç”¨æ›´å¤šæƒ…æ„Ÿè©å½™',
                    'æ·»åŠ äº’å‹•æ€§å•é¡Œ',
                    'ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢åŠ è¦ªå’ŒåŠ›',
                    'åˆ†äº«å€‹äººæ•…äº‹æˆ–ç¶“æ­·'
                ],
                confidence=0.8,
                timestamp=datetime.now(),
                impact_score=0.7,
                implementation_difficulty='medium'
            ))
        
        # æ™‚æ©Ÿé¢¨éšªæ´å¯Ÿ
        if risks['timing_risk'] > 0.5:
            insights.append(LearningInsight(
                insight_type='timing_optimization',
                priority='medium',
                description='ç™¼æ–‡æ™‚æ©Ÿä¸ä½³ï¼Œå»ºè­°åœ¨é«˜å³°æ™‚æ®µç™¼æ–‡',
                suggested_actions=[
                    'é¸æ“‡ 9-11 é»æˆ– 19-21 é»ç™¼æ–‡',
                    'é¿é–‹æ·±å¤œå’Œå‡Œæ™¨æ™‚æ®µ',
                    'è€ƒæ…®å—çœ¾çš„æ´»èºæ™‚é–“',
                    'æ¸¬è©¦ä¸åŒæ™‚æ®µçš„äº’å‹•æ•ˆæœ'
                ],
                confidence=0.7,
                timestamp=datetime.now(),
                impact_score=0.6,
                implementation_difficulty='easy'
            ))
        
        return insights
    
    def _generate_pattern_based_insights(self, patterns: Dict, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼æ¨¡å¼ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # æˆåŠŸæ¨¡å¼æ´å¯Ÿ
        for pattern in patterns['success_patterns']:
            if pattern['type'] == 'high_engagement':
                insights.append(LearningInsight(
                    insight_type='success_pattern',
                    priority='low',
                    description='ç™¼ç¾é«˜äº’å‹•æ¨¡å¼ï¼Œå»ºè­°ä¿æŒä¸¦å„ªåŒ–',
                    suggested_actions=[
                        'ä¿æŒç•¶å‰çš„å…§å®¹é¢¨æ ¼',
                        'é€²ä¸€æ­¥å„ªåŒ–å€‹äººåŒ–è¡¨é”',
                        'åœ¨ç›¸ä¼¼æ™‚æ®µç™¼æ–‡',
                        'æ“´å¤§æˆåŠŸæ¨¡å¼çš„æ‡‰ç”¨'
                    ],
                    confidence=pattern['confidence'],
                    timestamp=datetime.now(),
                    impact_score=0.6,
                    implementation_difficulty='easy'
                ))
        
        # å¤±æ•—æ¨¡å¼æ´å¯Ÿ
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'low_engagement':
                insights.append(LearningInsight(
                    insight_type='content_optimization',
                    priority='high',
                    description='ç™¼ç¾ä½äº’å‹•æ¨¡å¼ï¼Œéœ€è¦èª¿æ•´ç­–ç•¥',
                    suggested_actions=[
                        'å¢åŠ å…§å®¹çš„è¶£å‘³æ€§',
                        'ä½¿ç”¨æ›´å¤šè¦–è¦ºå…ƒç´ ',
                        'èª¿æ•´å…§å®¹é•·åº¦',
                        'å¢åŠ äº’å‹•æ€§å•é¡Œ'
                    ],
                    confidence=pattern['confidence'],
                    timestamp=datetime.now(),
                    impact_score=0.7,
                    implementation_difficulty='medium'
                ))
        
        return insights
    
    def _generate_trend_based_insights(self, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼è¶¨å‹¢ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        trend = analysis.get('performance_trend', {})
        if trend.get('trend') == 'declining':
            insights.append(LearningInsight(
                insight_type='performance_trend',
                priority='high',
                description='è¡¨ç¾å‘ˆä¸‹é™è¶¨å‹¢ï¼Œéœ€è¦ç­–ç•¥èª¿æ•´',
                suggested_actions=[
                    'åˆ†æä¸‹é™åŸå› ',
                    'èª¿æ•´å…§å®¹ç­–ç•¥',
                    'å„ªåŒ–ç™¼æ–‡æ™‚æ©Ÿ',
                    'å¢åŠ èˆ‡å—çœ¾çš„äº’å‹•'
                ],
                confidence=0.7,
                timestamp=datetime.now(),
                impact_score=0.7,
                implementation_difficulty='medium'
            ))
        
        return insights

class SelfLearningSystem:
    """æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’ç³»çµ±"""
    
    def __init__(self):
        self.real_time_analyzer = RealTimeAnalyzer()
        self.pattern_detector = PatternDetector()
        self.risk_assessor = RiskAssessor()
        self.insight_generator = LearningInsightGenerator()
        self.kol_strategies = {}
        
        logger.info("æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    async def process_interaction_data(self, interaction_data: InteractionData) -> Dict[str, Any]:
        """è™•ç†äº’å‹•æ•¸æ“šä¸¦ç”Ÿæˆå­¸ç¿’çµæœ"""
        try:
            logger.info(f"é–‹å§‹è™•ç† {interaction_data.kol_nickname} çš„äº’å‹•æ•¸æ“š")
            
            # 1. å¯¦æ™‚åˆ†æ
            performance_analysis = self.real_time_analyzer.analyze_interaction_performance(interaction_data)
            
            # 2. æ¨¡å¼åµæ¸¬
            patterns = self.pattern_detector.detect_patterns(interaction_data, performance_analysis)
            
            # 3. é¢¨éšªè©•ä¼°
            risks = self.risk_assessor.assess_risks(interaction_data, performance_analysis, patterns)
            
            # 4. ç”Ÿæˆæ´å¯Ÿ
            insights = self.insight_generator.generate_insights(interaction_data, performance_analysis, patterns, risks)
            
            # 5. æ›´æ–°ç­–ç•¥
            strategy_updates = self._update_kol_strategy(interaction_data.kol_id, insights)
            
            # 6. ç”Ÿæˆå ±å‘Š
            learning_report = {
                'kol_id': interaction_data.kol_id,
                'kol_nickname': interaction_data.kol_nickname,
                'article_id': interaction_data.article_id,
                'performance_analysis': performance_analysis,
                'patterns': patterns,
                'risks': risks,
                'insights': [asdict(insight) for insight in insights],
                'strategy_updates': strategy_updates,
                'learning_summary': self._generate_learning_summary(insights, risks),
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"å®Œæˆ {interaction_data.kol_nickname} çš„å­¸ç¿’åˆ†æ")
            return learning_report
            
        except Exception as e:
            logger.error(f"è™•ç†äº’å‹•æ•¸æ“šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _update_kol_strategy(self, kol_id: str, insights: List[LearningInsight]) -> Dict[str, Any]:
        """æ›´æ–° KOL ç­–ç•¥"""
        if kol_id not in self.kol_strategies:
            self.kol_strategies[kol_id] = KOLStrategy(
                kol_id=kol_id,
                content_type_weights={'investment': 0.5, 'personal': 0.3, 'news': 0.2},
                persona_adjustments={'personalization': 0.5, 'emotion': 0.5, 'authenticity': 0.5},
                timing_preferences={'optimal_hours': [9, 10, 11, 19, 20, 21]},
                interaction_style={'casual': 0.5, 'professional': 0.5},
                last_updated=datetime.now()
            )
        
        strategy = self.kol_strategies[kol_id]
        updates = {}
        
        # æ ¹æ“šæ´å¯Ÿèª¿æ•´ç­–ç•¥
        for insight in insights:
            if insight.insight_type == 'content_optimization':
                if insight.priority in ['high', 'critical']:
                    strategy.persona_adjustments['personalization'] += 0.1
                    strategy.persona_adjustments['emotion'] += 0.1
                    updates['personalization_increased'] = True
            
            elif insight.insight_type == 'timing_optimization':
                if insight.priority in ['medium', 'high']:
                    updates['timing_adjusted'] = True
            
            elif insight.insight_type == 'ai_detection_risk':
                if insight.priority == 'critical':
                    strategy.persona_adjustments['authenticity'] += 0.2
                    strategy.interaction_style['casual'] += 0.2
                    updates['authenticity_increased'] = True
        
        strategy.last_updated = datetime.now()
        return updates
    
    def _generate_learning_summary(self, insights: List[LearningInsight], risks: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå­¸ç¿’æ‘˜è¦"""
        # éæ¿¾æ‰éæ•¸å€¼é¡å‹çš„é¢¨éšª
        numeric_risks = {k: v for k, v in risks.items() 
                        if isinstance(v, (int, float)) and k not in ['overall_risk', 'risk_alerts']}
        
        return {
            'total_insights': len(insights),
            'critical_insights': len([i for i in insights if i.priority == 'critical']),
            'high_priority_insights': len([i for i in insights if i.priority == 'high']),
            'highest_risk': max(numeric_risks.keys(), key=lambda k: numeric_risks[k]) if numeric_risks else 'none',
            'overall_risk_level': risks.get('overall_risk', 0),
            'recommended_actions': len([i for i in insights if i.impact_score > 0.6]),
            'learning_progress': 'improving' if len(insights) > 0 else 'stable'
        }

async def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦è‡ªæˆ‘å­¸ç¿’ç³»çµ±"""
    print("ğŸ§  æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶å®Œæ•´ç‰ˆæ¸¬è©¦")
    print("=" * 60)
    
    # åˆå§‹åŒ–å­¸ç¿’ç³»çµ±
    learning_system = SelfLearningSystem()
    
    # æ¨¡æ“¬äº’å‹•æ•¸æ“š
    test_data = [
        InteractionData(
            article_id='173477844',
            kol_id='9505549',
            kol_nickname='é¾œç‹—ä¸€æ—¥æ•£æˆ¶',
            likes=8,
            comments=1,
            shares=1,
            emoji_total=8,
            total_interactions=18,
            engagement_rate=0.018,
            post_timestamp='2025-09-02T17:41:09.295405',
            content='è²¼æ–‡å…§å®¹...',
            topic_id='2025-09-02 17:41:13',
            views=1000,
            sentiment_score=0.7
        ),
        InteractionData(
            article_id='173477845',
            kol_id='9505550',
            kol_nickname='æ¿æ©‹å¤§who',
            likes=9,
            comments=0,
            shares=0,
            emoji_total=9,
            total_interactions=18,
            engagement_rate=0.018,
            post_timestamp='2025-09-02T17:41:09.704825',
            content='è²¼æ–‡å…§å®¹...',
            topic_id='2025-09-02 17:41:13',
            views=1000,
            sentiment_score=0.6
        )
    ]
    
    # è™•ç†æ¯å€‹äº’å‹•æ•¸æ“š
    for i, data in enumerate(test_data, 1):
        print(f"\nğŸ“Š è™•ç†ç¬¬ {i} å€‹äº’å‹•æ•¸æ“š:")
        print(f"   KOL: {data.kol_nickname}")
        print(f"   Article ID: {data.article_id}")
        print(f"   äº’å‹•æ•¸: {data.total_interactions}")
        
        # åŸ·è¡Œå­¸ç¿’åˆ†æ
        learning_report = await learning_system.process_interaction_data(data)
        
        if 'error' not in learning_report:
            # é¡¯ç¤ºåˆ†æçµæœ
            print(f"\nğŸ“ˆ åˆ†æçµæœ:")
            print(f"   æ•´é«”åˆ†æ•¸: {learning_report['performance_analysis']['overall_score']:.1f}/100")
            print(f"   äº’å‹•åˆ†æ•¸: {learning_report['performance_analysis']['engagement_score']:.1f}/100")
            print(f"   ç—…æ¯’å‚³æ’­æ½›åŠ›: {learning_report['performance_analysis']['viral_potential']:.1f}")
            print(f"   å“ç‰Œå½±éŸ¿åŠ›: {learning_report['performance_analysis']['brand_impact']:.1f}")
            
            # é¡¯ç¤ºé¢¨éšªè©•ä¼°
            risks = learning_report['risks']
            print(f"\nâš ï¸ é¢¨éšªè©•ä¼°:")
            print(f"   æ•´é«”é¢¨éšª: {risks['overall_risk']:.2f}")
            print(f"   AI åµæ¸¬é¢¨éšª: {risks['ai_detection_risk']:.2f}")
            print(f"   äº’å‹•é¢¨éšª: {risks['engagement_risk']:.2f}")
            
            # é¡¯ç¤ºé¢¨éšªé è­¦
            if risks['risk_alerts']:
                print(f"   é¢¨éšªé è­¦ ({len(risks['risk_alerts'])} å€‹):")
                for alert in risks['risk_alerts']:
                    print(f"     - [{alert['level'].upper()}] {alert['description']}")
            
            # é¡¯ç¤ºæ´å¯Ÿ
            insights = learning_report['insights']
            if insights:
                print(f"\nğŸ’¡ å­¸ç¿’æ´å¯Ÿ ({len(insights)} å€‹):")
                for j, insight in enumerate(insights, 1):
                    print(f"   {j}. [{insight['priority'].upper()}] {insight['description']}")
                    print(f"      å½±éŸ¿åˆ†æ•¸: {insight['impact_score']:.1f}, å¯¦æ–½é›£åº¦: {insight['implementation_difficulty']}")
                    for action in insight['suggested_actions'][:2]:  # åªé¡¯ç¤ºå‰2å€‹å»ºè­°
                        print(f"      - {action}")
            else:
                print(f"\nâœ… æ²’æœ‰ç™¼ç¾éœ€è¦èª¿æ•´çš„å•é¡Œ")
            
            # é¡¯ç¤ºå­¸ç¿’æ‘˜è¦
            summary = learning_report['learning_summary']
            print(f"\nğŸ“‹ å­¸ç¿’æ‘˜è¦:")
            print(f"   ç¸½æ´å¯Ÿæ•¸: {summary['total_insights']}")
            print(f"   é—œéµæ´å¯Ÿ: {summary['critical_insights']}")
            print(f"   é«˜å„ªå…ˆç´šæ´å¯Ÿ: {summary['high_priority_insights']}")
            print(f"   å»ºè­°è¡Œå‹•: {summary['recommended_actions']}")
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {learning_report['error']}")
    
    print(f"\n" + "=" * 60)
    print("âœ… æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶å®Œæ•´ç‰ˆæ¸¬è©¦å®Œæˆ")
    print("ğŸ’¡ ç³»çµ±èƒ½å¤ :")
    print("   - å¯¦æ™‚åˆ†æäº’å‹•è¡¨ç¾")
    print("   - åµæ¸¬æˆåŠŸå’Œå¤±æ•—æ¨¡å¼")
    print("   - è©•ä¼°å„ç¨®é¢¨éšª")
    print("   - ç”Ÿæˆå…·é«”æ”¹é€²å»ºè­°")
    print("   - è‡ªå‹•èª¿æ•´ KOL ç­–ç•¥")
    print("   - è¨ˆç®—ç—…æ¯’å‚³æ’­æ½›åŠ›")
    print("   - è©•ä¼°å“ç‰Œå½±éŸ¿åŠ›")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
