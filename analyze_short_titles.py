#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡æ–°åˆ†æ UGC æ¨™é¡Œ - å°ˆæ³¨æ–¼ä¸­çŸ­æ¨™é¡Œï¼ˆâ‰¤20å­—ï¼‰
éæ¿¾æ‰é•·æ¨™é¡Œï¼ˆé€šå¸¸æ˜¯æ–°èé€£çµï¼‰
"""

import re
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
import json
import time
from datetime import datetime
import gc
import os

class UGCShortTitleAnalyzer:
    """UGC çŸ­æ¨™é¡Œåˆ†æå™¨"""
    
    def __init__(self):
        self.data_files = [
            'anya-forumteam-1756973422036-23159383983751583.tsv',
            'anya-forumteam-1756973297558-23159259505732695.tsv'
        ]
        self.results = {}
        self.combined_stats = {}
        
        # å®šç¾©çŸ­æ¨™é¡Œçš„é•·åº¦ç¯„åœ
        self.short_title_ranges = {
            'very_short': 15,    # â‰¤15å­—
            'short': 20,         # â‰¤20å­—
            'medium': 30         # â‰¤30å­—
        }
    
    def smart_read_large_file(self, filepath, sample_size=100000):
        """æ™ºèƒ½è®€å–å¤§æ–‡ä»¶ï¼Œä½¿ç”¨æŠ½æ¨£æ–¹æ³•"""
        print(f"ğŸ“– æ­£åœ¨æ™ºèƒ½è®€å–å¤§æ–‡ä»¶: {filepath}")
        
        file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
        print(f"   æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        if file_size > 100:  # å¤§æ–¼ 100MB
            print("   ğŸ”„ ä½¿ç”¨æ™ºèƒ½æŠ½æ¨£ç­–ç•¥...")
            
            total_lines = sum(1 for _ in open(filepath, 'r', encoding='utf-8'))
            sample_lines = min(sample_size, total_lines // 10)
            
            titles = []
            
            # è®€å–é–‹é ­éƒ¨åˆ†
            with open(filepath, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= sample_lines:
                        break
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            # è®€å–çµå°¾éƒ¨åˆ†
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                end_start = max(0, len(lines) - sample_lines)
                for line in lines[end_start:]:
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            # éš¨æ©ŸæŠ½æ¨£ä¸­é–“éƒ¨åˆ†
            if len(lines) > sample_lines * 2:
                import random
                middle_lines = lines[sample_lines:-sample_lines]
                random_sample = random.sample(middle_lines, min(sample_lines, len(middle_lines)))
                for line in random_sample:
                    line = line.strip()
                    if line and line != 'title' and line != '""':
                        titles.append(line)
            
            print(f"   âœ… æŠ½æ¨£å®Œæˆï¼Œå…±è®€å– {len(titles)} å€‹æ¨™é¡Œ")
            
        else:
            print("   ğŸ“– ç›´æ¥è®€å–å®Œæ•´æ–‡ä»¶...")
            with open(filepath, 'r', encoding='utf-8') as f:
                titles = [line.strip() for line in f if line.strip() and line.strip() != 'title' and line.strip() != '""']
        
        return titles
    
    def filter_short_titles(self, titles):
        """éæ¿¾å‡ºçŸ­æ¨™é¡Œ"""
        short_titles = {
            'very_short': [],  # â‰¤15å­—
            'short': [],       # â‰¤20å­—
            'medium': []       # â‰¤30å­—
        }
        
        for title in titles:
            title_length = len(title)
            
            if title_length <= self.short_title_ranges['very_short']:
                short_titles['very_short'].append(title)
            if title_length <= self.short_title_ranges['short']:
                short_titles['short'].append(title)
            if title_length <= self.short_title_ranges['medium']:
                short_titles['medium'].append(title)
        
        return short_titles
    
    def analyze_length_distribution(self, titles):
        """åˆ†ææ¨™é¡Œé•·åº¦åˆ†å¸ƒ"""
        lengths = [len(title) for title in titles]
        
        return {
            'total_count': len(titles),
            'avg_length': np.mean(lengths),
            'median_length': np.median(lengths),
            'min_length': min(lengths),
            'max_length': max(lengths),
            'std_length': np.std(lengths),
            'distribution': {
                'very_short': len([l for l in lengths if l <= 15]),
                'short': len([l for l in lengths if 15 < l <= 20]),
                'medium': len([l for l in lengths if 20 < l <= 30]),
                'long': len([l for l in lengths if l > 30])
            }
        }
    
    def analyze_emojis(self, titles):
        """åˆ†æè¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨"""
        emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ï¸â¤ï¸ğŸ’ğŸ’°ğŸ’µğŸ’¸ğŸ‰ğŸŠğŸ¯ğŸªğŸ­ğŸ¨ğŸ¬ğŸ¤ğŸ§ğŸµğŸ¶ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ†ğŸˆğŸ‰ğŸŠğŸ‹ï¸ğŸŒï¸ğŸï¸ğŸï¸ğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ï¸ğŸ•ï¸ğŸ–ï¸ğŸ—ï¸ğŸ˜ï¸ğŸ™ï¸ğŸšï¸ğŸ›ï¸ğŸœï¸ğŸï¸ğŸï¸ğŸŸï¸ğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ï¸ğŸ´ï¸ğŸµï¸ğŸ¶ğŸ·ï¸ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ…ğŸ†ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ğŸ•ğŸ–ğŸ—ğŸ˜ğŸ™ğŸšğŸ›ğŸœğŸğŸğŸŸğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ğŸ´ğŸµğŸ¶ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ï¸ğŸ‘€ğŸ‘ï¸ğŸ‘‚ğŸ‘ƒğŸ‘„ğŸ‘…ğŸ‘†ğŸ‘‡ğŸ‘ˆğŸ‘‰ğŸ‘ŠğŸ‘‹ğŸ‘ŒğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘‘ğŸ‘’ğŸ‘“ğŸ‘”ğŸ‘•ğŸ‘–ğŸ‘—ğŸ‘˜ğŸ‘™ğŸ‘šğŸ‘›ğŸ‘œğŸ‘ğŸ‘ğŸ‘ŸğŸ‘ ğŸ‘¡ğŸ‘¢ğŸ‘£ğŸ‘¤ğŸ‘¥ğŸ‘¦ğŸ‘§ğŸ‘¨ğŸ‘©ğŸ‘ªğŸ‘«ğŸ‘¬ğŸ‘­ğŸ‘®ğŸ‘¯ğŸ‘°ğŸ‘±ğŸ‘²ğŸ‘³ğŸ‘´ğŸ‘µğŸ‘¶ğŸ‘·ğŸ‘¸ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ‘¼ğŸ‘½ğŸ‘¾ğŸ‘¿ğŸ’€ğŸ’ğŸ’‚ğŸ’ƒğŸ’„ğŸ’…ğŸ’†ğŸ’‡ğŸ’ˆğŸ’‰ğŸ’ŠğŸ’‹ğŸ’ŒğŸ’ğŸ’ğŸ’ğŸ’ğŸ’‘ğŸ’’ğŸ’“ğŸ’”ğŸ’•ğŸ’–ğŸ’—ğŸ’˜ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ’ğŸ’ğŸ’ŸğŸ’ ğŸ’¡ğŸ’¢ğŸ’£ğŸ’¤ğŸ’¥ğŸ’¦ğŸ’§ğŸ’¨ğŸ’©ğŸ’ªğŸ’«ğŸ’¬ğŸ’­ğŸ’®ğŸ’¯ğŸ’°ğŸ’±ğŸ’²ğŸ’³ğŸ’´ğŸ’µğŸ’¶ğŸ’·ğŸ’¸ğŸ’¹ğŸ’ºğŸ’»ğŸ’¼ğŸ’½ğŸ’¾ğŸ’¿ğŸ“€ğŸ“ğŸ“‚ğŸ“ƒğŸ“„ğŸ“…ğŸ“†ğŸ“‡ğŸ“ˆğŸ“‰ğŸ“ŠğŸ“‹ğŸ“ŒğŸ“ğŸ“ğŸ“ğŸ“ğŸ“‘ğŸ“’ğŸ““ğŸ“”ğŸ“•ğŸ“–ğŸ“—ğŸ“˜ğŸ“™ğŸ“šğŸ“›ğŸ“œğŸ“ğŸ“ğŸ“ŸğŸ“ ğŸ“¡ğŸ“¢ğŸ“£ğŸ“¤ğŸ“¥ğŸ“¦ğŸ“§ğŸ“¨ğŸ“©ğŸ“ªğŸ“«ğŸ“¬ğŸ“­ğŸ“®ğŸ“¯ğŸ“°ğŸ“±ğŸ“²ğŸ“³ğŸ“´ğŸ“µğŸ“¶ğŸ“·ğŸ“¸ğŸ“¹ğŸ“ºğŸ“»ğŸ“¼ğŸ“½ï¸ğŸ“¾ğŸ“¿ğŸ”€ğŸ”ğŸ”‚ğŸ”ƒğŸ”„ğŸ”…ğŸ”†ğŸ”‡ğŸ”ˆğŸ”‰ğŸ”ŠğŸ”‹ğŸ”ŒğŸ”ğŸ”ğŸ”ğŸ”ğŸ”‘ğŸ”’ğŸ”“ğŸ””ğŸ”•ğŸ”–ğŸ”—ğŸ”˜ğŸ”™ğŸ”šğŸ”›ğŸ”œğŸ”ğŸ”ğŸ”ŸğŸ” ğŸ”¡ğŸ”¢ğŸ”£ğŸ”¤ğŸ”¥ğŸ”¦ğŸ”§ğŸ”¨ğŸ”©ğŸ”ªğŸ”«ğŸ”¬ğŸ”­ğŸ”®ğŸ”¯ğŸ”°ğŸ”±ğŸ”²ğŸ”³ğŸ”´ğŸ”µğŸ”¶ğŸ”·ğŸ”¸ğŸ”¹ğŸ”ºğŸ”»ğŸ”¼ğŸ”½ğŸ”¾ğŸ”¿ğŸ•€ğŸ•ğŸ•‚ğŸ•ƒğŸ•„ğŸ•…ğŸ•†ğŸ•‡ğŸ•ˆğŸ•‰ğŸ•ŠğŸ•‹ğŸ•ŒğŸ•ğŸ•ğŸ•ğŸ•ğŸ•‘ğŸ•’ğŸ•“ğŸ•”ğŸ••ğŸ•–ğŸ•—ğŸ•˜ğŸ•™ğŸ•šğŸ•›ğŸ•œğŸ•ğŸ•ğŸ•ŸğŸ• ğŸ•¡ğŸ•¢ğŸ•£ğŸ•¤ğŸ•¥ğŸ•¦ğŸ•§ğŸ•¨ğŸ•©ğŸ•ªğŸ•«ğŸ•¬ğŸ•­ğŸ•®ğŸ•¯ğŸ•°ï¸ğŸ•±ğŸ•²ğŸ•³ğŸ•´ï¸ğŸ•µï¸ğŸ•¶ï¸ğŸ•·ï¸ğŸ•¸ï¸ğŸ•¹ï¸ğŸ•ºğŸ•»ğŸ•¼ğŸ•½ğŸ•¾ğŸ•¿ğŸ–€ğŸ–ğŸ–‚ğŸ–ƒğŸ–„ğŸ–…ğŸ–†ğŸ–‡ï¸ğŸ–ˆğŸ–‰ğŸ–Šï¸ğŸ–‹ï¸ğŸ–Œï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–‘ğŸ–’ğŸ–“ğŸ–”ï¸ğŸ–•ğŸ––ğŸ–—ğŸ–˜ğŸ–™ğŸ–šğŸ–›ğŸ–œğŸ–ğŸ–ğŸ–ŸğŸ– ğŸ–¡ğŸ–¢ğŸ–£ğŸ–¤ğŸ–¥ï¸ğŸ–¦ğŸ–§ğŸ–¨ï¸ğŸ–©ğŸ–ªğŸ–«ğŸ–¬ğŸ–­ğŸ–®ğŸ–¯ğŸ–°ï¸ğŸ–±ï¸ğŸ–²ï¸ğŸ–³ï¸ğŸ–´ï¸ğŸ–µï¸ğŸ–¶ğŸ–·ï¸ğŸ–¸ğŸ–¹ğŸ–ºğŸ–»ğŸ–¼ï¸ğŸ–½ğŸ–¾ğŸ–¿ğŸ—€ğŸ—ğŸ—‚ï¸ğŸ—ƒï¸ğŸ—„ï¸ğŸ—…ğŸ—†ğŸ—‡ğŸ—ˆğŸ—‰ğŸ—ŠğŸ—‹ğŸ—ŒğŸ—ğŸ—ğŸ—ğŸ—ğŸ—‘ğŸ—’ğŸ—“ï¸ğŸ—”ï¸ğŸ—•ï¸ğŸ—–ï¸ğŸ——ğŸ—˜ğŸ—™ğŸ—šğŸ—›ğŸ—œğŸ—ğŸ—ï¸ğŸ—Ÿï¸ğŸ— ï¸ğŸ—¡ï¸ğŸ—¢ï¸ğŸ—£ï¸ğŸ—¤ï¸ğŸ—¥ï¸ğŸ—¦ï¸ğŸ—§ï¸ğŸ—¨ï¸ğŸ—©ï¸ğŸ—ªï¸ğŸ—«ï¸ğŸ—¬ï¸ğŸ—­ï¸ğŸ—®ï¸ğŸ—¯ï¸ğŸ—°ï¸ğŸ—±ï¸ğŸ—²ï¸ğŸ—³ï¸ğŸ—´ï¸ğŸ—µï¸ğŸ—¶ï¸ğŸ—·ï¸ğŸ—¸ï¸ğŸ—¹ï¸ğŸ—ºï¸ğŸ—»ï¸ğŸ—¼ï¸ğŸ—½ï¸ğŸ—¾ï¸ğŸ—¿ï¸]')
        
        emoji_titles = [title for title in titles if emoji_pattern.search(title)]
        emoji_counts = Counter()
        
        for title in emoji_titles:
            emojis = emoji_pattern.findall(title)
            emoji_counts.update(emojis)
        
        return {
            'emoji_titles_count': len(emoji_titles),
            'emoji_usage_rate': len(emoji_titles) / len(titles) * 100,
            'top_emojis': dict(emoji_counts.most_common(10)),
            'emoji_diversity': len(emoji_counts)
        }
    
    def analyze_stock_codes(self, titles):
        """åˆ†æè‚¡ç¥¨ä»£è™Ÿä½¿ç”¨"""
        stock_pattern = re.compile(r'\b\d{4}\b')
        stock_titles = [title for title in titles if stock_pattern.search(title)]
        stock_codes = []
        
        for title in stock_titles:
            codes = stock_pattern.findall(title)
            stock_codes.extend(codes)
        
        stock_counts = Counter(stock_codes)
        
        return {
            'stock_titles_count': len(stock_titles),
            'stock_usage_rate': len(stock_titles) / len(titles) * 100,
            'top_stocks': dict(stock_counts.most_common(10)),
            'stock_diversity': len(stock_counts)
        }
    
    def analyze_interaction_patterns(self, titles):
        """åˆ†æäº’å‹•æ¨¡å¼"""
        patterns = {
            'question': ['?', 'ï¼Ÿ', 'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'è©²', 'å—'],
            'exclamation': ['!', 'ï¼', 'å•Š', 'å“‡', 'å–”'],
            'command': ['è«‹', 'æ³¨æ„', 'æé†’', 'å»ºè­°', 'å¿«'],
            'invitation': ['ä¸€èµ·', 'å¤§å®¶', 'æˆ‘å€‘', 'ä¾†', 'ä¾†å§']
        }
        
        results = {}
        for pattern_type, keywords in patterns.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[pattern_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_emotions(self, titles):
        """åˆ†ææƒ…æ„Ÿæ¨¡å¼"""
        emotions = {
            'very_positive': ['å¤ªæ£’äº†', 'è¶…è®š', 'ç¥äº†', 'å®Œç¾', 'ç„¡æ•µ', 'å¤ªç¥äº†'],
            'positive': ['å¥½æ£’', 'èˆ’æœ', 'é–‹å¿ƒ', 'æœŸå¾…', 'çœ‹å¥½', 'å¼·å‹¢'],
            'neutral': ['æ³¨æ„', 'é—œæ³¨', 'æé†’', 'å»ºè­°', 'åˆ†æ', 'è§€å¯Ÿ'],
            'negative': ['çˆ›', 'æ…˜', 'æ­»', 'é¨™', 'ç‹ ', 'è·Œ'],
            'very_negative': ['çˆ›é€äº†', 'æ…˜å…®å…®', 'æ²’æ•‘', 'å®Œè›‹', 'å´©æ½°']
        }
        
        results = {}
        for emotion, keywords in emotions.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[emotion] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_professional_terms(self, titles):
        """åˆ†æå°ˆæ¥­è¡“èª"""
        terms = {
            'technical': ['çªç ´', 'æ”¯æ’', 'å£“åŠ›', 'å‡ç·š', 'Kç·š', 'è¶¨å‹¢', 'æ´—ç›¤', 'åšé ­', 'èƒŒé›¢'],
            'fundamental': ['ç‡Ÿæ”¶', 'è²¡å ±', 'EPS', 'ç²åˆ©', 'æ¥­ç¸¾', 'è‚¡åˆ©', 'é…æ¯'],
            'sentiment': ['å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'ä¸»åŠ›', 'æ•£æˆ¶', 'èè³‡', 'èåˆ¸'],
            'news': ['æ–°è', 'å ±å°', 'å…¬å‘Š', 'åˆä½œ', 'æ“´ç”¢', 'å¸ƒå±€', 'é¡Œæ']
        }
        
        results = {}
        for term_type, keywords in terms.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[term_type] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def analyze_topics(self, titles):
        """åˆ†æç†±é–€è©±é¡Œ"""
        topics = {
            'AI': ['AI', 'äººå·¥æ™ºæ…§', 'æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’'],
            'semiconductor': ['åŠå°é«”', 'æ™¶ç‰‡', 'IC', 'æ™¶åœ“', 'å°ç©é›»', 'è¯ç™¼ç§‘'],
            'shipping': ['èˆªé‹', 'è²¨æ«ƒ', 'æµ·é‹', 'é•·æ¦®', 'é™½æ˜', 'è¬æµ·'],
            'biotech': ['ç”ŸæŠ€', 'é†«è—¥', 'è—¥å“', 'ç–«è‹—', 'åŸºå› '],
            'finance': ['é‡‘è', 'éŠ€è¡Œ', 'ä¿éšª', 'è­‰åˆ¸', 'é‡‘æ§'],
            'ev': ['é›»å‹•è»Š', 'ç‰¹æ–¯æ‹‰', 'é›»è»Š', 'æ–°èƒ½æº'],
            'metaverse': ['å…ƒå®‡å®™', 'VR', 'AR', 'è™›æ“¬å¯¦å¢ƒ'],
            '5g': ['5G', 'é€šè¨Š', 'é›»ä¿¡'],
            'green_energy': ['ç¶ èƒ½', 'å¤ªé™½èƒ½', 'é¢¨é›»', 'ç’°ä¿', 'ç¢³ä¸­å’Œ']
        }
        
        results = {}
        for topic, keywords in topics.items():
            matching_titles = [title for title in titles if any(keyword in title for keyword in keywords)]
            results[topic] = {
                'count': len(matching_titles),
                'rate': len(matching_titles) / len(titles) * 100
            }
        
        return results
    
    def run_short_title_analysis(self):
        """é‹è¡ŒçŸ­æ¨™é¡Œåˆ†æ"""
        print("ğŸš€ é–‹å§‹ UGC çŸ­æ¨™é¡Œåˆ†æ")
        print("="*80)
        
        all_titles = []
        
        # åˆ†ææ¯å€‹æ–‡ä»¶
        for i, filepath in enumerate(self.data_files):
            if os.path.exists(filepath):
                print(f"\nğŸ“Š åˆ†ææ–‡ä»¶ {i+1}: {filepath}")
                titles = self.smart_read_large_file(filepath)
                all_titles.extend(titles)
                
                # éæ¿¾çŸ­æ¨™é¡Œ
                short_titles = self.filter_short_titles(titles)
                
                # åˆ†æä¸åŒé•·åº¦ç¯„åœ
                for length_type, filtered_titles in short_titles.items():
                    if filtered_titles:
                        print(f"   ğŸ“ {length_type} (â‰¤{self.short_title_ranges[length_type]}å­—): {len(filtered_titles)} å€‹")
                        
                        # åˆ†æçŸ­æ¨™é¡Œ
                        analysis_results = {}
                        analysis_functions = [
                            ('length_distribution', self.analyze_length_distribution),
                            ('emoji_analysis', self.analyze_emojis),
                            ('stock_codes', self.analyze_stock_codes),
                            ('interaction_patterns', self.analyze_interaction_patterns),
                            ('emotion_analysis', self.analyze_emotions),
                            ('professional_terms', self.analyze_professional_terms),
                            ('topic_analysis', self.analyze_topics)
                        ]
                        
                        for analysis_name, analysis_func in analysis_functions:
                            try:
                                analysis_results[analysis_name] = analysis_func(filtered_titles)
                            except Exception as e:
                                print(f"      âš ï¸ {analysis_name} åˆ†æå¤±æ•—: {e}")
                        
                        self.results[f'file_{i+1}_{length_type}'] = {
                            'filepath': filepath,
                            'title_count': len(filtered_titles),
                            'max_length': self.short_title_ranges[length_type],
                            'analysis': analysis_results
                        }
                
                # æ¸…ç†è¨˜æ†¶é«”
                del titles
                gc.collect()
        
        # åˆä½µåˆ†æçŸ­æ¨™é¡Œ
        print(f"\nğŸ”„ åˆä½µåˆ†æçŸ­æ¨™é¡Œ...")
        
        # éæ¿¾æ‰€æœ‰æ¨™é¡Œä¸­çš„çŸ­æ¨™é¡Œ
        all_short_titles = self.filter_short_titles(all_titles)
        
        for length_type, filtered_titles in all_short_titles.items():
            if filtered_titles:
                print(f"   ğŸ“ ç¸½è¨ˆ {length_type} (â‰¤{self.short_title_ranges[length_type]}å­—): {len(filtered_titles)} å€‹")
                
                # åˆ†æåˆä½µçš„çŸ­æ¨™é¡Œ
                combined_analysis = {}
                analysis_functions = [
                    ('length_distribution', self.analyze_length_distribution),
                    ('emoji_analysis', self.analyze_emojis),
                    ('stock_codes', self.analyze_stock_codes),
                    ('interaction_patterns', self.analyze_interaction_patterns),
                    ('emotion_analysis', self.analyze_emotions),
                    ('professional_terms', self.analyze_professional_terms),
                    ('topic_analysis', self.analyze_topics)
                ]
                
                for analysis_name, analysis_func in analysis_functions:
                    try:
                        combined_analysis[analysis_name] = analysis_func(filtered_titles)
                    except Exception as e:
                        print(f"      âš ï¸ åˆä½µ {analysis_name} åˆ†æå¤±æ•—: {e}")
                
                self.combined_stats[length_type] = {
                    'title_count': len(filtered_titles),
                    'max_length': self.short_title_ranges[length_type],
                    'analysis': combined_analysis
                }
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_short_title_report()
        
        return self.results, self.combined_stats
    
    def generate_short_title_report(self):
        """ç”ŸæˆçŸ­æ¨™é¡Œåˆ†æå ±å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆ UGC çŸ­æ¨™é¡Œåˆ†æå ±å‘Š")
        print("="*80)
        
        # 1. æ•¸æ“šæ¦‚è¦½
        total_original = sum(result['title_count'] for result in self.results.values() if 'very_short' in result)
        print(f"ğŸ“ˆ çŸ­æ¨™é¡Œæ•¸æ“šæ¦‚è¦½:")
        print(f"   åŸå§‹æ¨™é¡Œç¸½æ•¸: {total_original:,}")
        
        for length_type, stats in self.combined_stats.items():
            print(f"   {length_type} (â‰¤{stats['max_length']}å­—): {stats['title_count']:,} å€‹")
        
        # 2. é—œéµç™¼ç¾
        print(f"\nğŸ¯ çŸ­æ¨™é¡Œé—œéµç™¼ç¾:")
        
        # é‡é»åˆ†æ â‰¤15å­— å’Œ â‰¤20å­—
        for length_type in ['very_short', 'short']:
            if length_type in self.combined_stats:
                stats = self.combined_stats[length_type]
                analysis = stats['analysis']
                
                print(f"\nğŸ“ {length_type} (â‰¤{stats['max_length']}å­—) åˆ†æ:")
                
                # é•·åº¦åˆ†å¸ƒ
                if 'length_distribution' in analysis:
                    length_stats = analysis['length_distribution']
                    print(f"   å¹³å‡é•·åº¦: {length_stats['avg_length']:.1f} å­—")
                    print(f"   ä¸­ä½æ•¸: {length_stats['median_length']:.1f} å­—")
                
                # è¡¨æƒ…ç¬¦è™Ÿ
                if 'emoji_analysis' in analysis:
                    emoji_stats = analysis['emoji_analysis']
                    print(f"   è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨ç‡: {emoji_stats['emoji_usage_rate']:.1f}%")
                    if emoji_stats['top_emojis']:
                        top_emoji = list(emoji_stats['top_emojis'].keys())[0]
                        print(f"   æœ€å¸¸è¦‹è¡¨æƒ…: {top_emoji}")
                
                # äº’å‹•æ¨¡å¼
                if 'interaction_patterns' in analysis:
                    interaction_stats = analysis['interaction_patterns']
                    exclamation_rate = interaction_stats['exclamation']['rate']
                    question_rate = interaction_stats['question']['rate']
                    print(f"   æ„Ÿå˜†å¥: {exclamation_rate:.1f}%")
                    print(f"   å•å¥: {question_rate:.1f}%")
                
                # å°ˆæ¥­è¡“èª
                if 'professional_terms' in analysis:
                    professional_stats = analysis['professional_terms']
                    fundamental_rate = professional_stats['fundamental']['rate']
                    technical_rate = professional_stats['technical']['rate']
                    print(f"   åŸºæœ¬é¢: {fundamental_rate:.1f}%")
                    print(f"   æŠ€è¡“é¢: {technical_rate:.1f}%")
        
        # 3. AI æ¨™é¡Œç”Ÿæˆå»ºè­°
        print(f"\nğŸ’¡ åŸºæ–¼çŸ­æ¨™é¡Œçš„ AI ç”Ÿæˆå»ºè­°:")
        
        # é‡é»åˆ†æ â‰¤15å­—
        if 'very_short' in self.combined_stats:
            very_short_stats = self.combined_stats['very_short']
            very_short_analysis = very_short_stats['analysis']
            
            recommendations = []
            
            # åŸºæ–¼é•·åº¦
            if 'length_distribution' in very_short_analysis:
                length_stats = very_short_analysis['length_distribution']
                recommendations.append(f"1. é‡é»ç”Ÿæˆ â‰¤15å­— çš„ç°¡æ½”æ¨™é¡Œï¼Œå¹³å‡é•·åº¦ {length_stats['avg_length']:.1f} å­—")
            
            # åŸºæ–¼è¡¨æƒ…ç¬¦è™Ÿ
            if 'emoji_analysis' in very_short_analysis:
                emoji_stats = very_short_analysis['emoji_analysis']
                recommendations.append(f"2. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç›®æ¨™ä½¿ç”¨ç‡ {emoji_stats['emoji_usage_rate']:.1f}%")
            
            # åŸºæ–¼äº’å‹•æ¨¡å¼
            if 'interaction_patterns' in very_short_analysis:
                interaction_stats = very_short_analysis['interaction_patterns']
                exclamation_rate = interaction_stats['exclamation']['rate']
                question_rate = interaction_stats['question']['rate']
                recommendations.append(f"3. å¢åŠ äº’å‹•æ€§ï¼šæ„Ÿå˜†å¥ {exclamation_rate:.1f}%ï¼Œå•å¥ {question_rate:.1f}%")
            
            # åŸºæ–¼å°ˆæ¥­è¡“èª
            if 'professional_terms' in very_short_analysis:
                professional_stats = very_short_analysis['professional_terms']
                fundamental_rate = professional_stats['fundamental']['rate']
                technical_rate = professional_stats['technical']['rate']
                recommendations.append(f"4. èå…¥å°ˆæ¥­è¡“èªï¼šåŸºæœ¬é¢ {fundamental_rate:.1f}%ï¼ŒæŠ€è¡“é¢ {technical_rate:.1f}%")
            
            for rec in recommendations:
                print(f"   {rec}")
        
        # 4. ç¤ºä¾‹æ¨™é¡Œ
        print(f"\nğŸ¯ çŸ­æ¨™é¡Œç¤ºä¾‹:")
        
        # å¾åˆ†æçµæœä¸­æå–ä¸€äº›ç¤ºä¾‹
        if 'very_short' in self.combined_stats:
            very_short_titles = []
            for result in self.results.values():
                if 'very_short' in result and result['title_count'] > 0:
                    # é€™è£¡å¯ä»¥æ·»åŠ ä¸€äº›ç¤ºä¾‹æ¨™é¡Œ
                    very_short_titles.extend([
                        "å°ç©é›»æ€éº¼äº†ï¼Ÿ",
                        "AIæ¦‚å¿µè‚¡å¤ªçŒ›äº†ï¼",
                        "æ³¨æ„ï¼èˆªé‹è‚¡èµ·é£›",
                        "ç‡Ÿæ”¶æˆé•·50%",
                        "æŠ€è¡“é¢çªç ´"
                    ])
            
            print("   â‰¤15å­— ç¤ºä¾‹:")
            for i, title in enumerate(very_short_titles[:5], 1):
                print(f"     {i}. {title}")
        
        # 5. ä¿å­˜è©³ç´°çµæœ
        self.save_short_title_results()
    
    def save_short_title_results(self):
        """ä¿å­˜çŸ­æ¨™é¡Œåˆ†æçµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ugc_short_title_analysis_{timestamp}.json"
        
        output_data = {
            'analysis_timestamp': timestamp,
            'short_title_ranges': self.short_title_ranges,
            'files_analyzed': self.results,
            'combined_statistics': self.combined_stats
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ çŸ­æ¨™é¡Œåˆ†æçµæœå·²ä¿å­˜è‡³: {filename}")

def main():
    """ä¸»å‡½æ•¸"""
    analyzer = UGCShortTitleAnalyzer()
    results, combined_stats = analyzer.run_short_title_analysis()
    
    print("\nâœ… UGC çŸ­æ¨™é¡Œåˆ†æå®Œæˆï¼")
    return results, combined_stats

if __name__ == "__main__":
    main()
