#!/usr/bin/env python3
"""
æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶åŸå‹
åŸºæ–¼å¯¦éš› CMoney äº’å‹•æ•¸æ“šçš„å­¸ç¿’ç³»çµ±
"""

import os
import sys
import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import numpy as np
from collections import defaultdict
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class InteractionData:
    """äº’å‹•æ•¸æ“šçµæ§‹"""
    article_id: str
    kol_id: str
    kol_nickname: str
    likes: int
    comments: int
    shares: int
    emoji_total: int
    total_interactions: int
    engagement_rate: float
    post_timestamp: str
    content: str
    topic_id: str

@dataclass
class LearningInsight:
    """å­¸ç¿’æ´å¯Ÿ"""
    insight_type: str
    priority: str  # 'low', 'medium', 'high', 'critical'
    description: str
    suggested_actions: List[str]
    confidence: float
    timestamp: datetime

@dataclass
class KOLStrategy:
    """KOL ç­–ç•¥"""
    kol_id: str
    content_type_weights: Dict[str, float]
    persona_adjustments: Dict[str, float]
    timing_preferences: Dict[str, Any]
    interaction_style: Dict[str, Any]
    last_updated: datetime

class RealTimeAnalyzer:
    """å¯¦æ™‚åˆ†æå™¨"""
    
    def __init__(self):
        self.interaction_history = []
        self.kol_performance = defaultdict(list)
        
    def analyze_interaction_performance(self, interaction_data: InteractionData) -> Dict[str, Any]:
        """åˆ†æäº’å‹•è¡¨ç¾"""
        # è¨ˆç®—ç¶œåˆäº’å‹•åˆ†æ•¸
        engagement_score = self._calculate_engagement_score(interaction_data)
        
        # åˆ†æè¡¨æƒ…æƒ…æ„Ÿ
        emoji_sentiment = self._analyze_emoji_sentiment(interaction_data)
        
        # åˆ†ææ™‚é–“æ¨¡å¼
        timing_analysis = self._analyze_timing_pattern(interaction_data)
        
        # åˆ†æå…§å®¹ç‰¹å¾µ
        content_analysis = self._analyze_content_features(interaction_data)
        
        return {
            'engagement_score': engagement_score,
            'emoji_sentiment': emoji_sentiment,
            'timing_analysis': timing_analysis,
            'content_analysis': content_analysis,
            'performance_trend': self._analyze_performance_trend(interaction_data)
        }
    
    def _calculate_engagement_score(self, data: InteractionData) -> float:
        """è¨ˆç®—ç¶œåˆäº’å‹•åˆ†æ•¸"""
        # åŠ æ¬Šè¨ˆç®—
        likes_weight = 0.3
        comments_weight = 0.4
        shares_weight = 0.2
        emoji_weight = 0.1
        
        weighted_score = (
            data.likes * likes_weight +
            data.comments * comments_weight +
            data.shares * shares_weight +
            data.emoji_total * emoji_weight
        )
        
        # æ¨™æº–åŒ–åˆ° 0-100 åˆ†
        normalized_score = min(weighted_score * 5, 100)
        
        return normalized_score
    
    def _analyze_emoji_sentiment(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æè¡¨æƒ…æƒ…æ„Ÿ"""
        # é€™è£¡å¯ä»¥æ ¹æ“šå¯¦éš›çš„è¡¨æƒ…æ•¸æ“šé€²è¡Œåˆ†æ
        # ç›®å‰ä½¿ç”¨ç°¡åŒ–çš„åˆ†æ
        return {
            'positive_ratio': 0.8,  # å‡è¨­ 80% æ­£é¢è¡¨æƒ…
            'engagement_level': 'high' if data.emoji_total > 5 else 'medium',
            'sentiment_score': 0.7  # 0-1 çš„æƒ…æ„Ÿåˆ†æ•¸
        }
    
    def _analyze_timing_pattern(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†ææ™‚é–“æ¨¡å¼"""
        try:
            post_time = datetime.fromisoformat(data.post_timestamp.replace('Z', '+00:00'))
            hour = post_time.hour
            
            return {
                'post_hour': hour,
                'is_peak_hour': 9 <= hour <= 11 or 19 <= hour <= 21,
                'timezone_effect': 'positive' if 9 <= hour <= 21 else 'negative',
                'optimal_timing': self._find_optimal_timing(hour)
            }
        except:
            return {
                'post_hour': 0,
                'is_peak_hour': False,
                'timezone_effect': 'unknown',
                'optimal_timing': 'unknown'
            }
    
    def _analyze_content_features(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æå…§å®¹ç‰¹å¾µ"""
        content = data.content
        if not content:
            content = "è²¼æ–‡å…§å®¹..."
        
        return {
            'content_length': len(content),
            'readability_score': self._calculate_readability(content),
            'personalization_level': self._analyze_personalization(content),
            'ai_detection_risk': self._assess_ai_detection_risk(content),
            'engagement_potential': self._assess_engagement_potential(content)
        }
    
    def _calculate_readability(self, content: str) -> float:
        """è¨ˆç®—å¯è®€æ€§åˆ†æ•¸"""
        # ç°¡åŒ–çš„å¯è®€æ€§è¨ˆç®—
        sentences = content.split('ã€‚')
        avg_sentence_length = sum(len(s) for s in sentences) / max(len(sentences), 1)
        
        # å¯è®€æ€§åˆ†æ•¸ (0-100)
        if avg_sentence_length < 20:
            return 90
        elif avg_sentence_length < 30:
            return 70
        else:
            return 50
    
    def _analyze_personalization(self, content: str) -> float:
        """åˆ†æå€‹äººåŒ–ç¨‹åº¦"""
        personal_indicators = [
            'æˆ‘è¦ºå¾—', 'æˆ‘èªç‚º', 'æˆ‘çš„çœ‹æ³•', 'å€‹äººè¦ºå¾—', 'æˆ‘', 'æˆ‘çš„',
            'å“ˆå“ˆ', 'å‘µå‘µ', 'é ', 'å¹¹', 'çœŸçš„å‡çš„', '...', '!!!'
        ]
        
        personal_count = sum(1 for indicator in personal_indicators if indicator in content)
        return min(personal_count / 5, 1.0)  # æ¨™æº–åŒ–åˆ° 0-1
    
    def _assess_ai_detection_risk(self, content: str) -> float:
        """è©•ä¼° AI åµæ¸¬é¢¨éšª"""
        risk_factors = 0
        
        # æª¢æŸ¥éæ–¼æ­£å¼çš„èªè¨€
        formal_indicators = ['å› æ­¤', 'ç„¶è€Œ', 'æ­¤å¤–', 'ç¸½è€Œè¨€ä¹‹', 'ç¶œä¸Šæ‰€è¿°']
        risk_factors += sum(1 for indicator in formal_indicators if indicator in content)
        
        # æª¢æŸ¥ç¼ºä¹å€‹äººåŒ–
        personal_score = self._analyze_personalization(content)
        risk_factors += (1 - personal_score) * 3
        
        # æª¢æŸ¥çµæ§‹éæ–¼è¦æ•´
        sentences = content.split('ã€‚')
        if len(sentences) > 2:
            sentence_lengths = [len(s) for s in sentences if s.strip()]
            if sentence_lengths:
                length_variance = np.var(sentence_lengths)
                if length_variance < 50:  # å¥å­é•·åº¦è®ŠåŒ–å¤ªå°
                    risk_factors += 2
        
        return min(risk_factors / 10, 1.0)  # æ¨™æº–åŒ–åˆ° 0-1
    
    def _assess_engagement_potential(self, content: str) -> float:
        """è©•ä¼°äº’å‹•æ½›åŠ›"""
        engagement_indicators = [
            'ä½ è¦ºå¾—', 'å¤§å®¶æ€éº¼çœ‹', 'æ­¡è¿è¨è«–', 'åˆ†äº«ä½ çš„çœ‹æ³•',
            '?', 'ï¼Ÿ', 'ï¼', '!!!', 'å“ˆå“ˆ', 'å‘µå‘µ'
        ]
        
        engagement_count = sum(1 for indicator in engagement_indicators if indicator in content)
        return min(engagement_count / 3, 1.0)
    
    def _find_optimal_timing(self, hour: int) -> str:
        """æ‰¾åˆ°æœ€ä½³ç™¼æ–‡æ™‚æ©Ÿ"""
        if 9 <= hour <= 11:
            return "morning_peak"
        elif 12 <= hour <= 14:
            return "lunch_time"
        elif 19 <= hour <= 21:
            return "evening_peak"
        else:
            return "off_peak"
    
    def _analyze_performance_trend(self, data: InteractionData) -> Dict[str, Any]:
        """åˆ†æè¡¨ç¾è¶¨å‹¢"""
        # ä¿å­˜æ­·å²æ•¸æ“š
        self.kol_performance[data.kol_id].append(data)
        
        if len(self.kol_performance[data.kol_id]) < 2:
            return {'trend': 'insufficient_data', 'change_rate': 0}
        
        # è¨ˆç®—è¶¨å‹¢
        recent_data = self.kol_performance[data.kol_id][-5:]  # æœ€è¿‘5ç¯‡
        if len(recent_data) >= 2:
            recent_avg = sum(d.total_interactions for d in recent_data) / len(recent_data)
            previous_avg = sum(d.total_interactions for d in self.kol_performance[data.kol_id][:-5]) / max(len(self.kol_performance[data.kol_id]) - 5, 1)
            
            change_rate = (recent_avg - previous_avg) / max(previous_avg, 1)
            
            if change_rate > 0.1:
                trend = 'improving'
            elif change_rate < -0.1:
                trend = 'declining'
            else:
                trend = 'stable'
        else:
            trend = 'insufficient_data'
            change_rate = 0
        
        return {'trend': trend, 'change_rate': change_rate}

class PatternDetector:
    """æ¨¡å¼åµæ¸¬å™¨"""
    
    def __init__(self):
        self.success_patterns = defaultdict(list)
        self.failure_patterns = defaultdict(list)
    
    def detect_patterns(self, interaction_data: InteractionData, performance_analysis: Dict) -> Dict[str, Any]:
        """åµæ¸¬æ¨¡å¼"""
        patterns = {
            'success_patterns': self._detect_success_patterns(interaction_data, performance_analysis),
            'failure_patterns': self._detect_failure_patterns(interaction_data, performance_analysis),
            'audience_patterns': self._detect_audience_patterns(interaction_data, performance_analysis)
        }
        
        return patterns
    
    def _detect_success_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬æˆåŠŸæ¨¡å¼"""
        patterns = []
        
        # é«˜äº’å‹•æ¨¡å¼
        if analysis['engagement_score'] > 70:
            patterns.append({
                'type': 'high_engagement',
                'description': 'é«˜äº’å‹•è¡¨ç¾',
                'confidence': 0.8,
                'features': {
                    'engagement_score': analysis['engagement_score'],
                    'emoji_sentiment': analysis['emoji_sentiment']['sentiment_score'],
                    'timing': analysis['timing_analysis']['optimal_timing']
                }
            })
        
        # æƒ…æ„Ÿè¡¨é”æ¨¡å¼
        if analysis['emoji_sentiment']['sentiment_score'] > 0.6:
            patterns.append({
                'type': 'positive_sentiment',
                'description': 'æ­£é¢æƒ…æ„Ÿè¡¨é”',
                'confidence': 0.7,
                'features': {
                    'sentiment_score': analysis['emoji_sentiment']['sentiment_score'],
                    'engagement_level': analysis['emoji_sentiment']['engagement_level']
                }
            })
        
        # æœ€ä½³æ™‚æ©Ÿæ¨¡å¼
        if analysis['timing_analysis']['is_peak_hour']:
            patterns.append({
                'type': 'optimal_timing',
                'description': 'æœ€ä½³ç™¼æ–‡æ™‚æ©Ÿ',
                'confidence': 0.6,
                'features': {
                    'post_hour': analysis['timing_analysis']['post_hour'],
                    'timezone_effect': analysis['timing_analysis']['timezone_effect']
                }
            })
        
        return patterns
    
    def _detect_failure_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬å¤±æ•—æ¨¡å¼"""
        patterns = []
        
        # ä½äº’å‹•æ¨¡å¼
        if analysis['engagement_score'] < 30:
            patterns.append({
                'type': 'low_engagement',
                'description': 'äº’å‹•ç‡åä½',
                'confidence': 0.8,
                'features': {
                    'engagement_score': analysis['engagement_score'],
                    'content_length': analysis['content_analysis']['content_length']
                }
            })
        
        # AI åµæ¸¬é¢¨éšªæ¨¡å¼
        if analysis['content_analysis']['ai_detection_risk'] > 0.7:
            patterns.append({
                'type': 'ai_detection_risk',
                'description': 'AI åµæ¸¬é¢¨éšªè¼ƒé«˜',
                'confidence': 0.9,
                'features': {
                    'ai_detection_risk': analysis['content_analysis']['ai_detection_risk'],
                    'personalization_level': analysis['content_analysis']['personalization_level']
                }
            })
        
        # éæœ€ä½³æ™‚æ©Ÿæ¨¡å¼
        if not analysis['timing_analysis']['is_peak_hour']:
            patterns.append({
                'type': 'suboptimal_timing',
                'description': 'ç™¼æ–‡æ™‚æ©Ÿä¸ä½³',
                'confidence': 0.5,
                'features': {
                    'post_hour': analysis['timing_analysis']['post_hour'],
                    'timezone_effect': analysis['timing_analysis']['timezone_effect']
                }
            })
        
        return patterns
    
    def _detect_audience_patterns(self, data: InteractionData, analysis: Dict) -> List[Dict]:
        """åµæ¸¬å—çœ¾æ¨¡å¼"""
        patterns = []
        
        # å—çœ¾æ´»èºåº¦æ¨¡å¼
        if data.comments > 0:
            patterns.append({
                'type': 'active_audience',
                'description': 'å—çœ¾äº’å‹•æ´»èº',
                'confidence': 0.7,
                'features': {
                    'comment_count': data.comments,
                    'engagement_rate': data.engagement_rate
                }
            })
        
        # è¡¨æƒ…å›æ‡‰æ¨¡å¼
        if data.emoji_total > 5:
            patterns.append({
                'type': 'emoji_response',
                'description': 'å—çœ¾è¡¨æƒ…å›æ‡‰ç©æ¥µ',
                'confidence': 0.6,
                'features': {
                    'emoji_total': data.emoji_total,
                    'sentiment_score': analysis['emoji_sentiment']['sentiment_score']
                }
            })
        
        return patterns

class RiskAssessor:
    """é¢¨éšªè©•ä¼°å™¨"""
    
    def __init__(self):
        self.risk_thresholds = {
            'ai_detection': 0.7,
            'low_engagement': 0.3,
            'negative_sentiment': 0.4,
            'suboptimal_timing': 0.5
        }
    
    def assess_risks(self, interaction_data: InteractionData, performance_analysis: Dict, patterns: Dict) -> Dict[str, Any]:
        """è©•ä¼°é¢¨éšª"""
        risks = {
            'ai_detection_risk': self._assess_ai_detection_risk(performance_analysis, patterns),
            'engagement_risk': self._assess_engagement_risk(performance_analysis, patterns),
            'timing_risk': self._assess_timing_risk(performance_analysis, patterns),
            'content_quality_risk': self._assess_content_quality_risk(performance_analysis, patterns),
            'overall_risk': 0.0
        }
        
        # è¨ˆç®—æ•´é«”é¢¨éšª
        risk_scores = [risks[k] for k in risks.keys() if k != 'overall_risk']
        risks['overall_risk'] = sum(risk_scores) / len(risk_scores)
        
        return risks
    
    def _assess_ai_detection_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼° AI åµæ¸¬é¢¨éšª"""
        base_risk = analysis['content_analysis']['ai_detection_risk']
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´é¢¨éšª
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'ai_detection_risk':
                base_risk += 0.2
        
        return min(base_risk, 1.0)
    
    def _assess_engagement_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°äº’å‹•é¢¨éšª"""
        base_risk = 1.0 - (analysis['engagement_score'] / 100)
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´é¢¨éšª
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'low_engagement':
                base_risk += 0.3
        
        return min(base_risk, 1.0)
    
    def _assess_timing_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°æ™‚æ©Ÿé¢¨éšª"""
        if analysis['timing_analysis']['is_peak_hour']:
            return 0.2
        else:
            return 0.6
    
    def _assess_content_quality_risk(self, analysis: Dict, patterns: Dict) -> float:
        """è©•ä¼°å…§å®¹å“è³ªé¢¨éšª"""
        risks = []
        
        # å¯è®€æ€§é¢¨éšª
        if analysis['content_analysis']['readability_score'] < 50:
            risks.append(0.5)
        
        # å€‹äººåŒ–é¢¨éšª
        if analysis['content_analysis']['personalization_level'] < 0.3:
            risks.append(0.4)
        
        # äº’å‹•æ½›åŠ›é¢¨éšª
        if analysis['content_analysis']['engagement_potential'] < 0.3:
            risks.append(0.3)
        
        return sum(risks) / max(len(risks), 1)

class LearningInsightGenerator:
    """å­¸ç¿’æ´å¯Ÿç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.insight_templates = {
            'content_optimization': {
                'low_engagement': 'äº’å‹•ç‡åä½ï¼Œå»ºè­°å¢åŠ å€‹äººåŒ–å…ƒç´ å’Œæƒ…æ„Ÿè¡¨é”',
                'ai_detection_risk': 'AI åµæ¸¬é¢¨éšªè¼ƒé«˜ï¼Œéœ€è¦å¢åŠ äººæ€§åŒ–è¡¨é”',
                'low_personalization': 'å€‹äººåŒ–ç¨‹åº¦ä¸è¶³ï¼Œå»ºè­°å¢åŠ å€‹äººè§€é»å’Œç¶“é©—'
            },
            'timing_optimization': {
                'suboptimal_timing': 'ç™¼æ–‡æ™‚æ©Ÿä¸ä½³ï¼Œå»ºè­°åœ¨é«˜å³°æ™‚æ®µç™¼æ–‡',
                'off_peak': 'éé«˜å³°æ™‚æ®µç™¼æ–‡ï¼Œå¯èƒ½å½±éŸ¿äº’å‹•æ•ˆæœ'
            },
            'content_quality': {
                'low_readability': 'å¯è®€æ€§è¼ƒä½ï¼Œå»ºè­°ç°¡åŒ–èªè¨€çµæ§‹',
                'low_engagement_potential': 'äº’å‹•æ½›åŠ›ä¸è¶³ï¼Œå»ºè­°å¢åŠ äº’å‹•å…ƒç´ '
            }
        }
    
    def generate_insights(self, interaction_data: InteractionData, performance_analysis: Dict, patterns: Dict, risks: Dict) -> List[LearningInsight]:
        """ç”Ÿæˆå­¸ç¿’æ´å¯Ÿ"""
        insights = []
        
        # åŸºæ–¼é¢¨éšªç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_risk_based_insights(risks, performance_analysis))
        
        # åŸºæ–¼æ¨¡å¼ç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_pattern_based_insights(patterns, performance_analysis))
        
        # åŸºæ–¼è¡¨ç¾è¶¨å‹¢ç”Ÿæˆæ´å¯Ÿ
        insights.extend(self._generate_trend_based_insights(performance_analysis))
        
        return insights
    
    def _generate_risk_based_insights(self, risks: Dict, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼é¢¨éšªç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # AI åµæ¸¬é¢¨éšªæ´å¯Ÿ
        if risks['ai_detection_risk'] > 0.7:
            insights.append(LearningInsight(
                insight_type='ai_detection_risk',
                priority='critical',
                description='AI åµæ¸¬é¢¨éšªè¼ƒé«˜ï¼Œéœ€è¦ç«‹å³èª¿æ•´',
                suggested_actions=[
                    'å¢åŠ å£èªåŒ–è¡¨é”',
                    'æ·»åŠ ä¸å®Œæ•´å¥å­',
                    'ä½¿ç”¨æ›´å¤šè¡¨æƒ…ç¬¦è™Ÿ',
                    'åŠ å…¥å€‹äººåŒ–ç´°ç¯€',
                    'é¿å…éæ–¼æ­£å¼çš„èªè¨€çµæ§‹'
                ],
                confidence=0.9,
                timestamp=datetime.now()
            ))
        
        # äº’å‹•é¢¨éšªæ´å¯Ÿ
        if risks['engagement_risk'] > 0.6:
            insights.append(LearningInsight(
                insight_type='content_optimization',
                priority='high',
                description='äº’å‹•ç‡åä½ï¼Œå»ºè­°å¢åŠ å€‹äººåŒ–å…ƒç´ å’Œæƒ…æ„Ÿè¡¨é”',
                suggested_actions=[
                    'å¢åŠ å€‹äººè§€é»å’Œç¶“é©—åˆ†äº«',
                    'ä½¿ç”¨æ›´å¤šæƒ…æ„Ÿè©å½™',
                    'æ·»åŠ äº’å‹•æ€§å•é¡Œ',
                    'ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢åŠ è¦ªå’ŒåŠ›',
                    'åˆ†äº«å€‹äººæ•…äº‹æˆ–ç¶“æ­·'
                ],
                confidence=0.8,
                timestamp=datetime.now()
            ))
        
        # æ™‚æ©Ÿé¢¨éšªæ´å¯Ÿ
        if risks['timing_risk'] > 0.5:
            insights.append(LearningInsight(
                insight_type='timing_optimization',
                priority='medium',
                description='ç™¼æ–‡æ™‚æ©Ÿä¸ä½³ï¼Œå»ºè­°åœ¨é«˜å³°æ™‚æ®µç™¼æ–‡',
                suggested_actions=[
                    'é¸æ“‡ 9-11 é»æˆ– 19-21 é»ç™¼æ–‡',
                    'é¿é–‹æ·±å¤œå’Œå‡Œæ™¨æ™‚æ®µ',
                    'è€ƒæ…®å—çœ¾çš„æ´»èºæ™‚é–“',
                    'æ¸¬è©¦ä¸åŒæ™‚æ®µçš„äº’å‹•æ•ˆæœ'
                ],
                confidence=0.7,
                timestamp=datetime.now()
            ))
        
        return insights
    
    def _generate_pattern_based_insights(self, patterns: Dict, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼æ¨¡å¼ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # æˆåŠŸæ¨¡å¼æ´å¯Ÿ
        for pattern in patterns['success_patterns']:
            if pattern['type'] == 'high_engagement':
                insights.append(LearningInsight(
                    insight_type='success_pattern',
                    priority='low',
                    description='ç™¼ç¾é«˜äº’å‹•æ¨¡å¼ï¼Œå»ºè­°ä¿æŒä¸¦å„ªåŒ–',
                    suggested_actions=[
                        'ä¿æŒç•¶å‰çš„å…§å®¹é¢¨æ ¼',
                        'é€²ä¸€æ­¥å„ªåŒ–å€‹äººåŒ–è¡¨é”',
                        'åœ¨ç›¸ä¼¼æ™‚æ®µç™¼æ–‡',
                        'æ“´å¤§æˆåŠŸæ¨¡å¼çš„æ‡‰ç”¨'
                    ],
                    confidence=pattern['confidence'],
                    timestamp=datetime.now()
                ))
        
        # å¤±æ•—æ¨¡å¼æ´å¯Ÿ
        for pattern in patterns['failure_patterns']:
            if pattern['type'] == 'low_engagement':
                insights.append(LearningInsight(
                    insight_type='content_optimization',
                    priority='high',
                    description='ç™¼ç¾ä½äº’å‹•æ¨¡å¼ï¼Œéœ€è¦èª¿æ•´ç­–ç•¥',
                    suggested_actions=[
                        'å¢åŠ å…§å®¹çš„è¶£å‘³æ€§',
                        'ä½¿ç”¨æ›´å¤šè¦–è¦ºå…ƒç´ ',
                        'èª¿æ•´å…§å®¹é•·åº¦',
                        'å¢åŠ äº’å‹•æ€§å•é¡Œ'
                    ],
                    confidence=pattern['confidence'],
                    timestamp=datetime.now()
                ))
        
        return insights
    
    def _generate_trend_based_insights(self, analysis: Dict) -> List[LearningInsight]:
        """åŸºæ–¼è¶¨å‹¢ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        trend = analysis.get('performance_trend', {})
        if trend.get('trend') == 'declining':
            insights.append(LearningInsight(
                insight_type='performance_trend',
                priority='high',
                description='è¡¨ç¾å‘ˆä¸‹é™è¶¨å‹¢ï¼Œéœ€è¦ç­–ç•¥èª¿æ•´',
                suggested_actions=[
                    'åˆ†æä¸‹é™åŸå› ',
                    'èª¿æ•´å…§å®¹ç­–ç•¥',
                    'å„ªåŒ–ç™¼æ–‡æ™‚æ©Ÿ',
                    'å¢åŠ èˆ‡å—çœ¾çš„äº’å‹•'
                ],
                confidence=0.7,
                timestamp=datetime.now()
            ))
        
        return insights

class SelfLearningSystem:
    """æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’ç³»çµ±"""
    
    def __init__(self):
        self.real_time_analyzer = RealTimeAnalyzer()
        self.pattern_detector = PatternDetector()
        self.risk_assessor = RiskAssessor()
        self.insight_generator = LearningInsightGenerator()
        self.kol_strategies = {}
        
        logger.info("æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    
    async def process_interaction_data(self, interaction_data: InteractionData) -> Dict[str, Any]:
        """è™•ç†äº’å‹•æ•¸æ“šä¸¦ç”Ÿæˆå­¸ç¿’çµæœ"""
        try:
            logger.info(f"é–‹å§‹è™•ç† {interaction_data.kol_nickname} çš„äº’å‹•æ•¸æ“š")
            
            # 1. å¯¦æ™‚åˆ†æ
            performance_analysis = self.real_time_analyzer.analyze_interaction_performance(interaction_data)
            
            # 2. æ¨¡å¼åµæ¸¬
            patterns = self.pattern_detector.detect_patterns(interaction_data, performance_analysis)
            
            # 3. é¢¨éšªè©•ä¼°
            risks = self.risk_assessor.assess_risks(interaction_data, performance_analysis, patterns)
            
            # 4. ç”Ÿæˆæ´å¯Ÿ
            insights = self.insight_generator.generate_insights(interaction_data, performance_analysis, patterns, risks)
            
            # 5. æ›´æ–°ç­–ç•¥
            strategy_updates = self._update_kol_strategy(interaction_data.kol_id, insights)
            
            # 6. ç”Ÿæˆå ±å‘Š
            learning_report = {
                'kol_id': interaction_data.kol_id,
                'kol_nickname': interaction_data.kol_nickname,
                'article_id': interaction_data.article_id,
                'performance_analysis': performance_analysis,
                'patterns': patterns,
                'risks': risks,
                'insights': [asdict(insight) for insight in insights],
                'strategy_updates': strategy_updates,
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"å®Œæˆ {interaction_data.kol_nickname} çš„å­¸ç¿’åˆ†æ")
            return learning_report
            
        except Exception as e:
            logger.error(f"è™•ç†äº’å‹•æ•¸æ“šå¤±æ•—: {e}")
            return {'error': str(e)}
    
    def _update_kol_strategy(self, kol_id: str, insights: List[LearningInsight]) -> Dict[str, Any]:
        """æ›´æ–° KOL ç­–ç•¥"""
        if kol_id not in self.kol_strategies:
            self.kol_strategies[kol_id] = KOLStrategy(
                kol_id=kol_id,
                content_type_weights={'investment': 0.5, 'personal': 0.3, 'news': 0.2},
                persona_adjustments={'personalization': 0.5, 'emotion': 0.5, 'authenticity': 0.5},
                timing_preferences={'optimal_hours': [9, 10, 11, 19, 20, 21]},
                interaction_style={'casual': 0.5, 'professional': 0.5},
                last_updated=datetime.now()
            )
        
        strategy = self.kol_strategies[kol_id]
        updates = {}
        
        # æ ¹æ“šæ´å¯Ÿèª¿æ•´ç­–ç•¥
        for insight in insights:
            if insight.insight_type == 'content_optimization':
                if insight.priority in ['high', 'critical']:
                    strategy.persona_adjustments['personalization'] += 0.1
                    strategy.persona_adjustments['emotion'] += 0.1
                    updates['personalization_increased'] = True
            
            elif insight.insight_type == 'timing_optimization':
                if insight.priority in ['medium', 'high']:
                    # èª¿æ•´æ™‚æ©Ÿåå¥½
                    updates['timing_adjusted'] = True
            
            elif insight.insight_type == 'ai_detection_risk':
                if insight.priority == 'critical':
                    strategy.persona_adjustments['authenticity'] += 0.2
                    strategy.interaction_style['casual'] += 0.2
                    updates['authenticity_increased'] = True
        
        strategy.last_updated = datetime.now()
        return updates

async def main():
    """ä¸»å‡½æ•¸ - æ¸¬è©¦è‡ªæˆ‘å­¸ç¿’ç³»çµ±"""
    print("ğŸ§  æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶åŸå‹æ¸¬è©¦")
    print("=" * 60)
    
    # åˆå§‹åŒ–å­¸ç¿’ç³»çµ±
    learning_system = SelfLearningSystem()
    
    # æ¨¡æ“¬äº’å‹•æ•¸æ“š
    test_data = [
        InteractionData(
            article_id='173477844',
            kol_id='9505549',
            kol_nickname='é¾œç‹—ä¸€æ—¥æ•£æˆ¶',
            likes=8,
            comments=1,
            shares=1,
            emoji_total=8,
            total_interactions=18,
            engagement_rate=0.018,
            post_timestamp='2025-09-02T17:41:09.295405',
            content='è²¼æ–‡å…§å®¹...',
            topic_id='2025-09-02 17:41:13'
        ),
        InteractionData(
            article_id='173477845',
            kol_id='9505550',
            kol_nickname='æ¿æ©‹å¤§who',
            likes=9,
            comments=0,
            shares=0,
            emoji_total=9,
            total_interactions=18,
            engagement_rate=0.018,
            post_timestamp='2025-09-02T17:41:09.704825',
            content='è²¼æ–‡å…§å®¹...',
            topic_id='2025-09-02 17:41:13'
        )
    ]
    
    # è™•ç†æ¯å€‹äº’å‹•æ•¸æ“š
    for i, data in enumerate(test_data, 1):
        print(f"\nğŸ“Š è™•ç†ç¬¬ {i} å€‹äº’å‹•æ•¸æ“š:")
        print(f"   KOL: {data.kol_nickname}")
        print(f"   Article ID: {data.article_id}")
        print(f"   äº’å‹•æ•¸: {data.total_interactions}")
        
        # åŸ·è¡Œå­¸ç¿’åˆ†æ
        learning_report = await learning_system.process_interaction_data(data)
        
        if 'error' not in learning_report:
            # é¡¯ç¤ºåˆ†æçµæœ
            print(f"\nğŸ“ˆ åˆ†æçµæœ:")
            print(f"   äº’å‹•åˆ†æ•¸: {learning_report['performance_analysis']['engagement_score']:.1f}/100")
            print(f"   AI åµæ¸¬é¢¨éšª: {learning_report['risks']['ai_detection_risk']:.2f}")
            print(f"   æ•´é«”é¢¨éšª: {learning_report['risks']['overall_risk']:.2f}")
            
            # é¡¯ç¤ºæ´å¯Ÿ
            insights = learning_report['insights']
            if insights:
                print(f"\nğŸ’¡ å­¸ç¿’æ´å¯Ÿ ({len(insights)} å€‹):")
                for j, insight in enumerate(insights, 1):
                    print(f"   {j}. [{insight['priority'].upper()}] {insight['description']}")
                    for action in insight['suggested_actions'][:2]:  # åªé¡¯ç¤ºå‰2å€‹å»ºè­°
                        print(f"      - {action}")
            else:
                print(f"\nâœ… æ²’æœ‰ç™¼ç¾éœ€è¦èª¿æ•´çš„å•é¡Œ")
            
            # é¡¯ç¤ºç­–ç•¥æ›´æ–°
            if learning_report['strategy_updates']:
                print(f"\nğŸ”„ ç­–ç•¥æ›´æ–°:")
                for update, value in learning_report['strategy_updates'].items():
                    print(f"   - {update}: {value}")
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {learning_report['error']}")
    
    print(f"\n" + "=" * 60)
    print("âœ… æ™ºèƒ½è‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶åŸå‹æ¸¬è©¦å®Œæˆ")
    print("ğŸ’¡ ç³»çµ±èƒ½å¤ :")
    print("   - å¯¦æ™‚åˆ†æäº’å‹•è¡¨ç¾")
    print("   - åµæ¸¬æˆåŠŸå’Œå¤±æ•—æ¨¡å¼")
    print("   - è©•ä¼°å„ç¨®é¢¨éšª")
    print("   - ç”Ÿæˆå…·é«”æ”¹é€²å»ºè­°")
    print("   - è‡ªå‹•èª¿æ•´ KOL ç­–ç•¥")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())



