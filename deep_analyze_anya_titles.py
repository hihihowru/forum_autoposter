#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æ anya TSV æ–‡ä»¶ - æ›´å…¨é¢çš„ UGC æ¨™é¡Œåˆ†æ
"""

import re
import pandas as pd
from collections import Counter, defaultdict
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

def deep_analyze_titles():
    """æ·±å…¥åˆ†ææ¨™é¡Œæ¨¡å¼"""
    
    # è®€å– TSV æ–‡ä»¶
    print("ğŸ“– æ­£åœ¨è®€å–æ•¸æ“š...")
    df = pd.read_csv('anya-forumteam-1756973422036-23159383983751583.tsv', sep='\t')
    
    # æ¸…ç†æ•¸æ“š
    titles = df['title'].dropna().tolist()
    titles = [title for title in titles if title and title != 'title' and title != '""']
    
    print(f"ğŸ“Š ç¸½å…±åˆ†æ {len(titles)} å€‹æ¨™é¡Œ")
    print("="*80)
    
    # 1. æ¨™é¡Œé•·åº¦æ·±åº¦åˆ†æ
    title_lengths = [len(title) for title in titles]
    print("ğŸ“ æ¨™é¡Œé•·åº¦æ·±åº¦åˆ†æ:")
    print(f"   å¹³å‡é•·åº¦: {sum(title_lengths)/len(title_lengths):.1f} å­—")
    print(f"   ä¸­ä½æ•¸: {sorted(title_lengths)[len(title_lengths)//2]} å­—")
    print(f"   æœ€çŸ­: {min(title_lengths)} å­—")
    print(f"   æœ€é•·: {max(title_lengths)} å­—")
    
    # é•·åº¦åˆ†å¸ƒ
    short_titles = [t for t in titles if len(t) <= 10]
    medium_titles = [t for t in titles if 10 < len(t) <= 20]
    long_titles = [t for t in titles if len(t) > 20]
    
    print(f"   çŸ­æ¨™é¡Œ (â‰¤10å­—): {len(short_titles)} å€‹ ({len(short_titles)/len(titles)*100:.1f}%)")
    print(f"   ä¸­æ¨™é¡Œ (11-20å­—): {len(medium_titles)} å€‹ ({len(medium_titles)/len(titles)*100:.1f}%)")
    print(f"   é•·æ¨™é¡Œ (>20å­—): {len(long_titles)} å€‹ ({len(long_titles)/len(titles)*100:.1f}%)")
    print()
    
    # 2. è¡¨æƒ…ç¬¦è™Ÿè©³ç´°åˆ†æ
    emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ï¸â¤ï¸ğŸ’ğŸ’°ğŸ’µğŸ’¸ğŸ‰ğŸŠğŸ¯ğŸªğŸ­ğŸ¨ğŸ¬ğŸ¤ğŸ§ğŸµğŸ¶ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ†ğŸˆğŸ‰ğŸŠğŸ‹ï¸ğŸŒï¸ğŸï¸ğŸï¸ğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ï¸ğŸ•ï¸ğŸ–ï¸ğŸ—ï¸ğŸ˜ï¸ğŸ™ï¸ğŸšï¸ğŸ›ï¸ğŸœï¸ğŸï¸ğŸï¸ğŸŸï¸ğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ï¸ğŸ´ï¸ğŸµï¸ğŸ¶ğŸ·ï¸ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ…ğŸ†ğŸ‡ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸğŸğŸğŸ‘ğŸ’ğŸ“ğŸ”ğŸ•ğŸ–ğŸ—ğŸ˜ğŸ™ğŸšğŸ›ğŸœğŸğŸğŸŸğŸ ğŸ¡ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ§ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ³ğŸ´ğŸµğŸ¶ğŸ·ğŸ¸ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ï¸ğŸ‘€ğŸ‘ï¸ğŸ‘‚ğŸ‘ƒğŸ‘„ğŸ‘…ğŸ‘†ğŸ‘‡ğŸ‘ˆğŸ‘‰ğŸ‘ŠğŸ‘‹ğŸ‘ŒğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘‘ğŸ‘’ğŸ‘“ğŸ‘”ğŸ‘•ğŸ‘–ğŸ‘—ğŸ‘˜ğŸ‘™ğŸ‘šğŸ‘›ğŸ‘œğŸ‘ğŸ‘ğŸ‘ŸğŸ‘ ğŸ‘¡ğŸ‘¢ğŸ‘£ğŸ‘¤ğŸ‘¥ğŸ‘¦ğŸ‘§ğŸ‘¨ğŸ‘©ğŸ‘ªğŸ‘«ğŸ‘¬ğŸ‘­ğŸ‘®ğŸ‘¯ğŸ‘°ğŸ‘±ğŸ‘²ğŸ‘³ğŸ‘´ğŸ‘µğŸ‘¶ğŸ‘·ğŸ‘¸ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ‘¼ğŸ‘½ğŸ‘¾ğŸ‘¿ğŸ’€ğŸ’ğŸ’‚ğŸ’ƒğŸ’„ğŸ’…ğŸ’†ğŸ’‡ğŸ’ˆğŸ’‰ğŸ’ŠğŸ’‹ğŸ’ŒğŸ’ğŸ’ğŸ’ğŸ’ğŸ’‘ğŸ’’ğŸ’“ğŸ’”ğŸ’•ğŸ’–ğŸ’—ğŸ’˜ğŸ’™ğŸ’šğŸ’›ğŸ’œğŸ’ğŸ’ğŸ’ŸğŸ’ ğŸ’¡ğŸ’¢ğŸ’£ğŸ’¤ğŸ’¥ğŸ’¦ğŸ’§ğŸ’¨ğŸ’©ğŸ’ªğŸ’«ğŸ’¬ğŸ’­ğŸ’®ğŸ’¯ğŸ’°ğŸ’±ğŸ’²ğŸ’³ğŸ’´ğŸ’µğŸ’¶ğŸ’·ğŸ’¸ğŸ’¹ğŸ’ºğŸ’»ğŸ’¼ğŸ’½ğŸ’¾ğŸ’¿ğŸ“€ğŸ“ğŸ“‚ğŸ“ƒğŸ“„ğŸ“…ğŸ“†ğŸ“‡ğŸ“ˆğŸ“‰ğŸ“ŠğŸ“‹ğŸ“ŒğŸ“ğŸ“ğŸ“ğŸ“ğŸ“‘ğŸ“’ğŸ““ğŸ“”ğŸ“•ğŸ“–ğŸ“—ğŸ“˜ğŸ“™ğŸ“šğŸ“›ğŸ“œğŸ“ğŸ“ğŸ“ŸğŸ“ ğŸ“¡ğŸ“¢ğŸ“£ğŸ“¤ğŸ“¥ğŸ“¦ğŸ“§ğŸ“¨ğŸ“©ğŸ“ªğŸ“«ğŸ“¬ğŸ“­ğŸ“®ğŸ“¯ğŸ“°ğŸ“±ğŸ“²ğŸ“³ğŸ“´ğŸ“µğŸ“¶ğŸ“·ğŸ“¸ğŸ“¹ğŸ“ºğŸ“»ğŸ“¼ğŸ“½ï¸ğŸ“¾ğŸ“¿ğŸ”€ğŸ”ğŸ”‚ğŸ”ƒğŸ”„ğŸ”…ğŸ”†ğŸ”‡ğŸ”ˆğŸ”‰ğŸ”ŠğŸ”‹ğŸ”ŒğŸ”ğŸ”ğŸ”ğŸ”ğŸ”‘ğŸ”’ğŸ”“ğŸ””ğŸ”•ğŸ”–ğŸ”—ğŸ”˜ğŸ”™ğŸ”šğŸ”›ğŸ”œğŸ”ğŸ”ğŸ”ŸğŸ” ğŸ”¡ğŸ”¢ğŸ”£ğŸ”¤ğŸ”¥ğŸ”¦ğŸ”§ğŸ”¨ğŸ”©ğŸ”ªğŸ”«ğŸ”¬ğŸ”­ğŸ”®ğŸ”¯ğŸ”°ğŸ”±ğŸ”²ğŸ”³ğŸ”´ğŸ”µğŸ”¶ğŸ”·ğŸ”¸ğŸ”¹ğŸ”ºğŸ”»ğŸ”¼ğŸ”½ğŸ”¾ğŸ”¿ğŸ•€ğŸ•ğŸ•‚ğŸ•ƒğŸ•„ğŸ•…ğŸ•†ğŸ•‡ğŸ•ˆğŸ•‰ğŸ•ŠğŸ•‹ğŸ•ŒğŸ•ğŸ•ğŸ•ğŸ•ğŸ•‘ğŸ•’ğŸ•“ğŸ•”ğŸ••ğŸ•–ğŸ•—ğŸ•˜ğŸ•™ğŸ•šğŸ•›ğŸ•œğŸ•ğŸ•ğŸ•ŸğŸ• ğŸ•¡ğŸ•¢ğŸ•£ğŸ•¤ğŸ•¥ğŸ•¦ğŸ•§ğŸ•¨ğŸ•©ğŸ•ªğŸ•«ğŸ•¬ğŸ•­ğŸ•®ğŸ•¯ğŸ•°ï¸ğŸ•±ğŸ•²ğŸ•³ğŸ•´ï¸ğŸ•µï¸ğŸ•¶ï¸ğŸ•·ï¸ğŸ•¸ï¸ğŸ•¹ï¸ğŸ•ºğŸ•»ğŸ•¼ğŸ•½ğŸ•¾ğŸ•¿ğŸ–€ğŸ–ğŸ–‚ğŸ–ƒğŸ–„ğŸ–…ğŸ–†ğŸ–‡ï¸ğŸ–ˆğŸ–‰ğŸ–Šï¸ğŸ–‹ï¸ğŸ–Œï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–ï¸ğŸ–‘ğŸ–’ğŸ–“ğŸ–”ï¸ğŸ–•ğŸ––ğŸ–—ğŸ–˜ğŸ–™ğŸ–šğŸ–›ğŸ–œğŸ–ğŸ–ğŸ–ŸğŸ– ğŸ–¡ğŸ–¢ğŸ–£ğŸ–¤ğŸ–¥ï¸ğŸ–¦ğŸ–§ğŸ–¨ï¸ğŸ–©ğŸ–ªğŸ–«ğŸ–¬ğŸ–­ğŸ–®ğŸ–¯ğŸ–°ï¸ğŸ–±ï¸ğŸ–²ï¸ğŸ–³ï¸ğŸ–´ï¸ğŸ–µï¸ğŸ–¶ğŸ–·ï¸ğŸ–¸ğŸ–¹ğŸ–ºğŸ–»ğŸ–¼ï¸ğŸ–½ğŸ–¾ğŸ–¿ğŸ—€ğŸ—ğŸ—‚ï¸ğŸ—ƒï¸ğŸ—„ï¸ğŸ—…ğŸ—†ğŸ—‡ğŸ—ˆğŸ—‰ğŸ—ŠğŸ—‹ğŸ—ŒğŸ—ğŸ—ğŸ—ğŸ—ğŸ—‘ğŸ—’ğŸ—“ï¸ğŸ—”ï¸ğŸ—•ï¸ğŸ—–ï¸ğŸ——ğŸ—˜ğŸ—™ğŸ—šğŸ—›ğŸ—œğŸ—ğŸ—ï¸ğŸ—Ÿï¸ğŸ— ï¸ğŸ—¡ï¸ğŸ—¢ï¸ğŸ—£ï¸ğŸ—¤ï¸ğŸ—¥ï¸ğŸ—¦ï¸ğŸ—§ï¸ğŸ—¨ï¸ğŸ—©ï¸ğŸ—ªï¸ğŸ—«ï¸ğŸ—¬ï¸ğŸ—­ï¸ğŸ—®ï¸ğŸ—¯ï¸ğŸ—°ï¸ğŸ—±ï¸ğŸ—²ï¸ğŸ—³ï¸ğŸ—´ï¸ğŸ—µï¸ğŸ—¶ï¸ğŸ—·ï¸ğŸ—¸ï¸ğŸ—¹ï¸ğŸ—ºï¸ğŸ—»ï¸ğŸ—¼ï¸ğŸ—½ï¸ğŸ—¾ï¸ğŸ—¿ï¸]')
    
    emoji_titles = [title for title in titles if emoji_pattern.search(title)]
    emoji_counts = Counter()
    for title in emoji_titles:
        emojis = emoji_pattern.findall(title)
        emoji_counts.update(emojis)
    
    print("ğŸ˜Š è¡¨æƒ…ç¬¦è™Ÿè©³ç´°åˆ†æ:")
    print(f"   ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿçš„æ¨™é¡Œ: {len(emoji_titles)} å€‹ ({len(emoji_titles)/len(titles)*100:.1f}%)")
    print("   æœ€å¸¸è¦‹è¡¨æƒ…ç¬¦è™Ÿ:")
    for emoji, count in emoji_counts.most_common(10):
        print(f"     {emoji}: {count} æ¬¡")
    print()
    
    # 3. è‚¡ç¥¨ä»£è™Ÿåˆ†æ
    stock_code_pattern = re.compile(r'\b\d{4}\b')
    stock_titles = [title for title in titles if stock_code_pattern.search(title)]
    stock_codes = []
    for title in stock_titles:
        codes = stock_code_pattern.findall(title)
        stock_codes.extend(codes)
    
    stock_code_counts = Counter(stock_codes)
    print("ğŸ“ˆ è‚¡ç¥¨ä»£è™Ÿåˆ†æ:")
    print(f"   åŒ…å«è‚¡ç¥¨ä»£è™Ÿçš„æ¨™é¡Œ: {len(stock_titles)} å€‹ ({len(stock_titles)/len(titles)*100:.1f}%)")
    print("   æœ€å¸¸æåˆ°çš„è‚¡ç¥¨ä»£è™Ÿ:")
    for code, count in stock_code_counts.most_common(10):
        print(f"     {code}: {count} æ¬¡")
    print()
    
    # 4. æ™‚é–“ç›¸é—œåˆ†æ
    time_patterns = {
        'æ—¥æœŸ': re.compile(r'\d{1,2}/\d{1,2}'),
        'æœˆä»½': re.compile(r'\d{1,2}æœˆ'),
        'å¹´ä»½': re.compile(r'20\d{2}'),
        'æ™‚é–“': re.compile(r'\d{1,2}:\d{2}')
    }
    
    time_titles = defaultdict(list)
    for title in titles:
        for time_type, pattern in time_patterns.items():
            if pattern.search(title):
                time_titles[time_type].append(title)
    
    print("â° æ™‚é–“ç›¸é—œåˆ†æ:")
    for time_type, titles_list in time_titles.items():
        print(f"   {time_type}: {len(titles_list)} å€‹ ({len(titles_list)/len(titles)*100:.1f}%)")
    print()
    
    # 5. æ•¸å­—åˆ†æ
    number_patterns = {
        'ç™¾åˆ†æ¯”': re.compile(r'\d+%'),
        'åƒ¹æ ¼': re.compile(r'\d+\.?\d*å…ƒ'),
        'å¼µæ•¸': re.compile(r'\d+å¼µ'),
        'è¬': re.compile(r'\d+è¬'),
        'å„„': re.compile(r'\d+å„„'),
        'é»æ•¸': re.compile(r'\d+é»')
    }
    
    number_titles = defaultdict(list)
    for title in titles:
        for number_type, pattern in number_patterns.items():
            if pattern.search(title):
                number_titles[number_type].append(title)
    
    print("ğŸ”¢ æ•¸å­—é¡å‹åˆ†æ:")
    for number_type, titles_list in number_titles.items():
        print(f"   {number_type}: {len(titles_list)} å€‹ ({len(titles_list)/len(titles)*100:.1f}%)")
    print()
    
    # 6. ç†±é–€è©±é¡Œåˆ†æ
    hot_topics = {
        'AI': ['AI', 'äººå·¥æ™ºæ…§', 'æ™ºæ…§'],
        'åŠå°é«”': ['åŠå°é«”', 'æ™¶ç‰‡', 'IC', 'æ™¶åœ“'],
        'èˆªé‹': ['èˆªé‹', 'è²¨æ«ƒ', 'æµ·é‹'],
        'ç”ŸæŠ€': ['ç”ŸæŠ€', 'é†«è—¥', 'è—¥å“'],
        'é‡‘è': ['é‡‘è', 'éŠ€è¡Œ', 'ä¿éšª'],
        'é›»å‹•è»Š': ['é›»å‹•è»Š', 'ç‰¹æ–¯æ‹‰', 'é›»è»Š'],
        'å…ƒå®‡å®™': ['å…ƒå®‡å®™', 'VR', 'AR'],
        '5G': ['5G', 'é€šè¨Š'],
        'ç¶ èƒ½': ['ç¶ èƒ½', 'å¤ªé™½èƒ½', 'é¢¨é›»', 'ç’°ä¿']
    }
    
    topic_counts = defaultdict(int)
    for title in titles:
        for topic, keywords in hot_topics.items():
            if any(keyword in title for keyword in keywords):
                topic_counts[topic] += 1
    
    print("ğŸ”¥ ç†±é–€è©±é¡Œåˆ†æ:")
    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"   {topic}: {count} å€‹ ({count/len(titles)*100:.1f}%)")
    print()
    
    # 7. æƒ…æ„Ÿå¼·åº¦åˆ†æ
    emotion_intensity = {
        'æ¥µåº¦æ­£é¢': ['å¤ªæ£’äº†', 'è¶…è®š', 'ç¥äº†', 'å®Œç¾', 'ç„¡æ•µ'],
        'æ­£é¢': ['å¥½æ£’', 'èˆ’æœ', 'é–‹å¿ƒ', 'æœŸå¾…', 'çœ‹å¥½'],
        'ä¸­æ€§': ['æ³¨æ„', 'é—œæ³¨', 'æé†’', 'å»ºè­°', 'åˆ†æ'],
        'è² é¢': ['çˆ›', 'æ…˜', 'æ­»', 'é¨™', 'ç‹ '],
        'æ¥µåº¦è² é¢': ['çˆ›é€äº†', 'æ…˜å…®å…®', 'æ²’æ•‘', 'å®Œè›‹', 'å´©æ½°']
    }
    
    intensity_counts = defaultdict(int)
    for title in titles:
        for intensity, words in emotion_intensity.items():
            if any(word in title for word in words):
                intensity_counts[intensity] += 1
    
    print("ğŸ˜Š æƒ…æ„Ÿå¼·åº¦åˆ†æ:")
    for intensity, count in intensity_counts.items():
        print(f"   {intensity}: {count} å€‹ ({count/len(titles)*100:.1f}%)")
    print()
    
    # 8. äº’å‹•æ€§åˆ†æ
    interaction_patterns = {
        'å•å¥': ['?', 'ï¼Ÿ', 'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'è©²', 'å—'],
        'æ„Ÿå˜†': ['!', 'ï¼', 'å•Š', 'å“‡', 'å–”'],
        'æŒ‡ä»¤': ['è«‹', 'æ³¨æ„', 'æé†’', 'å»ºè­°', 'å¿«'],
        'é‚€è«‹': ['ä¸€èµ·', 'å¤§å®¶', 'æˆ‘å€‘', 'ä¾†', 'ä¾†å§']
    }
    
    interaction_counts = defaultdict(int)
    for title in titles:
        for pattern_type, keywords in interaction_patterns.items():
            if any(keyword in title for keyword in keywords):
                interaction_counts[pattern_type] += 1
    
    print("ğŸ’¬ äº’å‹•æ€§åˆ†æ:")
    for pattern_type, count in interaction_counts.items():
        print(f"   {pattern_type}: {count} å€‹ ({count/len(titles)*100:.1f}%)")
    print()
    
    # 9. å°ˆæ¥­åº¦åˆ†æ
    professional_terms = {
        'æŠ€è¡“åˆ†æ': ['çªç ´', 'æ”¯æ’', 'å£“åŠ›', 'å‡ç·š', 'Kç·š', 'è¶¨å‹¢', 'æ´—ç›¤', 'åšé ­', 'èƒŒé›¢'],
        'åŸºæœ¬é¢': ['ç‡Ÿæ”¶', 'è²¡å ±', 'EPS', 'ç²åˆ©', 'æ¥­ç¸¾', 'è‚¡åˆ©', 'é…æ¯'],
        'ç±Œç¢¼é¢': ['å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'ä¸»åŠ›', 'æ•£æˆ¶', 'èè³‡', 'èåˆ¸'],
        'æ¶ˆæ¯é¢': ['æ–°è', 'å ±å°', 'å…¬å‘Š', 'åˆä½œ', 'æ“´ç”¢', 'å¸ƒå±€', 'é¡Œæ']
    }
    
    professional_counts = defaultdict(int)
    for title in titles:
        for term_type, keywords in professional_terms.items():
            if any(keyword in title for keyword in keywords):
                professional_counts[term_type] += 1
    
    print("ğŸ“Š å°ˆæ¥­åº¦åˆ†æ:")
    for term_type, count in professional_counts.items():
        print(f"   {term_type}: {count} å€‹ ({count/len(titles)*100:.1f}%)")
    print()
    
    # 10. ç¸½çµå»ºè­°
    print("ğŸ’¡ å° AI æ¨™é¡Œç”Ÿæˆçš„æ·±åº¦å»ºè­°:")
    print("   1. æ¨™é¡Œé•·åº¦æ‡‰è©²å¤šæ¨£åŒ–ï¼Œé‡é»é—œæ³¨ 11-20 å­—çš„ä¸­ç­‰é•·åº¦")
    print("   2. é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼Œç‰¹åˆ¥æ˜¯ ğŸ˜±ğŸ‘ğŸ˜Œâ¤ï¸ ç­‰å¸¸è¦‹è¡¨æƒ…")
    print("   3. å¤§é‡ä½¿ç”¨è‚¡ç¥¨ä»£è™Ÿï¼Œå¢åŠ å…·é«”æ€§å’Œå¯ä¿¡åº¦")
    print("   4. èå…¥æ™‚é–“å…ƒç´ ï¼Œå¢åŠ æ™‚æ•ˆæ€§")
    print("   5. ä½¿ç”¨å…·é«”æ•¸å­—ï¼Œå¦‚åƒ¹æ ¼ã€ç™¾åˆ†æ¯”ã€å¼µæ•¸ç­‰")
    print("   6. é—œæ³¨ç†±é–€è©±é¡Œï¼Œç‰¹åˆ¥æ˜¯ AIã€åŠå°é«”ã€èˆªé‹ç­‰")
    print("   7. å¹³è¡¡æƒ…æ„Ÿå¼·åº¦ï¼Œé¿å…éåº¦æ¥µç«¯")
    print("   8. å¢åŠ äº’å‹•æ€§ï¼Œä½¿ç”¨å•å¥ã€æ„Ÿå˜†å¥ã€æŒ‡ä»¤å¥")
    print("   9. èå…¥å°ˆæ¥­è¡“èªï¼Œæå‡å¯ä¿¡åº¦")
    print("   10. ä½¿ç”¨ç¶²è·¯ç”¨èªï¼Œå¢åŠ è¦ªè¿‘æ„Ÿ")
    
    # 11. ç”Ÿæˆç¤ºä¾‹æ¨™é¡Œ
    print("\nğŸ¯ åŸºæ–¼åˆ†æçš„æ¨™é¡Œç¤ºä¾‹:")
    print("   å•å¥é¡: 'å°ç©é›»(2330)ä»Šå¤©æ€éº¼äº†ï¼Ÿ'")
    print("   æ„Ÿå˜†é¡: 'AIæ¦‚å¿µè‚¡å¤ªçŒ›äº†ï¼ğŸ˜±'")
    print("   æŒ‡ä»¤é¡: 'æ³¨æ„ï¼èˆªé‹è‚¡è¦èµ·é£›äº†'")
    print("   æ•¸æ“šé¡: 'ç‡Ÿæ”¶æˆé•·50%ï¼Œè‚¡åƒ¹é£†å‡ï¼'")
    print("   å°ˆæ¥­é¡: 'æŠ€è¡“é¢çªç ´ï¼Œç±Œç¢¼é¢è½‰å¥½'")
    print("   äº’å‹•é¡: 'å¤§å®¶ä¸€èµ·ä¾†è¨è«–å§ï¼'")

if __name__ == "__main__":
    deep_analyze_titles()
