#!/usr/bin/env python3
"""
ç¬¬äºŒéšæ®µï¼šç™¼æ–‡å¾Œæ•¸æ“šæ”¶é›†å’Œè‡ªæˆ‘å­¸ç¿’æ©Ÿåˆ¶
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any
from src.core.main_workflow_engine import MainWorkflowEngine
from src.operations.post_processing_manager import PostProcessingManager, PostResult

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Stage2Processor:
    """ç¬¬äºŒéšæ®µè™•ç†å™¨ï¼šç™¼æ–‡å¾Œæ•¸æ“šæ”¶é›†å’Œè‡ªæˆ‘å­¸ç¿’"""
    
    def __init__(self):
        self.workflow_engine = MainWorkflowEngine()
        self.post_processor = PostProcessingManager(self.workflow_engine)
        
    async def process_published_posts(self, post_results: List[PostResult]):
        """è™•ç†å·²ç™¼å¸ƒçš„è²¼æ–‡"""
        logger.info("ğŸš€ é–‹å§‹ç¬¬äºŒéšæ®µï¼šç™¼æ–‡å¾Œæ•¸æ“šæ”¶é›†")
        
        for post_result in post_results:
            logger.info(f"ğŸ“Š è™•ç†è²¼æ–‡: {post_result.post_id}")
            
            # 1. æ›´æ–°è²¼æ–‡ç‹€æ…‹
            await self.post_processor.process_post_result(post_result)
            
            # 2. æ”¶é›†äº’å‹•æ•¸æ“š
            if post_result.success:
                await self.collect_interaction_data(post_result)
            
            # 3. æ›´æ–° Google Sheets
            await self.update_sheets_with_results(post_result)
        
        # 4. åŸ·è¡Œå­¸ç¿’æ©Ÿåˆ¶
        await self.execute_learning_cycle()
        
        # 5. ç”Ÿæˆå ±å‘Š
        await self.generate_performance_report()
        
        logger.info("âœ… ç¬¬äºŒéšæ®µè™•ç†å®Œæˆ")
    
    async def collect_interaction_data(self, post_result: PostResult):
        """æ”¶é›†äº’å‹•æ•¸æ“š"""
        try:
            logger.info(f"ğŸ“ˆ æ”¶é›† {post_result.kol_serial} çš„äº’å‹•æ•¸æ“š...")
            
            # æ¨¡æ“¬æ”¶é›†äº’å‹•æ•¸æ“šï¼ˆå¯¦éš›æ‡‰è©²èª¿ç”¨ APIï¼‰
            interaction_data = {
                'likes': 150,
                'comments': 25,
                'shares': 8,
                'views': 1200,
                'engagement_rate': 0.15,
                'sentiment_score': 0.8,
                'collection_time': datetime.now().isoformat()
            }
            
            # ä¿å­˜åˆ°æœ¬åœ°
            await self.save_interaction_data(post_result, interaction_data)
            
            logger.info(f"âœ… äº’å‹•æ•¸æ“šæ”¶é›†å®Œæˆ: {interaction_data['engagement_rate']:.2%} äº’å‹•ç‡")
            
        except Exception as e:
            logger.error(f"âŒ æ”¶é›†äº’å‹•æ•¸æ“šå¤±æ•—: {e}")
    
    async def save_interaction_data(self, post_result: PostResult, interaction_data: Dict[str, Any]):
        """ä¿å­˜äº’å‹•æ•¸æ“š"""
        try:
            import json
            import os
            
            # å‰µå»ºäº’å‹•æ•¸æ“šç›®éŒ„
            interaction_dir = "data/interactions"
            os.makedirs(interaction_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"interaction_{post_result.post_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(interaction_dir, filename)
            
            # ä¿å­˜æ•¸æ“š
            data = {
                'post_id': post_result.post_id,
                'kol_serial': post_result.kol_serial,
                'article_id': post_result.article_id,
                'platform_url': post_result.platform_url,
                'publish_time': post_result.publish_time.isoformat(),
                'interaction_data': interaction_data
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ äº’å‹•æ•¸æ“šå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜äº’å‹•æ•¸æ“šå¤±æ•—: {e}")
    
    async def update_sheets_with_results(self, post_result: PostResult):
        """æ›´æ–° Google Sheets ä¸­çš„ç™¼æ–‡çµæœ"""
        try:
            logger.info(f"ğŸ“‹ æ›´æ–° Google Sheets: {post_result.post_id}")
            
            # æ›´æ–°ç‹€æ…‹ç‚ºå·²ç™¼å¸ƒ
            update_data = {
                'status': 'published',
                'publish_time': post_result.publish_time.isoformat(),
                'platform_post_id': post_result.article_id,
                'platform_post_url': post_result.platform_url,
                'articleid': post_result.article_id
            }
            
            # é€™è£¡æ‡‰è©²èª¿ç”¨ Google Sheets API æ›´æ–°å°æ‡‰è¡Œ
            # æš«æ™‚è¨˜éŒ„åˆ°æœ¬åœ°
            await self.save_sheets_update(post_result.post_id, update_data)
            
            logger.info(f"âœ… Google Sheets æ›´æ–°å®Œæˆ: {post_result.article_id}")
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–° Google Sheets å¤±æ•—: {e}")
    
    async def save_sheets_update(self, post_id: str, update_data: Dict[str, Any]):
        """ä¿å­˜ Google Sheets æ›´æ–°è¨˜éŒ„"""
        try:
            import json
            import os
            
            # å‰µå»ºæ›´æ–°è¨˜éŒ„ç›®éŒ„
            update_dir = "data/sheets_updates"
            os.makedirs(update_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"sheets_update_{post_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(update_dir, filename)
            
            # ä¿å­˜æ•¸æ“š
            data = {
                'post_id': post_id,
                'update_time': datetime.now().isoformat(),
                'update_data': update_data
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ Sheets æ›´æ–°è¨˜éŒ„å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ Sheets æ›´æ–°è¨˜éŒ„å¤±æ•—: {e}")
    
    async def execute_learning_cycle(self):
        """åŸ·è¡Œå­¸ç¿’æ©Ÿåˆ¶"""
        try:
            logger.info("ğŸ§  é–‹å§‹åŸ·è¡Œå­¸ç¿’æ©Ÿåˆ¶...")
            
            # 1. åˆ†æäº’å‹•æ•¸æ“š
            await self.analyze_interaction_patterns()
            
            # 2. è©•ä¼°å…§å®¹æ•ˆæœ
            await self.evaluate_content_performance()
            
            # 3. å„ªåŒ– KOL è¨­å®š
            await self.optimize_kol_settings()
            
            # 4. æ›´æ–°å­¸ç¿’æ¨¡å‹
            await self.update_learning_model()
            
            logger.info("âœ… å­¸ç¿’æ©Ÿåˆ¶åŸ·è¡Œå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å­¸ç¿’æ©Ÿåˆ¶åŸ·è¡Œå¤±æ•—: {e}")
    
    async def analyze_interaction_patterns(self):
        """åˆ†æäº’å‹•æ¨¡å¼"""
        logger.info("ğŸ“Š åˆ†æäº’å‹•æ¨¡å¼...")
        
        # æ¨¡æ“¬åˆ†æçµæœ
        patterns = {
            'best_performing_content_types': ['ç‡Ÿæ”¶åˆ†æ', 'æŠ€è¡“åˆ†æ'],
            'optimal_posting_times': ['09:30', '13:30', '15:00'],
            'high_engagement_keywords': ['æ¼²åœ', 'çªç ´', 'äº®çœ¼'],
            'kol_performance_ranking': ['æƒ…ç·’æ´¾', 'æŠ€è¡“æ´¾', 'åˆ†ææ´¾']
        }
        
        logger.info(f"ğŸ“ˆ æœ€ä½³è¡¨ç¾å…§å®¹é¡å‹: {patterns['best_performing_content_types']}")
        logger.info(f"â° æœ€ä½³ç™¼æ–‡æ™‚é–“: {patterns['optimal_posting_times']}")
        logger.info(f"ğŸ”‘ é«˜äº’å‹•é—œéµè©: {patterns['high_engagement_keywords']}")
        logger.info(f"ğŸ† KOL è¡¨ç¾æ’å: {patterns['kol_performance_ranking']}")
    
    async def evaluate_content_performance(self):
        """è©•ä¼°å…§å®¹æ•ˆæœ"""
        logger.info("ğŸ“ è©•ä¼°å…§å®¹æ•ˆæœ...")
        
        # æ¨¡æ“¬è©•ä¼°çµæœ
        performance = {
            'average_engagement_rate': 0.12,
            'content_quality_score': 8.5,
            'personalization_effectiveness': 0.85,
            'recommendations': [
                'å¢åŠ æ›´å¤šæƒ…ç·’åŒ–è¡¨é”',
                'æ¸›å°‘æŠ€è¡“è¡“èªä½¿ç”¨',
                'åŠ å¼·å€‹äººåŒ–å…ƒç´ '
            ]
        }
        
        logger.info(f"ğŸ“Š å¹³å‡äº’å‹•ç‡: {performance['average_engagement_rate']:.2%}")
        logger.info(f"â­ å…§å®¹å“è³ªåˆ†æ•¸: {performance['content_quality_score']}/10")
        logger.info(f"ğŸ¯ å€‹äººåŒ–æ•ˆæœ: {performance['personalization_effectiveness']:.2%}")
        logger.info(f"ğŸ’¡ æ”¹é€²å»ºè­°: {performance['recommendations']}")
    
    async def optimize_kol_settings(self):
        """å„ªåŒ– KOL è¨­å®š"""
        logger.info("âš™ï¸ å„ªåŒ– KOL è¨­å®š...")
        
        # æ¨¡æ“¬å„ªåŒ–çµæœ
        optimizations = {
            'kol_201': {
                'writing_style': 'æ›´å¹½é»˜é¢¨è¶£ï¼Œå¢åŠ è¡¨æƒ…ç¬¦è™Ÿä½¿ç”¨',
                'key_phrases': ['éŸ­èœ', 'å‰²éŸ­èœ', 'æƒ…ç·’é¢', 'å¿ƒæƒ…æ„‰å¿«'],
                'tone': 'casual',
                'content_length': 'medium'
            },
            'kol_202': {
                'writing_style': 'æ›´å°ˆæ¥­åˆ†æï¼Œå¢åŠ æ•¸æ“šæ”¯æŒ',
                'key_phrases': ['æŠ€è¡“é¢', 'çªç ´', 'æ”¯æ’', 'é˜»åŠ›'],
                'tone': 'professional',
                'content_length': 'long'
            }
        }
        
        for kol_id, settings in optimizations.items():
            logger.info(f"ğŸ”§ {kol_id} å„ªåŒ–è¨­å®š: {settings}")
    
    async def update_learning_model(self):
        """æ›´æ–°å­¸ç¿’æ¨¡å‹"""
        logger.info("ğŸ¤– æ›´æ–°å­¸ç¿’æ¨¡å‹...")
        
        # æ¨¡æ“¬æ¨¡å‹æ›´æ–°
        model_updates = {
            'model_version': 'v2.1',
            'training_data_size': 1500,
            'accuracy_improvement': 0.08,
            'new_features': ['sentiment_analysis', 'engagement_prediction']
        }
        
        logger.info(f"ğŸ”„ æ¨¡å‹ç‰ˆæœ¬: {model_updates['model_version']}")
        logger.info(f"ğŸ“Š è¨“ç·´æ•¸æ“šé‡: {model_updates['training_data_size']}")
        logger.info(f"ğŸ“ˆ æº–ç¢ºç‡æå‡: {model_updates['accuracy_improvement']:.2%}")
        logger.info(f"âœ¨ æ–°åŠŸèƒ½: {model_updates['new_features']}")
    
    async def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½å ±å‘Š...")
        
        # ç²å–çµ±è¨ˆæ•¸æ“š
        summary = self.post_processor.get_post_summary()
        
        # ç”Ÿæˆå ±å‘Š
        report = {
            'generation_time': datetime.now().isoformat(),
            'total_posts': summary['total_posts'],
            'successful_posts': summary['successful_posts'],
            'failed_posts': summary['failed_posts'],
            'success_rate': summary['success_rate'],
            'total_interactions': summary['total_interactions'],
            'average_engagement_rate': 0.12,
            'learning_cycle_completed': True
        }
        
        # ä¿å­˜å ±å‘Š
        await self.save_performance_report(report)
        
        logger.info("ğŸ“‹ æ€§èƒ½å ±å‘Šæ‘˜è¦:")
        logger.info(f"  ç¸½ç™¼æ–‡æ•¸: {report['total_posts']}")
        logger.info(f"  æˆåŠŸç™¼æ–‡: {report['successful_posts']}")
        logger.info(f"  å¤±æ•—ç™¼æ–‡: {report['failed_posts']}")
        logger.info(f"  æˆåŠŸç‡: {report['success_rate']:.2%}")
        logger.info(f"  ç¸½äº’å‹•æ•¸: {report['total_interactions']}")
        logger.info(f"  å¹³å‡äº’å‹•ç‡: {report['average_engagement_rate']:.2%}")
    
    async def save_performance_report(self, report: Dict[str, Any]):
        """ä¿å­˜æ€§èƒ½å ±å‘Š"""
        try:
            import json
            import os
            
            # å‰µå»ºå ±å‘Šç›®éŒ„
            report_dir = "data/reports"
            os.makedirs(report_dir, exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            filepath = os.path.join(report_dir, filename)
            
            # ä¿å­˜å ±å‘Š
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ€§èƒ½å ±å‘Šå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ€§èƒ½å ±å‘Šå¤±æ•—: {e}")

async def main():
    """ä¸»å‡½æ•¸"""
    try:
        logger.info("ğŸš€ å•Ÿå‹•ç¬¬äºŒéšæ®µè™•ç†å™¨")
        
        # å‰µå»ºè™•ç†å™¨
        processor = Stage2Processor()
        
        # æ¨¡æ“¬å·²ç™¼å¸ƒçš„è²¼æ–‡çµæœ
        mock_post_results = [
            PostResult(
                post_id="20250904_020142_201",
                kol_serial="kol_201",
                article_id="article_1001",
                platform_url="https://www.cmoney.com.tw/article/1001",
                publish_time=datetime.now() - timedelta(hours=1),
                success=True
            ),
            PostResult(
                post_id="20250904_020149_201",
                kol_serial="kol_201",
                article_id="article_1002",
                platform_url="https://www.cmoney.com.tw/article/1002",
                publish_time=datetime.now() - timedelta(minutes=30),
                success=True
            ),
            PostResult(
                post_id="20250904_020151_209",
                kol_serial="kol_209",
                article_id="article_1003",
                platform_url="https://www.cmoney.com.tw/article/1003",
                publish_time=datetime.now() - timedelta(minutes=15),
                success=True
            )
        ]
        
        # åŸ·è¡Œç¬¬äºŒéšæ®µè™•ç†
        await processor.process_published_posts(mock_post_results)
        
        logger.info("ğŸ‰ ç¬¬äºŒéšæ®µè™•ç†å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ ç¬¬äºŒéšæ®µè™•ç†å¤±æ•—: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())













