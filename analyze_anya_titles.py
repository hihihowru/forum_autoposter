#!/usr/bin/env python3
"""
åˆ†æ anya TSV æ–‡ä»¶ä¸­çš„çœŸå¯¦ UGC æ¨™é¡Œé¡å‹
"""

import re
from collections import Counter, defaultdict

def analyze_title_patterns():
    """åˆ†ææ¨™é¡Œæ¨¡å¼"""
    
    # è®€å– TSV æ–‡ä»¶
    titles = []
    with open('anya-forumteam-1756973422036-23159383983751583.tsv', 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and line != 'title' and line != '""':
                titles.append(line)
    
    print(f"ğŸ“Š ç¸½å…±åˆ†æ {len(titles)} å€‹æ¨™é¡Œ")
    print("="*60)
    
    # 1. æ¨™é¡Œé•·åº¦åˆ†æ
    title_lengths = [len(title) for title in titles]
    print(f"ğŸ“ æ¨™é¡Œé•·åº¦çµ±è¨ˆ:")
    print(f"   å¹³å‡é•·åº¦: {sum(title_lengths)/len(title_lengths):.1f} å­—")
    print(f"   æœ€çŸ­: {min(title_lengths)} å­—")
    print(f"   æœ€é•·: {max(title_lengths)} å­—")
    print()
    
    # 2. è¡¨æƒ…ç¬¦è™Ÿåˆ†æ
    emoji_pattern = re.compile(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿ğŸš€-ğŸ›¿ï¸]')
    emoji_titles = [title for title in titles if emoji_pattern.search(title)]
    print(f"ğŸ˜Š ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿçš„æ¨™é¡Œ: {len(emoji_titles)} å€‹ ({len(emoji_titles)/len(titles)*100:.1f}%)")
    print("   ç¯„ä¾‹:", emoji_titles[:5])
    print()
    
    # 3. æ¨™é¡Œé¡å‹åˆ†é¡
    title_categories = {
        'ç‡Ÿæ”¶è²¡å ±': [],
        'æŠ€è¡“åˆ†æ': [],
        'æ“ä½œå»ºè­°': [],
        'å¸‚å ´æƒ…ç·’': [],
        'æ–°èè³‡è¨Š': [],
        'å€‹äººåˆ†äº«': [],
        'å•ç­”è¨è«–': [],
        'å…¶ä»–': []
    }
    
    # é—œéµè©åˆ†é¡
    keywords = {
        'ç‡Ÿæ”¶è²¡å ±': ['ç‡Ÿæ”¶', 'è²¡å ±', 'EPS', 'ç²åˆ©', 'æ¥­ç¸¾', 'å…¬å‘Š', 'å¿«è¨Š'],
        'æŠ€è¡“åˆ†æ': ['çªç ´', 'æ”¯æ’', 'å£“åŠ›', 'å‡ç·š', 'æŠ€è¡“', 'Kç·š', 'è¶¨å‹¢', 'æ´—ç›¤', 'åšé ­'],
        'æ“ä½œå»ºè­°': ['è²·é€²', 'è³£å‡º', 'åŠ ç¢¼', 'åœæ', 'åˆ†æ‰¹', 'ç•¶æ²–', 'æ“ä½œ', 'å»ºè­°'],
        'å¸‚å ´æƒ…ç·’': ['ç˜‹ç‹‚', 'ç†±æ½®', 'ç‹‚é£†', 'å¤§æ¼²', 'å¤§è·Œ', 'ææ…Œ', 'èˆˆå¥®', 'æ­»æ°£æ²‰æ²‰'],
        'æ–°èè³‡è¨Š': ['æ–°è', 'å ±å°', 'å…¬å‘Š', 'åˆä½œ', 'æ“´ç”¢', 'å¸ƒå±€', 'é¡Œæ'],
        'å€‹äººåˆ†äº«': ['æˆ‘çš„', 'è‡ªå·±', 'åˆ†äº«', 'ç¶“é©—', 'å¿ƒå¾—', 'æ“ä½œé‚è¼¯'],
        'å•ç­”è¨è«–': ['?', 'ï¼Ÿ', 'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'è©²', 'å—']
    }
    
    for title in titles:
        categorized = False
        for category, words in keywords.items():
            if any(word in title for word in words):
                title_categories[category].append(title)
                categorized = True
                break
        if not categorized:
            title_categories['å…¶ä»–'].append(title)
    
    print("ğŸ“‹ æ¨™é¡Œé¡å‹åˆ†é¡:")
    for category, titles_list in title_categories.items():
        if titles_list:
            print(f"   {category}: {len(titles_list)} å€‹ ({len(titles_list)/len(titles)*100:.1f}%)")
            print(f"      ç¯„ä¾‹: {titles_list[:3]}")
            print()
    
    # 4. ç‰¹æ®Šç¬¦è™Ÿåˆ†æ
    special_chars = {
        'ã€ã€‘': [],
        'ã€Šã€‹': [],
        '()': [],
        'ğŸ’°ğŸ’': [],
        'æ•¸å­—+%': [],
        'è‚¡ç¥¨ä»£è™Ÿ': []
    }
    
    for title in titles:
        if 'ã€' in title and 'ã€‘' in title:
            special_chars['ã€ã€‘'].append(title)
        if 'ã€Š' in title and 'ã€‹' in title:
            special_chars['ã€Šã€‹'].append(title)
        if '(' in title and ')' in title:
            special_chars['()'].append(title)
        if any(char in title for char in ['ğŸ’°', 'ğŸ’', 'ğŸ’µ', 'ğŸ’¸']):
            special_chars['ğŸ’°ğŸ’'].append(title)
        if re.search(r'\d+%', title):
            special_chars['æ•¸å­—+%'].append(title)
        if re.search(r'\d{4}', title):  # è‚¡ç¥¨ä»£è™Ÿé€šå¸¸æ˜¯4ä½æ•¸å­—
            special_chars['è‚¡ç¥¨ä»£è™Ÿ'].append(title)
    
    print("ğŸ” ç‰¹æ®Šç¬¦è™Ÿåˆ†æ:")
    for symbol_type, titles_list in special_chars.items():
        if titles_list:
            print(f"   {symbol_type}: {len(titles_list)} å€‹")
            print(f"      ç¯„ä¾‹: {titles_list[:2]}")
            print()
    
    # 5. æƒ…æ„Ÿè©å½™åˆ†æ
    emotion_words = {
        'æ­£é¢': ['å¥½æ£’', 'èˆ’æœ', 'é–‹å¿ƒ', 'æœŸå¾…', 'çœ‹å¥½', 'å¼·å‹¢', 'çŒ›çŒ›çš„', 'æ£’æ£’çš„'],
        'è² é¢': ['çˆ›', 'æ…˜', 'æ­»', 'é¨™', 'ç‹ ', 'çˆ›é€äº†', 'æ…˜å…®å…®', 'æ²’æ•‘'],
        'ä¸­æ€§': ['æ³¨æ„', 'é—œæ³¨', 'æé†’', 'å»ºè­°', 'åˆ†æ', 'å±•æœ›']
    }
    
    emotion_counts = defaultdict(int)
    for title in titles:
        for emotion, words in emotion_words.items():
            if any(word in title for word in words):
                emotion_counts[emotion] += 1
    
    print("ğŸ˜Š æƒ…æ„Ÿè©å½™åˆ†æ:")
    for emotion, count in emotion_counts.items():
        print(f"   {emotion}: {count} å€‹ ({count/len(titles)*100:.1f}%)")
    print()
    
    # 6. ç¶²è·¯ç”¨èªåˆ†æ
    internet_slang = ['GG', 'GGäº†', 'åŠ æ²¹', 'èˆ’æœ', 'çŒ›çŒ›çš„', 'æ£’æ£’çš„', 'å¥½ç‹ ', 'ä¸æ¼”äº†', 'æ‹­ç›®ä»¥å¾…']
    slang_titles = [title for title in titles if any(slang in title for slang in internet_slang)]
    print(f"ğŸŒ ç¶²è·¯ç”¨èªæ¨™é¡Œ: {len(slang_titles)} å€‹")
    print("   ç¯„ä¾‹:", slang_titles[:5])
    print()
    
    # 7. æ¨™é¡Œçµæ§‹åˆ†æ
    structure_patterns = {
        'å•å¥': [],
        'æ„Ÿå˜†å¥': [],
        'é™³è¿°å¥': [],
        'æŒ‡ä»¤å¥': []
    }
    
    for title in titles:
        if title.endswith('?') or title.endswith('ï¼Ÿ'):
            structure_patterns['å•å¥'].append(title)
        elif title.endswith('!') or title.endswith('ï¼'):
            structure_patterns['æ„Ÿå˜†å¥'].append(title)
        elif any(word in title for word in ['è«‹', 'æ³¨æ„', 'æé†’', 'å»ºè­°']):
            structure_patterns['æŒ‡ä»¤å¥'].append(title)
        else:
            structure_patterns['é™³è¿°å¥'].append(title)
    
    print("ğŸ“ æ¨™é¡Œçµæ§‹åˆ†æ:")
    for structure, titles_list in structure_patterns.items():
        print(f"   {structure}: {len(titles_list)} å€‹ ({len(titles_list)/len(titles)*100:.1f}%)")
        print(f"      ç¯„ä¾‹: {titles_list[:2]}")
        print()
    
    # 8. æœ€å¸¸è¦‹è©å½™
    all_words = []
    for title in titles:
        words = re.findall(r'[\u4e00-\u9fff]+', title)  # ä¸­æ–‡å­—
        all_words.extend(words)
    
    word_counts = Counter(all_words)
    print("ğŸ“Š æœ€å¸¸è¦‹è©å½™ (å‰20):")
    for word, count in word_counts.most_common(20):
        print(f"   {word}: {count} æ¬¡")
    print()
    
    # 9. ç¸½çµå»ºè­°
    print("ğŸ’¡ å° AI æ¨™é¡Œç”Ÿæˆçš„å»ºè­°:")
    print("   1. çœŸå¯¦ UGC æ¨™é¡Œé•·åº¦å¤šæ¨£åŒ–ï¼Œå¾ç°¡çŸ­åˆ°è©³ç´°éƒ½æœ‰")
    print("   2. å¤§é‡ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢åŠ æƒ…æ„Ÿè¡¨é”")
    print("   3. ç¶²è·¯ç”¨èªå’Œå£èªåŒ–è¡¨é”å¾ˆå¸¸è¦‹")
    print("   4. å•å¥å’Œæ„Ÿå˜†å¥çµæ§‹å¢åŠ äº’å‹•æ€§")
    print("   5. è‚¡ç¥¨ä»£è™Ÿã€æ•¸å­—ã€ç™¾åˆ†æ¯”ç­‰å…·é«”è³‡è¨Šå¾ˆé‡è¦")
    print("   6. æƒ…æ„Ÿè©å½™è±å¯Œï¼Œæ­£é¢ã€è² é¢ã€ä¸­æ€§éƒ½æœ‰")
    print("   7. ç‰¹æ®Šç¬¦è™Ÿå¦‚ã€ã€‘ã€ã€Šã€‹ã€() ç”¨æ–¼åˆ†é¡å’Œå¼·èª¿")
    print("   8. å€‹äººåŒ–è¡¨é”å¦‚'æˆ‘çš„'ã€'åˆ†äº«'å¢åŠ è¦ªè¿‘æ„Ÿ")

if __name__ == "__main__":
    analyze_title_patterns()
