"""
OHLC æ•¸æ“šç·©å­˜ç®¡ç†å™¨
å¯¦ç¾æœ¬åœ° CSV å­˜å„²ï¼Œé¿å…é‡è¤‡ API èª¿ç”¨
"""

import os
import pandas as pd
import logging
from datetime import datetime, timedelta
from typing import Dict, Optional, List, Tuple
from pathlib import Path
import finlab
import finlab.data as fdata

logger = logging.getLogger(__name__)

class OHLCCacheManager:
    """OHLC æ•¸æ“šç·©å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "data/cache/ohlc"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¢ºä¿ Finlab å·²ç™»å…¥
        import os
        finlab_key = os.getenv('FINLAB_API_KEY')
        if finlab_key:
            try:
                finlab.login(finlab_key)
                logger.info("OHLC ç·©å­˜ç®¡ç†å™¨ï¼šFinlab API ç™»å…¥æˆåŠŸ")
            except Exception as e:
                logger.warning(f"OHLC ç·©å­˜ç®¡ç†å™¨ï¼šFinlab API ç™»å…¥å¤±æ•— - {e}")
        else:
            logger.warning("OHLC ç·©å­˜ç®¡ç†å™¨ï¼šæœªæ‰¾åˆ° FINLAB_API_KEY ç’°å¢ƒè®Šæ•¸")
        
        # æ•¸æ“šé¡å‹æ˜ å°„
        self.data_types = {
            'close': 'price:æ”¶ç›¤åƒ¹',
            'open': 'price:é–‹ç›¤åƒ¹', 
            'high': 'price:æœ€é«˜åƒ¹',
            'low': 'price:æœ€ä½åƒ¹',
            'volume': 'price:æˆäº¤è‚¡æ•¸'
        }
        
        logger.info(f"OHLC ç·©å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç·©å­˜ç›®éŒ„: {self.cache_dir}")
    
    def _get_cache_filename(self, data_type: str, date: str) -> Path:
        """ç²å–ç·©å­˜æ–‡ä»¶å"""
        return self.cache_dir / f"{data_type}_{date}.csv"
    
    def _get_today_date(self) -> str:
        """ç²å–ä»Šæ—¥æ—¥æœŸå­—ç¬¦ä¸²"""
        return datetime.now().strftime('%Y%m%d')
    
    def _is_cache_valid(self, cache_file: Path, target_date: str) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ"""
        
        if not cache_file.exists():
            return False
        
        try:
            # æª¢æŸ¥æ–‡ä»¶ä¿®æ”¹æ™‚é–“
            file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
            target_datetime = datetime.strptime(target_date, '%Y%m%d')
            
            # å¦‚æœæ˜¯ä»Šå¤©çš„æ•¸æ“šï¼Œæª¢æŸ¥æ˜¯å¦åœ¨äº¤æ˜“æ™‚é–“å¾Œæ›´æ–°
            if target_date == self._get_today_date():
                # å°è‚¡äº¤æ˜“æ™‚é–“é€šå¸¸åœ¨ 13:30 çµæŸ
                market_close = target_datetime.replace(hour=14, minute=0)
                current_time = datetime.now()
                
                # å¦‚æœç¾åœ¨é‚„åœ¨äº¤æ˜“æ™‚é–“å…§ï¼Œä½¿ç”¨ç·©å­˜ä½†æ¨™è¨˜å¯èƒ½éœ€è¦æ›´æ–°
                if current_time < market_close:
                    return True
                
                # å¦‚æœäº¤æ˜“å·²çµæŸï¼Œæª¢æŸ¥ç·©å­˜æ˜¯å¦åœ¨æ”¶ç›¤å¾Œæ›´æ–°
                return file_mtime > market_close
            
            # æ­·å²æ•¸æ“šï¼Œæª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”éç©º
            if cache_file.stat().st_size > 0:
                return True
                
        except Exception as e:
            logger.warning(f"ç·©å­˜æœ‰æ•ˆæ€§æª¢æŸ¥å¤±æ•—: {e}")
            
        return False
    
    def _load_cache(self, data_type: str, date: str) -> Optional[pd.DataFrame]:
        """è¼‰å…¥ç·©å­˜æ•¸æ“š"""
        
        cache_file = self._get_cache_filename(data_type, date)
        
        if not self._is_cache_valid(cache_file, date):
            return None
            
        try:
            df = pd.read_csv(cache_file, index_col=0, parse_dates=True)
            logger.info(f"âœ… å¾ç·©å­˜è¼‰å…¥ {data_type} æ•¸æ“š: {cache_file}")
            return df
            
        except Exception as e:
            logger.warning(f"è¼‰å…¥ç·©å­˜å¤±æ•—: {e}")
            return None
    
    def _save_cache(self, data: pd.DataFrame, data_type: str, date: str):
        """ä¿å­˜æ•¸æ“šåˆ°ç·©å­˜"""
        
        cache_file = self._get_cache_filename(data_type, date)
        
        try:
            data.to_csv(cache_file)
            logger.info(f"ğŸ’¾ ä¿å­˜ {data_type} æ•¸æ“šåˆ°ç·©å­˜: {cache_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç·©å­˜å¤±æ•—: {e}")
    
    def _fetch_from_finlab(self, data_type: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """å¾ Finlab API ç²å–æ•¸æ“š"""
        
        finlab_key = self.data_types[data_type]
        logger.info(f"ğŸ“¡ å¾ Finlab API ç²å– {data_type} æ•¸æ“š...")
        
        # ä½¿ç”¨ä¸å¸¶æ™‚é–“åƒæ•¸çš„ç‰ˆæœ¬ï¼Œé¿å…é©—è­‰ç¢¼å•é¡Œ
        full_data = finlab.data.get(finlab_key)
        
        # æ‰‹å‹•ç¯©é¸æ™‚é–“ç¯„åœ
        if not full_data.empty:
            # ç¢ºä¿ç´¢å¼•æ˜¯ datetime æ ¼å¼
            if not isinstance(full_data.index, pd.DatetimeIndex):
                full_data.index = pd.to_datetime(full_data.index)
            
            # ç¯©é¸æ—¥æœŸç¯„åœ
            mask = (full_data.index >= start_date) & (full_data.index <= end_date)
            filtered_data = full_data.loc[mask]
            
            logger.info(f"âœ… ç²å– {data_type} æ•¸æ“šï¼š{len(filtered_data)} å€‹äº¤æ˜“æ—¥")
            return filtered_data
        
        return full_data
    
    def get_ohlc_data(self, stock_ids: List[str], days: int = 300) -> Dict[str, pd.DataFrame]:
        """ç²å– OHLC æ•¸æ“šï¼ˆå„ªå…ˆä½¿ç”¨ç·©å­˜ï¼‰"""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        today_str = self._get_today_date()
        
        result = {}
        
        for data_type in self.data_types.keys():
            # å˜—è©¦è¼‰å…¥ç·©å­˜
            cached_data = self._load_cache(data_type, today_str)
            
            if cached_data is not None:
                # éæ¿¾æ‰€éœ€è‚¡ç¥¨ä¸¦æª¢æŸ¥æ—¥æœŸç¯„åœ
                available_stocks = [sid for sid in stock_ids if sid in cached_data.columns]
                
                if available_stocks:
                    # æª¢æŸ¥æ—¥æœŸç¯„åœæ˜¯å¦è¶³å¤ 
                    data_start = cached_data.index.min()
                    if data_start <= start_date:
                        # ç·©å­˜æ•¸æ“šå……è¶³ï¼Œç›´æ¥ä½¿ç”¨
                        filtered_data = cached_data[available_stocks]
                        filtered_data = filtered_data[filtered_data.index >= start_date]
                        result[data_type] = filtered_data
                        
                        logger.info(f"âœ… ä½¿ç”¨ç·©å­˜ {data_type} æ•¸æ“šï¼Œè‚¡ç¥¨: {available_stocks}")
                        continue
            
            # ç·©å­˜ä¸è¶³ï¼Œå¾ API ç²å–
            logger.info(f"ğŸ“¡ ç·©å­˜ä¸è¶³ï¼Œå¾ API ç²å– {data_type} æ•¸æ“š...")
            
            try:
                api_data = self._fetch_from_finlab(data_type, start_date, end_date)
                
                # ä¿å­˜åˆ°ç·©å­˜
                self._save_cache(api_data, data_type, today_str)
                
                # éæ¿¾æ‰€éœ€è‚¡ç¥¨
                available_stocks = [sid for sid in stock_ids if sid in api_data.columns]
                if available_stocks:
                    result[data_type] = api_data[available_stocks]
                    logger.info(f"âœ… API ç²å– {data_type} æ•¸æ“šæˆåŠŸï¼Œè‚¡ç¥¨: {available_stocks}")
                else:
                    logger.warning(f"âš ï¸ {data_type} æ•¸æ“šä¸­ç„¡æ‰€éœ€è‚¡ç¥¨: {stock_ids}")
                    
            except Exception as e:
                logger.error(f"å¾ API ç²å– {data_type} æ•¸æ“šå¤±æ•—: {e}")
                result[data_type] = pd.DataFrame()
        
        return result
    
    def get_stock_ohlc(self, stock_id: str, days: int = 300) -> Optional[pd.DataFrame]:
        """ç²å–å–®ä¸€è‚¡ç¥¨çš„å®Œæ•´ OHLC æ•¸æ“š"""
        
        ohlc_data = self.get_ohlc_data([stock_id], days)
        
        if not all(data_type in ohlc_data for data_type in self.data_types.keys()):
            logger.error(f"ç„¡æ³•ç²å– {stock_id} çš„å®Œæ•´ OHLC æ•¸æ“š")
            return None
        
        try:
            # çµ„åˆæˆå®Œæ•´çš„ OHLC DataFrame
            df = pd.DataFrame({
                'open': ohlc_data['open'][stock_id] if stock_id in ohlc_data['open'].columns else pd.Series(),
                'high': ohlc_data['high'][stock_id] if stock_id in ohlc_data['high'].columns else pd.Series(),
                'low': ohlc_data['low'][stock_id] if stock_id in ohlc_data['low'].columns else pd.Series(),
                'close': ohlc_data['close'][stock_id] if stock_id in ohlc_data['close'].columns else pd.Series(),
                'volume': ohlc_data['volume'][stock_id] if stock_id in ohlc_data['volume'].columns else pd.Series()
            })
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            df = df.dropna()
            
            if len(df) == 0:
                logger.warning(f"âš ï¸ {stock_id} ç„¡æœ‰æ•ˆæ•¸æ“š")
                return None
                
            logger.info(f"âœ… æˆåŠŸç²å– {stock_id} OHLC æ•¸æ“š: {len(df)} å€‹äº¤æ˜“æ—¥")
            return df
            
        except Exception as e:
            logger.error(f"çµ„åˆ {stock_id} OHLC æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def clear_old_cache(self, days_to_keep: int = 7):
        """æ¸…ç†èˆŠç·©å­˜æ–‡ä»¶"""
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        
        try:
            for cache_file in self.cache_dir.glob("*.csv"):
                file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                
                if file_mtime < cutoff_date:
                    cache_file.unlink()
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠç·©å­˜: {cache_file}")
                    
        except Exception as e:
            logger.error(f"æ¸…ç†ç·©å­˜å¤±æ•—: {e}")
    
    def get_cache_status(self) -> Dict[str, any]:
        """ç²å–ç·©å­˜ç‹€æ…‹"""
        
        status = {
            'cache_dir': str(self.cache_dir),
            'cache_files': [],
            'total_size_mb': 0
        }
        
        try:
            total_size = 0
            for cache_file in self.cache_dir.glob("*.csv"):
                file_size = cache_file.stat().st_size
                file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                
                status['cache_files'].append({
                    'filename': cache_file.name,
                    'size_mb': round(file_size / 1024 / 1024, 2),
                    'modified': file_mtime.strftime('%Y-%m-%d %H:%M:%S')
                })
                
                total_size += file_size
            
            status['total_size_mb'] = round(total_size / 1024 / 1024, 2)
            status['file_count'] = len(status['cache_files'])
            
        except Exception as e:
            logger.error(f"ç²å–ç·©å­˜ç‹€æ…‹å¤±æ•—: {e}")
        
        return status

def create_ohlc_cache_manager(cache_dir: str = "data/cache/ohlc") -> OHLCCacheManager:
    """å‰µå»º OHLC ç·©å­˜ç®¡ç†å™¨"""
    return OHLCCacheManager(cache_dir)
