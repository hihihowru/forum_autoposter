"""
GPTå…§å®¹ç”Ÿæˆå™¨
ä½¿ç”¨OpenAI GPTæ¨¡å‹ç”Ÿæˆé«˜è³ªé‡è‚¡ç¥¨åˆ†æå…§å®¹
"""

import os
import openai
from typing import Dict, List, Any, Optional
import json
import logging
from dotenv import load_dotenv
import re

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv('../../../../.env')

logger = logging.getLogger(__name__)

def format_price(value) -> str:
    """æ ¼å¼åŒ–è‚¡åƒ¹ï¼Œç§»é™¤ä¸å¿…è¦çš„å°æ•¸é»ï¼ˆå¦‚ 201.0 â†’ 201ï¼‰"""
    try:
        num = float(value)
        if num == int(num):
            return str(int(num))
        return f"{num:.2f}"
    except (ValueError, TypeError):
        return str(value)

def format_number_chinese(value) -> str:
    """æ ¼å¼åŒ–æ•¸å­—ç‚ºä¸­æ–‡å–®ä½ï¼ˆè¬ã€å„„ï¼‰"""
    try:
        num = float(value)
        if abs(num) >= 100000000:  # 1å„„ä»¥ä¸Š
            return f"{num/100000000:.2f}å„„"
        elif abs(num) >= 10000:  # 1è¬ä»¥ä¸Š
            return f"{num/10000:.2f}è¬"
        elif num == int(num):
            return str(int(num))
        return f"{num:.2f}"
    except (ValueError, TypeError):
        return str(value)

class GPTContentGenerator:
    """GPTå…§å®¹ç”Ÿæˆå™¨

    é è¨­ä½¿ç”¨ gpt-4o-mini æ¨¡å‹ï¼Œæä¾›è‰¯å¥½çš„é€Ÿåº¦å’Œè³ªé‡å¹³è¡¡ã€‚
    æ³¨æ„ï¼šgpt-5 ç³»åˆ—æ¨¡å‹å·²ç¦ç”¨ï¼ˆOpenAI å°šæœªç™¼å¸ƒï¼‰
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        # é‡æ–°è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä»¥ç¢ºä¿API Keyæ­£ç¢ºè¼‰å…¥
        load_dotenv('../../../../.env')
        # ğŸ”¥ FIX: Strip whitespace and newlines from API key (Railway env var issue)
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if self.api_key:
            self.api_key = self.api_key.strip()
        self.model = model

        if self.api_key:
            openai.api_key = self.api_key
            logger.info(f"GPTå…§å®¹ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model}")
        else:
            logger.warning("OPENAI_API_KEY æœªè¨­å®šï¼Œå°‡ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")
    
    def generate_stock_analysis(self,
                             stock_id: str,
                             stock_name: str,
                             kol_profile: Dict[str, Any],
                             posting_type: str = "analysis",
                             trigger_type: str = "custom_stocks",
                             serper_analysis: Optional[Dict[str, Any]] = None,
                             realtime_price_data: Optional[Dict[str, Any]] = None,
                             ohlc_data: Optional[Dict[str, Any]] = None,
                             technical_indicators: Optional[Dict[str, Any]] = None,
                             dtno_data: Optional[Dict[str, Any]] = None,  # ğŸ”¥ NEW: DTNO æ•¸æ“š
                             content_length: str = "medium",
                             max_words: int = 1000,  # ğŸ”¥ å¢åŠ å­—æ•¸é™åˆ¶ä»¥ç²å¾—æ›´è©³ç´°çš„åˆ†æ
                             model: Optional[str] = None,
                             template_id: Optional[int] = None,
                             db_connection = None) -> Dict[str, Any]:
        """ä½¿ç”¨GPTç”Ÿæˆè‚¡ç¥¨åˆ†æå…§å®¹ - Prompt æ¨¡æ¿ç³»çµ±

        Args:
            stock_id: è‚¡ç¥¨ä»£è™Ÿ
            stock_name: è‚¡ç¥¨åç¨±
            kol_profile: å®Œæ•´çš„KOLè³‡æ–™
            posting_type: ç™¼æ–‡é¡å‹ (analysis/interaction/personalized)
            trigger_type: è§¸ç™¼å™¨é¡å‹
            serper_analysis: Serperæ–°èåˆ†æçµæœ
            realtime_price_data: CMoneyå³æ™‚è‚¡åƒ¹è³‡è¨Š (åŒ…å« current_price, volume, changeç­‰)
            ohlc_data: OHLCåƒ¹æ ¼æ•¸æ“š
            technical_indicators: æŠ€è¡“æŒ‡æ¨™æ•¸æ“š
            dtno_data: DTNO æ•¸æ“š (åŸºæœ¬é¢/æŠ€è¡“é¢/ç±Œç¢¼é¢)
            content_length: å…§å®¹é•·åº¦
            max_words: æœ€å¤§å­—æ•¸
            model: æ¨¡å‹ID
            template_id: Prompt æ¨¡æ¿ IDï¼ˆå¯é¸ï¼‰
            db_connection: è³‡æ–™åº«é€£ç·šï¼ˆå¯é¸ï¼‰
        """

        try:
            if not self.api_key:
                kol_persona = kol_profile.get('persona', 'mixed')
                return self._fallback_generation(stock_id, stock_name, kol_persona)

            # ğŸ”¥ ç¢ºå®šä½¿ç”¨çš„æ¨¡å‹
            chosen_model = model if model else self.model
            logger.info(f"ğŸ¤– GPT ç”Ÿæˆå™¨ä½¿ç”¨æ¨¡å‹: {chosen_model}, posting_type: {posting_type}")

            # è™•ç†é è¨­å€¼
            serper_analysis = serper_analysis or {}

            # ğŸ¯ è¼‰å…¥ Prompt æ¨¡æ¿
            template = self._load_prompt_template(posting_type, template_id, db_connection)
            logger.info(f"ğŸ“‹ ä½¿ç”¨æ¨¡æ¿: {template.get('name', 'é è¨­æ¨¡æ¿')}")

            # ğŸ¯ æº–å‚™åƒæ•¸
            params = self._prepare_template_parameters(
                kol_profile, stock_id, stock_name, trigger_type,
                serper_analysis, realtime_price_data, ohlc_data, technical_indicators, dtno_data, max_words
            )

            # ğŸ¯ æ³¨å…¥åƒæ•¸åˆ°æ¨¡æ¿
            system_prompt = self._inject_parameters(template['system_prompt_template'], params)
            user_prompt = self._inject_parameters(template['user_prompt_template'], params)

            # ğŸ” DEBUG: å°å‡ºé—œéµåƒæ•¸
            logger.info(f"ğŸ” DEBUG params keys: {list(params.keys())}")
            logger.info(f"ğŸ” DEBUG has_realtime_price: {params.get('has_realtime_price', False)}")
            logger.info(f"ğŸ” DEBUG ohlc_summary é•·åº¦: {len(params.get('ohlc_summary', ''))} å­—")
            if params.get('ohlc_summary'):
                logger.info(f"ğŸ” DEBUG ohlc_summary å‰ 200 å­—: {params['ohlc_summary'][:200]}")
            logger.info(f"ğŸ“ System Prompt é•·åº¦: {len(system_prompt)} å­—")
            logger.info(f"ğŸ“ User Prompt é•·åº¦: {len(user_prompt)} å­—")
            logger.info(f"ğŸ” DEBUG User Prompt å‰ 500 å­—: {user_prompt[:500]}")

            # ğŸ”¥ åˆ¤æ–·æ˜¯å¦ç‚º GPT-5 ç³»åˆ—
            # âš ï¸ GPT-5 å·²ç¦ç”¨ - OpenAI å°šæœªç™¼å¸ƒ gpt-5 æ¨¡å‹ï¼Œæœƒå°è‡´ API éŒ¯èª¤
            is_gpt5_model = False  # chosen_model.startswith('gpt-5') - DISABLED

            # ğŸ”¥ å¦‚æœä½¿ç”¨è€…é¸æ“‡äº† gpt-5ï¼Œè‡ªå‹•é™ç´šåˆ° gpt-4o-mini
            if chosen_model.startswith('gpt-5'):
                logger.warning(f"âš ï¸ GPT-5 æ¨¡å‹å·²ç¦ç”¨ï¼Œè‡ªå‹•é™ç´šåˆ° gpt-4o-mini")
                chosen_model = 'gpt-4o-mini'

            # ğŸ”¥ GPT-5 å¯ä»¥ä½¿ç”¨å…©ç¨® APIï¼š
            # 1. Responses API (æ¨è–¦ï¼Œæ”¯æ´ CoT) - DISABLED
            # 2. Chat Completions API (å‚³çµ±æ–¹å¼ï¼Œç”¨ reasoning_effort åƒæ•¸)
            # ç›®å‰çµ±ä¸€ä½¿ç”¨ Chat Completions API

            if is_gpt5_model:
                # ğŸ”¥ GPT-5: ä½¿ç”¨ Responses API
                logger.info(f"ğŸ¤– ä½¿ç”¨ GPT-5 Responses API")

                # ğŸ”¥ æ‰€æœ‰ GPT-5 æ¨¡å‹éƒ½ä½¿ç”¨ medium reasoning effort
                # medium æä¾›æœ€ä½³çš„é€Ÿåº¦/è³ªé‡å¹³è¡¡ï¼š
                # - gpt-5: ~30-40ç§’ï¼Œ800-1200å­— âœ…
                # - gpt-5-mini: ~15-25ç§’ï¼Œ600-1000å­— âœ…
                # - gpt-5-nano: ~10-15ç§’ï¼Œ400-800å­— âœ…
                #
                # é¿å…ä½¿ç”¨ highï¼ˆå¤ªæ…¢ï¼Œ60-90ç§’ï¼Œç¶“å¸¸ incompleteï¼‰
                # é¿å…ä½¿ç”¨ lowï¼ˆå¤ªå¿«ï¼Œä½†å…§å®¹å¤ªçŸ­ 200-300å­—ï¼‰
                reasoning_effort = "medium"

                # ğŸ”¥ ä½¿ç”¨ instructions (system prompt) å’Œ input (user prompt) åˆ†é–‹å‚³é
                api_params = {
                    "model": chosen_model,
                    "instructions": system_prompt,  # System/developer message
                    "input": user_prompt,  # User input
                    "max_output_tokens": 3000,  # å¢åŠ è¼¸å‡ºé•·åº¦é™åˆ¶
                    "reasoning": {"effort": reasoning_effort},  # ğŸ”¥ æ ¹æ“šæ¨¡å‹å‹•æ…‹èª¿æ•´
                    "text": {"verbosity": "high"}  # ğŸ”¥ ä¿æŒ high ä»¥ç²å¾—è©³ç´°å…§å®¹
                }

                logger.info(f"ğŸ¤– GPT-5 åƒæ•¸: model={chosen_model}, max_output_tokens=3000, reasoning={reasoning_effort}, verbosity=high")

                # èª¿ç”¨ Responses API
                try:
                    response = openai.responses.create(**api_params)
                except Exception as api_error:
                    logger.error(f"âŒ OpenAI Responses API èª¿ç”¨å¤±æ•—: {type(api_error).__name__}: {api_error}")
                    logger.error(f"âŒ ä½¿ç”¨çš„æ¨¡å‹: {chosen_model}")
                    logger.error(f"âŒ API åƒæ•¸: {api_params}")
                    raise

                # ğŸ” DEBUG: å°å‡º Responses API å›æ‡‰çµæ§‹
                logger.info(f"ğŸ” DEBUG response.status: {response.status}")
                logger.info(f"ğŸ” DEBUG response.output é•·åº¦: {len(response.output)}")

                # ğŸ”¥ å¦‚æœ response é‚„æ²’å®Œæˆï¼Œç­‰å¾…å®ƒå®Œæˆ
                if response.status == "incomplete" or response.status == "in_progress":
                    logger.warning(f"âš ï¸ Response ç‹€æ…‹ç‚º {response.status}ï¼Œå˜—è©¦è¼ªè©¢ç²å–å®Œæ•´çµæœ...")

                    # è¼ªè©¢ç­‰å¾…å®Œæˆï¼ˆæœ€å¤šç­‰å¾… 60 ç§’ï¼‰
                    import time
                    max_retries = 60
                    retry_count = 0

                    while retry_count < max_retries and response.status in ["incomplete", "in_progress"]:
                        time.sleep(1)
                        retry_count += 1

                        # é‡æ–°ç²å– response
                        try:
                            response = openai.responses.retrieve(response.id)
                            logger.info(f"ğŸ”„ è¼ªè©¢ {retry_count}/{max_retries}: status={response.status}")
                        except Exception as poll_error:
                            logger.error(f"âŒ è¼ªè©¢å¤±æ•—: {poll_error}")
                            break

                    if response.status != "completed":
                        logger.error(f"âŒ Response æœªåœ¨æ™‚é™å…§å®Œæˆï¼Œæœ€çµ‚ç‹€æ…‹: {response.status}")
                    else:
                        logger.info(f"âœ… Response å®Œæˆï¼Œå…±è¼ªè©¢ {retry_count} æ¬¡")

                # å¾ Responses API æå–å…§å®¹
                content = None

                # ğŸ”¥ é¦–å…ˆå˜—è©¦ä½¿ç”¨ SDK çš„ä¾¿æ·å±¬æ€§ output_text
                if hasattr(response, 'output_text') and response.output_text:
                    content = response.output_text
                    logger.info(f"âœ… ä½¿ç”¨ SDK output_text å±¬æ€§æå–å…§å®¹ï¼Œé•·åº¦: {len(content)} å­—")

                # å¦‚æœæ²’æœ‰ output_textï¼Œæ‰‹å‹•éæ­· output array
                elif response.output and len(response.output) > 0:
                    logger.info(f"âš ï¸ SDK æ²’æœ‰ output_textï¼Œæ‰‹å‹•éæ­· output array")

                    # éæ­·æ‰€æœ‰ output itemsï¼Œæ‰¾åˆ° message é¡å‹
                    for i, output_item in enumerate(response.output):
                        logger.info(f"ğŸ” DEBUG output[{i}].type: {output_item.type}")

                        if output_item.type == "message":
                            logger.info(f"âœ… æ‰¾åˆ° message item at index {i}")

                            # æª¢æŸ¥ message æ˜¯å¦æœ‰ content
                            if hasattr(output_item, 'content') and output_item.content:
                                # æå– output_text
                                for content_item in output_item.content:
                                    if hasattr(content_item, 'type') and content_item.type == "output_text":
                                        content = content_item.text
                                        logger.info(f"âœ… æˆåŠŸæå–æ–‡å­—å…§å®¹ï¼Œé•·åº¦: {len(content)} å­—")
                                        break

                                if content:
                                    break  # æ‰¾åˆ°å…§å®¹å¾Œè·³å‡ºå¾ªç’°

                    if not content:
                        logger.error(f"âŒ ç„¡æ³•å¾ Responses API æå–æ–‡å­—å…§å®¹")
                        logger.error(f"âŒ response.status: {response.status}")
                        logger.error(f"âŒ æ‰€æœ‰ output types: {[item.type for item in response.output]}")

                        # ğŸ”¥ FIX: å¦‚æœ GPT-5 ç„¡æ³•æå–å…§å®¹ï¼Œç›´æ¥ fallback åˆ°æ¨¡æ¿
                        logger.warning(f"âš ï¸ GPT-5 ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ¨¡æ¿")
                        kol_persona = kol_profile.get('persona', 'mixed')
                        return self._fallback_generation(stock_id, stock_name, kol_persona)
                else:
                    logger.error(f"âŒ Responses API å›æ‡‰æ²’æœ‰ output")
                    # ğŸ”¥ FIX: å¦‚æœæ²’æœ‰ outputï¼Œç›´æ¥ fallback åˆ°æ¨¡æ¿
                    logger.warning(f"âš ï¸ GPT-5 æ²’æœ‰å›æ‡‰ï¼Œä½¿ç”¨å‚™ç”¨æ¨¡æ¿")
                    kol_persona = kol_profile.get('persona', 'mixed')
                    return self._fallback_generation(stock_id, stock_name, kol_persona)

            else:
                # ğŸ”¥ èˆŠæ¨¡å‹: ä½¿ç”¨ Chat Completions API
                api_params = {
                    "model": chosen_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ]
                }

                # åˆ¤æ–·æ˜¯å¦ç‚ºæ¨ç†æ¨¡å‹ï¼ˆo1, o3 ç­‰ï¼‰
                is_reasoning_model = any(model_prefix in chosen_model.lower() for model_prefix in ['o1', 'o3'])

                # ğŸ”¥ æ ¹æ“š max_words å‹•æ…‹è¨ˆç®— max_tokens
                # ä¸­æ–‡: 1 token â‰ˆ 1-2 å€‹å­—ï¼Œçµ¦3å€ç·©è¡ç¢ºä¿å®Œæ•´ç”Ÿæˆ
                # æœ€å° 1500ï¼Œæœ€å¤§ 16000ï¼ˆGPT-4 é™åˆ¶ï¼‰
                calculated_max_tokens = min(max(max_words * 3, 1500), 16000)

                if is_reasoning_model:
                    # æ¨ç†æ¨¡å‹ï¼šä½¿ç”¨ max_completion_tokensï¼Œä¸ä½¿ç”¨ temperature
                    api_params["max_completion_tokens"] = calculated_max_tokens
                    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨ç†æ¨¡å‹åƒæ•¸: max_completion_tokens={calculated_max_tokens} (max_words={max_words})")
                else:
                    # ä¸€èˆ¬æ¨¡å‹ï¼šä½¿ç”¨ max_tokens + temperature
                    api_params["max_tokens"] = calculated_max_tokens
                    api_params["temperature"] = 0.7
                    logger.info(f"ğŸ¤– ä½¿ç”¨ä¸€èˆ¬æ¨¡å‹åƒæ•¸: max_tokens={calculated_max_tokens}, temperature=0.7 (max_words={max_words})")

                # èª¿ç”¨ Chat Completions API
                try:
                    response = openai.chat.completions.create(**api_params)
                except Exception as api_error:
                    logger.error(f"âŒ OpenAI Chat Completions API èª¿ç”¨å¤±æ•—: {type(api_error).__name__}: {api_error}")
                    logger.error(f"âŒ ä½¿ç”¨çš„æ¨¡å‹: {chosen_model}")
                    logger.error(f"âŒ API åƒæ•¸: {api_params}")
                    raise

                # ğŸ” DEBUG: å°å‡ºå®Œæ•´ response çµæ§‹
                logger.info(f"ğŸ” DEBUG response.choices é•·åº¦: {len(response.choices)}")
                logger.info(f"ğŸ” DEBUG response.choices[0].message: {response.choices[0].message}")
                logger.info(f"ğŸ” DEBUG response.choices[0].finish_reason: {response.choices[0].finish_reason}")

                # âš ï¸ æª¢æŸ¥æ˜¯å¦å›  token é™åˆ¶è€Œæˆªæ–·
                finish_reason = response.choices[0].finish_reason
                if finish_reason == "length":
                    logger.warning(f"âš ï¸ å…§å®¹å› é”åˆ° max_tokens é™åˆ¶è€Œè¢«æˆªæ–·ï¼")
                    logger.warning(f"âš ï¸ ç•¶å‰è¨­å®š: max_tokens={calculated_max_tokens}, max_words={max_words}")
                    logger.warning(f"âš ï¸ å»ºè­°: æ¸›å°‘ max_words æˆ–å…§å®¹æœƒä¸å®Œæ•´")

                content = response.choices[0].message.content

            # ğŸ” DEBUG: å°å‡º GPT åŸå§‹å›æ‡‰
            logger.info(f"ğŸ” DEBUG GPT åŸå§‹å›æ‡‰é•·åº¦: {len(content) if content else 0} å­—")
            logger.info(f"ğŸ” DEBUG GPT åŸå§‹å›æ‡‰å‰ 200 å­—: {content[:200] if content else 'None'}")

            # è§£æGPTå›æ‡‰
            result = self._parse_gpt_response(content, stock_id, stock_name)

            # ğŸ” DEBUG: å°å‡ºè§£æå¾Œçš„çµæœ
            logger.info(f"ğŸ” DEBUG è§£æå¾Œ title: {result.get('title', 'None')}")
            logger.info(f"ğŸ” DEBUG è§£æå¾Œ content é•·åº¦: {len(result.get('content', ''))}")

            # è¨˜éŒ„ä½¿ç”¨çš„æ¨¡æ¿å’Œ prompt
            result['template_id'] = template.get('id')
            result['prompt_system_used'] = system_prompt
            result['prompt_user_used'] = user_prompt

            return result

        except Exception as e:
            logger.error(f"GPTå…§å®¹ç”Ÿæˆå¤±æ•—: {e}")
            kol_persona = kol_profile.get('persona', 'mixed')
            return self._fallback_generation(stock_id, stock_name, kol_persona)

    def _load_prompt_template(self, posting_type: str, template_id: Optional[int] = None, db_connection = None) -> Dict[str, Any]:
        """è¼‰å…¥ Prompt æ¨¡æ¿

        å„ªå…ˆç´šï¼š
        1. æŒ‡å®š template_id â†’ å¾è³‡æ–™åº«è¼‰å…¥
        2. é è¨­æ¨¡æ¿ â†’ å¾è³‡æ–™åº«è¼‰å…¥ (posting_type + is_default=TRUE)
        3. Fallback â†’ ä½¿ç”¨ç¡¬ç·¨ç¢¼é è¨­æ¨¡æ¿
        """

        # TODO: å¯¦ä½œè³‡æ–™åº«æŸ¥è©¢ï¼ˆç•¶ db_connection å¯ç”¨æ™‚ï¼‰
        # if db_connection and template_id:
        #     return db_connection.fetchone("SELECT * FROM prompt_templates WHERE id = %s", (template_id,))
        # elif db_connection:
        #     return db_connection.fetchone("""
        #         SELECT * FROM prompt_templates
        #         WHERE posting_type = %s AND is_default = TRUE AND is_active = TRUE
        #         ORDER BY performance_score DESC LIMIT 1
        #     """, (posting_type,))

        # ğŸ”¥ Fallback: ç¡¬ç·¨ç¢¼é è¨­æ¨¡æ¿ï¼ˆèˆ‡è³‡æ–™åº«SQLä¸­çš„ä¸€è‡´ï¼‰
        default_templates = {
            'analysis': {
                'id': None,
                'name': 'é è¨­æ·±åº¦åˆ†ææ¨¡æ¿',
                'posting_type': 'analysis',
                'system_prompt_template': '''ä½ æ˜¯ {kol_nickname}ï¼Œä¸€ä½{persona_name}é¢¨æ ¼çš„è‚¡ç¥¨åˆ†æå¸«ã€‚

ã€è§’è‰²è¨­å®šã€‘
{prompt_persona}

ã€å¯«ä½œé¢¨æ ¼ã€‘
{writing_style}

ã€å…§å®¹è­·æ¬„ã€‘
{prompt_guardrails}

ä½ çš„ç›®æ¨™æ˜¯æä¾›å°ˆæ¥­ã€æ·±å…¥çš„è‚¡ç¥¨åˆ†æï¼ŒåŒ…å«æŠ€è¡“é¢ã€åŸºæœ¬é¢ã€å¸‚å ´æƒ…ç·’ç­‰å¤šè§’åº¦è§€é»ã€‚

è«‹å±•ç¾ä½ çš„ç¨ç‰¹åˆ†æé¢¨æ ¼ï¼Œç”¨ä½ ç¿’æ…£çš„æ–¹å¼è¡¨é”è§€é»ã€‚

ğŸ”¥ é‡è¦åŸå‰‡ï¼š
- å¦‚æœæœ‰æä¾›å³æ™‚è‚¡åƒ¹æ•¸æ“šï¼Œè¦è‡ªç„¶åœ°èå…¥æ–‡ç« æ•˜è¿°ä¸­ï¼ˆä¾‹å¦‚ï¼š"å°ç©é›»ä»Šæ—¥æ”¶åœ¨1465å…ƒï¼Œä¸Šæ¼²2.3%"ï¼‰
- ä¸è¦æŠŠè‚¡åƒ¹æ•¸æ“šç•¶æˆåˆ—è¡¨å‘ˆç¾ï¼Œè¦åƒèªªæ•…äº‹ä¸€æ¨£è‡ªç„¶æåˆ°
- è‚¡åƒ¹åªæ˜¯åˆ†æçš„ä¸€éƒ¨åˆ†ï¼Œé‡é»æ˜¯ä½ çš„è§€é»å’Œè¦‹è§£

ğŸ”¥ æ ¼å¼è¦æ±‚ï¼š
- ä¸è¦ä½¿ç”¨ Markdown æ ¼å¼ç¬¦è™Ÿï¼ˆä¸è¦ç”¨ #, ##, ###, **, __ ç­‰ï¼‰
- ä½¿ç”¨ç´”æ–‡æœ¬æ ¼å¼ï¼Œè‡ªç„¶åˆ†æ®µ
- å¯ä»¥ä½¿ç”¨ä¸­æ–‡æ¨™é»ç¬¦è™Ÿï¼ˆï¼šã€ã€‚ã€ï¼ã€ï¼Ÿï¼‰ä¾†çµ„ç¹”å…§å®¹''',
                'user_prompt_template': '''æˆ‘æƒ³äº†è§£ {stock_name}({stock_id}) æœ€è¿‘çš„è¡¨ç¾å’ŒæŠ•è³‡æ©Ÿæœƒã€‚

ã€èƒŒæ™¯ã€‘{trigger_description}

ã€å¸‚å ´æ•¸æ“šã€‘
{news_summary}{ohlc_summary}{tech_summary}{dtno_summary}
è«‹åˆ†æé€™æª”è‚¡ç¥¨ï¼ŒåŒ…å«ï¼š
1. ç‚ºä»€éº¼å€¼å¾—é—œæ³¨
2. ä½ çš„å°ˆæ¥­çœ‹æ³•
3. æ½›åœ¨æ©Ÿæœƒå’Œé¢¨éšª

ğŸ”¥ é‡è¦æ ¼å¼è¦æ±‚ï¼š
- ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œï¼Œå¿…é ˆç²¾ç°¡ï¼ˆé™åˆ¶ 15 å­—ä»¥å…§ï¼‰
- æ¨™é¡Œç¯„ä¾‹ï¼šã€Œåº·èˆ’æ¼²åœåˆ†æã€ã€Œå°ç©é›»å±•æœ›ã€ã€Œè¯ç™¼ç§‘è§€å¯Ÿã€
- âš ï¸ æ¨™é¡Œè¶…é 15 å­—æœƒè¢«æˆªæ–·
{price_instruction}- å…§å®¹é•·åº¦ï¼šç´„ {max_words} å­—ï¼Œæä¾›æ·±å…¥åˆ†æ'''
            },
            'interaction': {
                'id': None,
                'name': 'é è¨­äº’å‹•æå•æ¨¡æ¿',
                'posting_type': 'interaction',
                'system_prompt_template': '''ä½ æ˜¯ {kol_nickname}ï¼Œä¸€ä½{persona_name}é¢¨æ ¼çš„è‚¡ç¥¨åˆ†æå¸«ã€‚

ã€è§’è‰²è¨­å®šã€‘
{prompt_persona}

ã€å¯«ä½œé¢¨æ ¼ã€‘
{writing_style}

ã€å…§å®¹è­·æ¬„ã€‘
{prompt_guardrails}

ä½ çš„ç›®æ¨™æ˜¯èˆ‡è®€è€…äº’å‹•ï¼Œæå‡ºå¼•ç™¼æ€è€ƒçš„å•é¡Œï¼Œé¼“å‹µè¨è«–ã€‚ä¾‹å¦‚ï¼šã€Œä½ è¦ºå¾—é€™æª”è‚¡ç¥¨ç¾åœ¨é©åˆé€²å ´å—ï¼Ÿç•™è¨€åˆ†äº«ä½ çš„çœ‹æ³•ï¼ã€å…§å®¹è¦ç°¡çŸ­æœ‰åŠ›ã€‚

è«‹å±•ç¾ä½ çš„ç¨ç‰¹é¢¨æ ¼ï¼Œç”¨ä½ ç¿’æ…£çš„æ–¹å¼æå•ã€‚

ğŸ”¥ é‡è¦åŸå‰‡ï¼š
- å¦‚æœæœ‰å³æ™‚è‚¡åƒ¹æ•¸æ“šï¼Œåœ¨æè¿°å¸‚æ³æ™‚è‡ªç„¶æåˆ°ï¼ˆä¾‹å¦‚ï¼š"çœ‹åˆ°å°ç©é›»ä»Šå¤©æ¼²äº†2.3%åˆ°1465å…ƒ"ï¼‰
- ç”¨å°è©±çš„æ–¹å¼èå…¥è‚¡åƒ¹ï¼Œä¸è¦ç¡¬æ¢†æ¢†åœ°åˆ—å‡ºæ•¸å­—
- é‡é»æ˜¯å¼•ç™¼è¨è«–ï¼Œä¸æ˜¯å ±åƒ¹

ğŸ”¥ æ ¼å¼è¦æ±‚ï¼š
- ä¸è¦ä½¿ç”¨ Markdown æ ¼å¼ç¬¦è™Ÿï¼ˆä¸è¦ç”¨ #, ##, ###, **, __ ç­‰ï¼‰
- ä½¿ç”¨ç´”æ–‡æœ¬æ ¼å¼ï¼Œè‡ªç„¶åˆ†æ®µ
- å¯ä»¥ä½¿ç”¨ä¸­æ–‡æ¨™é»ç¬¦è™Ÿï¼ˆï¼šã€ã€‚ã€ï¼ã€ï¼Ÿï¼‰ä¾†çµ„ç¹”å…§å®¹''',
                'user_prompt_template': '''æˆ‘æƒ³äº†è§£ {stock_name}({stock_id}) æœ€è¿‘çš„è¡¨ç¾ã€‚

ã€èƒŒæ™¯ã€‘{trigger_description}

ã€å¸‚å ´æ•¸æ“šã€‘
{news_summary}{ohlc_summary}{dtno_summary}
è«‹é‡å°é€™æª”è‚¡ç¥¨æå‡ºä¸€å€‹å¼•ç™¼è¨è«–çš„å•é¡Œï¼Œé¼“å‹µè®€è€…åˆ†äº«çœ‹æ³•ã€‚

è¦æ±‚ï¼š
- ğŸ”¥ ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œï¼Œå¿…é ˆç²¾ç°¡ï¼ˆé™åˆ¶ 15 å­—ä»¥å…§ï¼‰
- æ¨™é¡Œç¯„ä¾‹ï¼šã€Œåº·èˆ’æ€éº¼çœ‹ï¼Ÿã€ã€Œå°ç©é›»é€²å ´ï¼Ÿã€
- âš ï¸ æ¨™é¡Œè¶…é 15 å­—æœƒè¢«æˆªæ–·
{price_instruction}- å…§å®¹é•·åº¦ï¼šç´„ {max_words} å­—
- æå‡ºå–®ä¸€æ ¸å¿ƒå•é¡Œ
- å¼•ç™¼è®€è€…æ€è€ƒå’Œäº’å‹•'''
            },
            'personalized': {
                'id': None,
                'name': 'é è¨­å€‹æ€§åŒ–é¢¨æ ¼æ¨¡æ¿',
                'posting_type': 'personalized',
                'system_prompt_template': '''ä½ æ˜¯ {kol_nickname}ï¼Œä¸€ä½{persona_name}é¢¨æ ¼çš„è‚¡ç¥¨åˆ†æå¸«ã€‚

ã€è§’è‰²è¨­å®šã€‘
{prompt_persona}

ã€å¯«ä½œé¢¨æ ¼ã€‘
{writing_style}

ã€å…§å®¹è­·æ¬„ã€‘
{prompt_guardrails}

ã€å…§å®¹éª¨æ¶åƒè€ƒã€‘
{prompt_skeleton}

ä½ çš„ç›®æ¨™æ˜¯å±•ç¾ä½ ç¨ç‰¹çš„å€‹äººé¢¨æ ¼å’Œè§€é»ï¼Œè®“è®€è€…æ„Ÿå—åˆ°ä½ çš„å€‹æ€§å’Œå°ˆæ¥­ã€‚

è«‹å……åˆ†ç™¼æ®ä½ çš„å€‹äººç‰¹è‰²ï¼Œç”¨ä½ æœ€è‡ªç„¶ã€æœ€èˆ’æœçš„æ–¹å¼è¡¨é”ã€‚

ğŸ”¥ é‡è¦åŸå‰‡ï¼š
- å¦‚æœæœ‰å³æ™‚è‚¡åƒ¹ï¼Œç”¨ä½ å€‹äººçš„æ–¹å¼æåˆ°ï¼ˆä¾‹å¦‚ï¼š"å‰›çœ‹äº†ä¸€ä¸‹ï¼Œç¾åœ¨1465å…ƒï¼Œæ¼²äº†2.3%ï¼Œä¸éŒ¯å•Š"ï¼‰
- æŠŠè‚¡åƒ¹ç•¶æˆä½ åˆ†æçš„ç´ æï¼Œä¸æ˜¯è¦èƒŒèª¦çš„æ•¸æ“š
- å±•ç¾ä½ çš„å€‹æ€§ï¼Œè‚¡åƒ¹åªæ˜¯ä½ è§€é»çš„ä½è­‰

ğŸ”¥ æ ¼å¼è¦æ±‚ï¼š
- ä¸è¦ä½¿ç”¨ Markdown æ ¼å¼ç¬¦è™Ÿï¼ˆä¸è¦ç”¨ #, ##, ###, **, __ ç­‰ï¼‰
- ä½¿ç”¨ç´”æ–‡æœ¬æ ¼å¼ï¼Œè‡ªç„¶åˆ†æ®µ
- å¯ä»¥ä½¿ç”¨ä¸­æ–‡æ¨™é»ç¬¦è™Ÿï¼ˆï¼šã€ã€‚ã€ï¼ã€ï¼Ÿï¼‰ä¾†çµ„ç¹”å…§å®¹''',
                'user_prompt_template': '''æˆ‘æƒ³äº†è§£ {stock_name}({stock_id}) æœ€è¿‘çš„è¡¨ç¾å’ŒæŠ•è³‡æ©Ÿæœƒã€‚

ã€èƒŒæ™¯ã€‘{trigger_description}

ã€å¸‚å ´æ•¸æ“šã€‘
{news_summary}{ohlc_summary}{tech_summary}{dtno_summary}
è«‹ç”¨ä½ ç¨ç‰¹çš„é¢¨æ ¼åˆ†æé€™æª”è‚¡ç¥¨ï¼Œå±•ç¾ä½ çš„å€‹æ€§å’Œå°ˆæ¥­ã€‚

è¦æ±‚ï¼š
- ğŸ”¥ ç¬¬ä¸€è¡Œæ˜¯æ¨™é¡Œï¼Œå¿…é ˆç²¾ç°¡ï¼ˆé™åˆ¶ 15 å­—ä»¥å…§ï¼‰
- æ¨™é¡Œç¯„ä¾‹ï¼šã€Œåº·èˆ’çœ‹æ³•ã€ã€Œå°ç©é›»ç­†è¨˜ã€
- âš ï¸ æ¨™é¡Œè¶…é 15 å­—æœƒè¢«æˆªæ–·
{price_instruction}- ç›®æ¨™é•·åº¦ï¼šç´„ {max_words} å­—ï¼Œæä¾›æ·±å…¥åˆ†æ
- å……åˆ†å±•ç¾ä½ çš„å€‹äººé¢¨æ ¼
- ç”¨ä½ ç¿’æ…£çš„æ–¹å¼çµ„ç¹”å…§å®¹'''
            }
        }

        template = default_templates.get(posting_type, default_templates['analysis'])
        logger.info(f"ğŸ“‹ è¼‰å…¥æ¨¡æ¿: {template['name']} (posting_type={posting_type})")
        return template

    def _prepare_template_parameters(self,
                                     kol_profile: Dict[str, Any],
                                     stock_id: str,
                                     stock_name: str,
                                     trigger_type: str,
                                     serper_analysis: Dict[str, Any],
                                     realtime_price_data: Optional[Dict[str, Any]],
                                     ohlc_data: Optional[Dict[str, Any]],
                                     technical_indicators: Optional[Dict[str, Any]],
                                     dtno_data: Optional[Dict[str, Any]],
                                     max_words: int) -> Dict[str, Any]:
        """æº–å‚™æ¨¡æ¿åƒæ•¸"""

        # åŸºæœ¬åƒæ•¸
        params = {
            'kol_nickname': kol_profile.get('nickname', 'è‚¡å¸‚åˆ†æå¸«'),
            'persona_name': self._get_persona_name(kol_profile.get('persona', 'mixed')),
            'writing_style': kol_profile.get('writing_style', 'è«‹ç”¨ä½ çš„å°ˆæ¥­é¢¨æ ¼åˆ†æè‚¡ç¥¨ã€‚'),
            # ğŸ”¥ NEW: å®Œæ•´çš„ KOL Prompt è¨­å®š
            'prompt_persona': kol_profile.get('prompt_persona', ''),
            'prompt_guardrails': kol_profile.get('prompt_guardrails', ''),
            'prompt_skeleton': kol_profile.get('prompt_skeleton', ''),
            'stock_id': stock_id,
            'stock_name': stock_name,
            'trigger_description': self._get_trigger_description(trigger_type),
            'max_words': max_words,
        }

        # æ–°èæ‘˜è¦
        news_items = serper_analysis.get('news_items', [])
        if news_items:
            news_summary = "è¿‘æœŸç›¸é—œæ–°èï¼š\n"
            for i, news in enumerate(news_items[:5], 1):
                title = news.get('title', '')
                snippet = news.get('snippet', '')
                news_summary += f"{i}. {title}\n"
                if snippet:
                    news_summary += f"   {snippet}\n"
            news_summary += "\n"
            params['news_summary'] = news_summary
        else:
            params['news_summary'] = ''

        # ğŸ”¥ NEW: å³æ™‚è‚¡åƒ¹è³‡è¨Š (å„ªå…ˆä½¿ç”¨ CMoney å³æ™‚æ•¸æ“š)
        if realtime_price_data and realtime_price_data.get('is_realtime'):
            current_price = realtime_price_data.get('current_price', 'N/A')
            price_change = realtime_price_data.get('price_change', 0)
            price_change_pct = realtime_price_data.get('price_change_pct', 0)
            volume = realtime_price_data.get('volume', 'N/A')
            high_price = realtime_price_data.get('high_price', 'N/A')
            low_price = realtime_price_data.get('low_price', 'N/A')
            timestamp = realtime_price_data.get('timestamp', '')

            # æ ¼å¼åŒ–æ¼²è·Œå¹…
            change_sign = '+' if price_change >= 0 else ''
            change_str = f"{change_sign}{price_change:.2f} ({change_sign}{price_change_pct:.2f}%)"

            # ğŸ”¥ NEW APPROACH: Provide context instead of pre-formatted text
            # Let GPT naturally integrate the price info into narrative
            # ğŸ”¥ FIX: Use format_price() to remove unnecessary decimal (.0)
            params['ohlc_summary'] = f"""ã€åƒè€ƒæ•¸æ“š - è«‹è‡ªç„¶èå…¥æ–‡ç« ä¸­ï¼Œä¸è¦ç›´æ¥åˆ—å‡ºã€‘
æ™‚é–“: {timestamp}
ç•¶å‰è‚¡åƒ¹: {format_price(current_price)} å…ƒ
æ¼²è·Œ: {change_str}
é–‹ç›¤: {format_price(realtime_price_data.get('open_price', 'N/A'))} å…ƒ
æœ€é«˜: {format_price(high_price)} å…ƒ
æœ€ä½: {format_price(low_price)} å…ƒ
æˆäº¤é‡: {volume:,} å¼µ

"""
            # æ”¯æ´åµŒå¥—åƒæ•¸ {price.current}, {price.change_pct}
            params['price'] = realtime_price_data
            params['has_realtime_price'] = True
            # ğŸ”¥ UPDATED: Natural integration instruction
            params['price_instruction'] = '- åœ¨æ–‡ç« é–‹é ­è‡ªç„¶åœ°æåˆ°ç•¶å‰è‚¡åƒ¹å’Œæ¼²è·Œæƒ…æ³ï¼ˆç”¨æ•˜è¿°æ–¹å¼ï¼Œä¸è¦åˆ—é»ï¼‰\n'
        # Fallback: OHLC æ‘˜è¦
        elif ohlc_data:
            close_price = ohlc_data.get('close', 'N/A')
            change_pct = ohlc_data.get('change_percent', 'N/A')
            volume = ohlc_data.get('volume', 'N/A')
            # ğŸ”¥ FIX: Use format_price() to remove unnecessary decimal (.0)
            params['ohlc_summary'] = f"""åƒ¹æ ¼è³‡è¨Šï¼š
- æ”¶ç›¤åƒ¹ï¼š{format_price(close_price)}
- æ¼²è·Œå¹…ï¼š{change_pct}%
- æˆäº¤é‡ï¼š{volume}

"""
            # æ”¯æ´åµŒå¥—åƒæ•¸ {ohlc.close}
            params['ohlc'] = ohlc_data
            params['has_realtime_price'] = False
            # No price instruction for historical data
            params['price_instruction'] = ''
        else:
            params['ohlc_summary'] = ''
            params['ohlc'] = {}
            params['price'] = {}
            params['has_realtime_price'] = False
            # No price instruction when no price data
            params['price_instruction'] = ''

        # æŠ€è¡“æŒ‡æ¨™æ‘˜è¦
        if technical_indicators:
            tech_summary = "æŠ€è¡“æŒ‡æ¨™ï¼š\n"
            for key, value in technical_indicators.items():
                tech_summary += f"- {key}: {value}\n"
            tech_summary += "\n"
            params['tech_summary'] = tech_summary
            # æ”¯æ´åµŒå¥—åƒæ•¸ {tech.RSI}
            params['tech'] = technical_indicators
        else:
            params['tech_summary'] = ''
            params['tech'] = {}

        # ğŸ”¥ NEW: DTNO æ•¸æ“šæ‘˜è¦ (åŸºæœ¬é¢/æŠ€è¡“é¢/ç±Œç¢¼é¢)
        if dtno_data:
            dtno_summary = self._format_dtno_summary(dtno_data)
            params['dtno_summary'] = dtno_summary
            params['dtno'] = dtno_data
            params['has_dtno_data'] = True
            logger.info(f"ğŸ“Š DTNO æ•¸æ“šå·²æ³¨å…¥: {len(dtno_data)} å€‹åˆ†é¡")
        else:
            params['dtno_summary'] = ''
            params['dtno'] = {}
            params['has_dtno_data'] = False

        # æ–°èåˆ—è¡¨ï¼ˆæ”¯æ´ {news[0].title}ï¼‰
        params['news'] = news_items

        return params

    def _format_dtno_summary(self, dtno_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ– DTNO æ•¸æ“šç‚º prompt æ‘˜è¦"""
        if not dtno_data:
            return ''

        # åˆ†é¡åç¨±å°ç…§ (30å¼µ DTNO è¡¨ + åˆ¥å)
        sub_cat_names = {
            # åŸºæœ¬é¢ (9å¼µ)
            'revenue_stats': 'æœˆç‡Ÿæ”¶çµ±è¨ˆ',
            'revenue': 'æœˆç‡Ÿæ”¶è©³ç´°',
            'eps': 'è²¡å ±æ‘˜è¦',
            'profitability': 'ç²åˆ©èƒ½åŠ›',
            'eps_estimate': 'æ©Ÿæ§‹é ä¼°EPS',
            'quarterly_earnings': 'å­£ç›ˆé¤˜è‡ªçµ',
            'financial_health': 'IFRSå¹´è²¡å ±',
            'dividend': 'è‚¡åˆ©æ”¿ç­–',
            'analyst_rating': 'æ©Ÿæ§‹è©•ç­‰',
            # æŠ€è¡“é¢ (8å¼µ)
            'daily_close': 'æ—¥æ”¶ç›¤è¡¨',
            'prediction': 'é æ¸¬ä¸»è¦è‚¡',
            'daily_kline': 'æ—¥Kç·š',
            'ma': 'å‡ç·šç³»çµ±',
            'momentum': 'å ±é…¬ç‡å‹•èƒ½',
            'yearly_stats': 'å¹´åº¦çµ±è¨ˆ',
            'technical': 'æŠ€è¡“æŒ‡æ¨™',
            'industry': 'ç”¢æ¥­æ¨™çš„',
            # æŠ€è¡“é¢åˆ¥å
            'kd': 'KDæŒ‡æ¨™',
            'rsi': 'RSIæŒ‡æ¨™',
            'macd': 'MACDæŒ‡æ¨™',
            'bias': 'ä¹–é›¢ç‡',
            'volatility': 'æ³¢å‹•ç‡',
            # ç±Œç¢¼é¢ (14å¼µ)
            'institutional': 'ä¸‰å¤§æ³•äºº',
            'foreign_detail': 'å¤–è³‡è©³ç´°',
            'trust_detail': 'æŠ•ä¿¡è©³ç´°',
            'dealer_detail': 'è‡ªç‡Ÿå•†è©³ç´°',
            'broker_top1': 'Top1åˆ¸å•†',
            'broker_top5': 'Top5åˆ¸å•†',
            'broker_top10': 'Top10åˆ¸å•†',
            'broker_top15': 'Top15åˆ¸å•†',
            'broker_daily_top15': 'æ¯æ—¥Top15åˆ¸å•†',
            'winner_loser': 'è´å®¶è¼¸å®¶çµ±è¨ˆ',
            'major_select': 'åˆ†é»ä¸»åŠ›é¸è‚¡',
            'major_daily': 'æ—¥ä¸»åŠ›è²·è¶…',
            'major_trading': 'ä¸»åŠ›è²·è¶…çµ±è¨ˆ',
            'broker_analysis': 'åˆ†é»ç±Œç¢¼åˆ†æ',
            # ç±Œç¢¼é¢åˆ¥å
            'concentration': 'ç±Œç¢¼é›†ä¸­åº¦',
            'broker': 'åˆ¸å•†åˆ†é»',
            'major_streak': 'ä¸»åŠ›é€£çºŒè²·è³£',
        }

        lines = ["\nã€DTNO æ•¸æ“šåˆ†æè³‡æ–™ - è«‹èå…¥æ–‡ç« åˆ†æä¸­ã€‘\n"]

        for sub_cat, data in dtno_data.items():
            if not data or not data.get('data'):
                continue

            titles = data.get('titles', [])
            rows = data.get('data', [])
            display_name = sub_cat_names.get(sub_cat, sub_cat)

            lines.append(f"\n### {display_name}")

            # åªå–æœ€æ–°ä¸€ç­†è³‡æ–™
            if rows:
                latest_row = rows[0]
                # è·³éå‰å¹¾å€‹ meta columns (æ—¥æœŸã€ä»£è™Ÿã€åç¨±)
                for i, title in enumerate(titles):
                    if i < 4:  # è·³é date, time, code, name
                        continue
                    if i >= len(latest_row):
                        break

                    value = latest_row[i]
                    if value is not None and value != '':
                        try:
                            num_val = float(value)

                            # ğŸ”¥ FIX: Handle unit conversion based on title
                            # If title contains "åƒå…ƒ", multiply by 1000 to get actual å…ƒ value
                            # If title contains "ç™¾è¬", multiply by 1000000
                            display_title = title
                            if '(åƒå…ƒ)' in title or 'ï¼ˆåƒå…ƒï¼‰' in title:
                                num_val = num_val * 1000  # Convert åƒå…ƒ to å…ƒ
                                display_title = title.replace('(åƒå…ƒ)', '').replace('ï¼ˆåƒå…ƒï¼‰', '').strip()
                            elif '(ç™¾è¬)' in title or 'ï¼ˆç™¾è¬ï¼‰' in title:
                                num_val = num_val * 1000000  # Convert ç™¾è¬ to å…ƒ
                                display_title = title.replace('(ç™¾è¬)', '').replace('ï¼ˆç™¾è¬ï¼‰', '').strip()

                            # ğŸ”¥ FIX: Use Chinese units (è¬ã€å„„) and format cleanly
                            if abs(num_val) >= 100000000:  # 1å„„ä»¥ä¸Š
                                formatted = f"{num_val/100000000:.2f}å„„"
                            elif abs(num_val) >= 10000:  # 1è¬ä»¥ä¸Š
                                formatted = f"{num_val/10000:.2f}è¬"
                            elif num_val == int(num_val):  # æ•´æ•¸ (å¦‚è‚¡åƒ¹ 201.0 â†’ 201)
                                formatted = f"{int(num_val)}"
                            else:
                                formatted = f"{num_val:.2f}"
                            lines.append(f"- {display_title}: {formatted}")
                        except (ValueError, TypeError):
                            lines.append(f"- {title}: {value}")

        return "\n".join(lines)

    def _inject_parameters(self, template: str, params: Dict[str, Any]) -> str:
        """æ³¨å…¥åƒæ•¸åˆ°æ¨¡æ¿

        æ”¯æ´ï¼š
        - ç°¡å–®è®Šæ•¸ï¼š{kol_nickname}, {stock_id}
        - åµŒå¥—è®Šæ•¸ï¼š{ohlc.close}, {tech.RSI}
        - é™£åˆ—ç´¢å¼•ï¼š{news[0].title}
        """

        result = template

        # è™•ç†ç°¡å–®è®Šæ•¸å’ŒåµŒå¥—è®Šæ•¸
        for key, value in params.items():
            if isinstance(value, dict):
                # è™•ç†åµŒå¥—åƒæ•¸ {ohlc.close}
                for sub_key, sub_value in value.items():
                    pattern = f"{{{key}.{sub_key}}}"
                    result = result.replace(pattern, str(sub_value))
            elif isinstance(value, list):
                # è™•ç†é™£åˆ—ç´¢å¼• {news[0].title}
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        for item_key, item_value in item.items():
                            pattern = f"{{{key}[{i}].{item_key}}}"
                            result = result.replace(pattern, str(item_value))
            else:
                # è™•ç†ç°¡å–®è®Šæ•¸ {kol_nickname}
                pattern = f"{{{key}}}"
                result = result.replace(pattern, str(value))

        return result

    def _build_system_prompt(self, kol_profile: Dict[str, Any]) -> str:
        """æ§‹å»º System Prompt - å®šç¾© KOL è§’è‰²å’Œé¢¨æ ¼"""

        nickname = kol_profile.get('nickname', 'è‚¡å¸‚åˆ†æå¸«')
        persona = kol_profile.get('persona', 'mixed')
        writing_style = kol_profile.get('writing_style', '')

        # ğŸ¯ ç°¡æ½”çš„è§’è‰²å®šç¾©ï¼Œä¸åŠ é™åˆ¶
        persona_name = self._get_persona_name(persona)

        system_prompt = f"""ä½ æ˜¯ {nickname}ï¼Œä¸€ä½{persona_name}é¢¨æ ¼çš„è‚¡ç¥¨åˆ†æå¸«ã€‚

{writing_style if writing_style else 'è«‹ç”¨ä½ çš„å°ˆæ¥­é¢¨æ ¼åˆ†æè‚¡ç¥¨ã€‚'}

è«‹å±•ç¾ä½ çš„ç¨ç‰¹åˆ†æé¢¨æ ¼ï¼Œç”¨ä½ ç¿’æ…£çš„æ–¹å¼è¡¨é”è§€é»ã€‚"""

        return system_prompt

    def _build_user_prompt(self,
                          stock_id: str,
                          stock_name: str,
                          trigger_type: str,
                          serper_analysis: Dict[str, Any],
                          ohlc_data: Optional[Dict[str, Any]],
                          technical_indicators: Optional[Dict[str, Any]],
                          max_words: int) -> str:
        """æ§‹å»º User Prompt - æ•´åˆæ‰€æœ‰æ•¸æ“šï¼ˆå°è©±å¼ï¼‰"""

        # ğŸ¯ è§¸ç™¼å™¨ä¸Šä¸‹æ–‡
        trigger_desc = self._get_trigger_description(trigger_type)

        # ğŸ¯ æ–°è summaryï¼ˆæ°¸é è™•ç†ï¼ŒSerper API æ°¸é æœƒè·‘ï¼‰
        news_summary = ""
        news_items = serper_analysis.get('news_items', [])
        if news_items:
            news_summary = "è¿‘æœŸç›¸é—œæ–°èï¼š\n"
            for i, news in enumerate(news_items[:5], 1):
                title = news.get('title', '')
                snippet = news.get('snippet', '')
                news_summary += f"{i}. {title}\n"
                if snippet:
                    news_summary += f"   {snippet}\n"
            news_summary += "\n"

        # ğŸ¯ OHLCï¼ˆç©ºå€¼ç”¨ ''ï¼Œä¸è£œå……èªªæ˜æ–‡å­—ï¼‰
        ohlc_summary = ""
        if ohlc_data:
            close_price = ohlc_data.get('close', 'N/A')
            change_pct = ohlc_data.get('change_percent', 'N/A')
            volume = ohlc_data.get('volume', 'N/A')
            ohlc_summary = f"""åƒ¹æ ¼è³‡è¨Šï¼š
- æ”¶ç›¤åƒ¹ï¼š{close_price}
- æ¼²è·Œå¹…ï¼š{change_pct}%
- æˆäº¤é‡ï¼š{volume}

"""

        # ğŸ¯ æŠ€è¡“æŒ‡æ¨™ï¼ˆç©ºå€¼ç”¨ ''ï¼Œä¸è£œå……èªªæ˜æ–‡å­—ï¼‰
        tech_summary = ""
        if technical_indicators:
            tech_summary = "æŠ€è¡“æŒ‡æ¨™ï¼š\n"
            for key, value in technical_indicators.items():
                tech_summary += f"- {key}: {value}\n"
            tech_summary += "\n"

        # ğŸ¯ çµ„åˆæ•¸æ“šå€å¡Š
        data_section = news_summary + ohlc_summary + tech_summary

        # ğŸ¯ å°è©±å¼ User Promptï¼ˆæ›´è‡ªç„¶ã€ä¸å¼·åˆ¶çµæ§‹ï¼‰
        user_prompt = f"""è«‹åˆ†æ {stock_name}({stock_id}) çš„æŠ•è³‡åƒ¹å€¼ã€‚

èƒŒæ™¯ï¼š{trigger_desc}

ç›¸é—œè³‡è¨Šï¼š
{data_section}
ç”¨ä½ çš„å°ˆæ¥­è§’åº¦åˆ†æé€™æª”è‚¡ç¥¨ï¼ŒåŒ…æ‹¬å€¼å¾—é—œæ³¨çš„é‡é»ã€ä½ çš„çœ‹æ³•ã€ä»¥åŠæŠ•è³‡äººæ‡‰è©²æ³¨æ„çš„æ©Ÿæœƒèˆ‡é¢¨éšªã€‚

è«‹ç”¨è‡ªç„¶æµæš¢çš„æ–¹å¼è¡¨é”ï¼Œä¸éœ€è¦å›ºå®šæ ¼å¼ï¼Œç´„ {max_words} å­—ã€‚"""

        return user_prompt

    def _get_trigger_description(self, trigger_type: str) -> str:
        """ç²å–è§¸ç™¼å™¨æè¿°"""
        descriptions = {
            'limit_up_after_hours': 'é€™æ˜¯ä»Šæ—¥ç›¤å¾Œæ¼²åœçš„è‚¡ç¥¨',
            'intraday_gainers_by_amount': 'é€™æ˜¯ä»Šæ—¥æ¼²å¹…é ˜å…ˆçš„è‚¡ç¥¨',
            'trending_topics': 'é€™æ˜¯ç¤¾ç¾¤ç†±é–€è¨è«–çš„è‚¡ç¥¨',
            'custom_stocks': 'é€™æ˜¯ç‰¹å®šé—œæ³¨çš„è‚¡ç¥¨'
        }
        return descriptions.get(trigger_type, 'é€™æ˜¯éœ€è¦åˆ†æçš„è‚¡ç¥¨')

    def _get_persona_name(self, persona: str) -> str:
        """ç²å–äººè¨­åç¨±"""
        names = {
            'technical': 'æŠ€è¡“åˆ†æ',
            'fundamental': 'åŸºæœ¬é¢åˆ†æ',
            'news_driven': 'æ¶ˆæ¯é¢åˆ†æ',
            'mixed': 'ç¶œåˆåˆ†æ'
        }
        return names.get(persona, 'ç¶œåˆåˆ†æ')

    def _clean_markdown(self, text: str) -> str:
        """æ¸…ç† Markdown æ ¼å¼ç¬¦è™Ÿ

        ç§»é™¤ï¼š
        - ### ## # æ¨™é¡Œç¬¦è™Ÿ
        - ** __ ç²—é«”ç¬¦è™Ÿ
        - * _ æ–œé«”ç¬¦è™Ÿ
        """
        if not text:
            return text

        # ç§»é™¤æ¨™é¡Œç¬¦è™Ÿï¼ˆä¿ç•™å…§å®¹ï¼‰
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)

        # ç§»é™¤ç²—é«”ç¬¦è™Ÿ **text** æˆ– __text__
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        text = re.sub(r'__(.+?)__', r'\1', text)

        # ç§»é™¤æ–œé«”ç¬¦è™Ÿ *text* æˆ– _text_ (ä½†ä¿ç•™å–®ç¨çš„ _ ç”¨æ–¼åˆ†éš”)
        text = re.sub(r'(?<!\w)\*(.+?)\*(?!\w)', r'\1', text)
        text = re.sub(r'(?<!\w)_(.+?)_(?!\w)', r'\1', text)

        return text

    def _parse_gpt_response(self, content: str, stock_id: str, stock_name: str) -> Dict[str, Any]:
        """è§£æGPTå›æ‡‰"""

        # ğŸ”¥ FIX: é˜²ç¦¦æ€§æª¢æŸ¥ - å¦‚æœ content æ˜¯ None æˆ–ç©ºå­—ä¸²
        if not content:
            logger.error(f"âŒ _parse_gpt_response æ”¶åˆ°ç©ºå…§å®¹ï¼Œè¿”å›é è¨­çµæ§‹")
            return {
                "title": f"{stock_name}({stock_id}) å¸‚å ´åˆ†æ",
                "content": f"ã€{stock_name}({stock_id}) å¸‚å ´è§€å¯Ÿã€‘\n\nç›®å‰æš«ç„¡è©³ç´°åˆ†æå…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                "content_md": f"ã€{stock_name}({stock_id}) å¸‚å ´è§€å¯Ÿã€‘\n\nç›®å‰æš«ç„¡è©³ç´°åˆ†æå…§å®¹ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚",
                "commodity_tags": [{"type": "Stock", "key": stock_id, "bullOrBear": 0}],
                "community_topic": None,
                "generation_method": "empty_fallback",
                "model_used": self.model
            }

        # ğŸ”¥ æ¸…ç† Markdown æ ¼å¼ï¼ˆé˜²ç¦¦æ€§ç·¨ç¨‹ï¼šå³ä½¿ GPT ä½¿ç”¨äº† Markdownï¼Œä¹Ÿè¦ç§»é™¤ï¼‰
        content = self._clean_markdown(content)

        # ç°¡å–®çš„å…§å®¹åˆ†å‰²
        lines = content.split('\n')
        title = ""
        main_content = content

        # æå–æ¨™é¡Œï¼ˆç¬¬ä¸€è¡Œéç©ºè¡Œï¼‰
        title_line_index = -1
        for i, line in enumerate(lines):
            if line.strip() and not line.startswith(' '):
                title = line.strip()
                title_line_index = i
                break

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ¨™é¡Œï¼Œä½¿ç”¨é è¨­
        if not title:
            title = f"{stock_name} åˆ†æ"

        # ğŸ”¥ æ¨™é¡Œé•·åº¦æ§åˆ¶ï¼ˆæœ€å¤š 25 å­—ï¼ŒCMoney æ¨™é¡Œé™åˆ¶ç´„ 30 å­—ï¼‰
        MAX_TITLE_LENGTH = 25
        if len(title) > MAX_TITLE_LENGTH:
            logger.warning(f"âš ï¸ æ¨™é¡Œéé•· ({len(title)} å­—)ï¼Œé€²è¡Œæˆªæ–·: {title[:30]}...")

            # ç­–ç•¥ 1: å˜—è©¦åœ¨æ¨™é»ç¬¦è™Ÿè™•æˆªæ–·
            punctuation_marks = ['ï¼Œ', 'ã€', 'ï¼', 'ï¼Ÿ', 'ï¼š', 'ï½œ', '|', '-', 'â€”', ' ']
            truncated = False
            for i in range(MAX_TITLE_LENGTH - 1, 5, -1):  # å¾æœ€å¤§é•·åº¦å¾€å›æ‰¾ï¼Œæœ€å°‘ä¿ç•™ 5 å­—
                if title[i] in punctuation_marks:
                    title = title[:i]
                    truncated = True
                    break

            # ç­–ç•¥ 2: å¦‚æœæ²’æœ‰åˆé©çš„æ¨™é»ï¼Œç›´æ¥æˆªæ–·
            if not truncated and len(title) > MAX_TITLE_LENGTH:
                title = title[:MAX_TITLE_LENGTH]

            logger.info(f"âœ‚ï¸ æˆªæ–·å¾Œæ¨™é¡Œ: {title}")

        # ğŸ”¥ ç§»é™¤å…§å®¹é–‹é ­çš„é‡è¤‡æ¨™é¡Œ
        # å¦‚æœå…§å®¹ä»¥æ¨™é¡Œé–‹é ­ï¼Œå‰‡ç§»é™¤ç¬¬ä¸€è¡Œï¼ˆæ¨™é¡Œè¡Œï¼‰åŠå…¶å¾Œçš„ç©ºè¡Œ
        if title_line_index >= 0:
            # å¾æ¨™é¡Œè¡Œä¹‹å¾Œé–‹å§‹
            content_lines = lines[title_line_index + 1:]

            # è·³éæ¨™é¡Œå¾Œçš„ç©ºè¡Œ
            while content_lines and not content_lines[0].strip():
                content_lines.pop(0)

            # é‡æ–°çµ„åˆå…§å®¹ï¼ˆä¸åŒ…å«æ¨™é¡Œï¼‰
            main_content = '\n'.join(content_lines).strip()

        # å¦‚æœç§»é™¤æ¨™é¡Œå¾Œå…§å®¹ç‚ºç©ºï¼Œä½¿ç”¨åŸå§‹å…§å®¹
        if not main_content:
            main_content = content

        return {
            "title": title,
            "content": main_content,
            "content_md": main_content,
            "commodity_tags": [{"type": "Stock", "key": stock_id, "bullOrBear": 0}],
            "community_topic": None,
            "generation_method": "gpt",
            "model_used": self.model
        }
    
    def _fallback_generation(self, stock_id: str, stock_name: str, kol_persona: str) -> Dict[str, Any]:
        """å›é€€åˆ°æ¨¡æ¿ç”Ÿæˆ"""
        logger.warning(f"ä½¿ç”¨å‚™ç”¨æ¨¡æ¿ç”Ÿæˆå…§å®¹: {stock_name}({stock_id})")

        # æ ¹æ“š KOL è§’è‰²é¸æ“‡ä¸åŒçš„åˆ†æé¢¨æ ¼
        if kol_persona == "technical":
            title = f"{stock_name}({stock_id}) æŠ€è¡“é¢åˆ†æèˆ‡æ“ä½œç­–ç•¥"
            content = f"""ã€{stock_name}({stock_id}) æŠ€è¡“é¢æ·±åº¦åˆ†æã€‘

ä¸€ã€æŠ€è¡“æŒ‡æ¨™åˆ†æ
å¾æŠ€è¡“é¢ä¾†çœ‹ï¼Œ{stock_name}ç›®å‰å‘ˆç¾å€¼å¾—é—œæ³¨çš„è¨Šè™Ÿã€‚RSIæŒ‡æ¨™é¡¯ç¤ºè‚¡åƒ¹å‹•èƒ½è®ŠåŒ–ï¼ŒMACDæŒ‡æ¨™å‰‡åæ˜ çŸ­ä¸­æœŸè¶¨å‹¢èµ°å‘ã€‚æˆäº¤é‡æ–¹é¢ï¼Œè¿‘æœŸé‡èƒ½æœ‰æ‰€æ”¾å¤§ï¼Œé¡¯ç¤ºå¸‚å ´é—œæ³¨åº¦æå‡ã€‚

äºŒã€é—œéµåƒ¹ä½è§€å¯Ÿ
å»ºè­°é—œæ³¨æ”¯æ’èˆ‡å£“åŠ›å€é–“ï¼Œè‹¥èƒ½ç«™ç©©é—œéµåƒ¹ä½ï¼Œå¾ŒçºŒå¯èƒ½æœ‰é€²ä¸€æ­¥è¡¨ç¾ç©ºé–“ã€‚æ“ä½œä¸Šå»ºè­°è¨­å®šåˆç†çš„åœæåœåˆ©é»ã€‚

ä¸‰ã€æ“ä½œå»ºè­°
â€¢ çŸ­ç·šï¼šè§€å¯Ÿçªç ´å¾Œçš„é‡åƒ¹é…åˆ
â€¢ ä¸­ç·šï¼šç•™æ„è¶¨å‹¢æ˜¯å¦å»¶çºŒ
â€¢ é¢¨æ§ï¼šåš´æ ¼åŸ·è¡Œåœæç´€å¾‹

âš ï¸ ä»¥ä¸Šåˆ†æåƒ…ä¾›åƒè€ƒï¼ŒæŠ•è³‡éœ€è¬¹æ…è©•ä¼°é¢¨éšªã€‚

#æŠ€è¡“åˆ†æ #æ“ä½œç­–ç•¥ #{stock_name}"""

        elif kol_persona == "fundamental":
            title = f"{stock_name}({stock_id}) åŸºæœ¬é¢åˆ†æèˆ‡æŠ•è³‡å±•æœ›"
            content = f"""ã€{stock_name}({stock_id}) åŸºæœ¬é¢è§€å¯Ÿã€‘

ä¸€ã€ç”¢æ¥­åœ°ä½
{stock_name}åœ¨ç”¢æ¥­ä¸­å…·æœ‰é‡è¦åœ°ä½ï¼Œç‡Ÿé‹ç‹€æ³å€¼å¾—æŒçºŒè¿½è¹¤ã€‚æŠ•è³‡äººæ‡‰é—œæ³¨å…¬å¸è²¡å ±æ•¸æ“šã€ç‡Ÿæ”¶è¡¨ç¾ï¼Œä»¥åŠç”¢æ¥­æ•´é«”æ™¯æ°£è®ŠåŒ–ã€‚

äºŒã€è²¡å‹™è¡¨ç¾
å»ºè­°é—œæ³¨å…¬å¸çš„ç²åˆ©èƒ½åŠ›ã€æˆé•·æ€§ï¼Œä»¥åŠç¾é‡‘æµç‹€æ³ã€‚åŒæ™‚ç•™æ„ç”¢æ¥­ç«¶çˆ­æ…‹å‹¢èˆ‡å…¬å¸è­·åŸæ²³ã€‚

ä¸‰ã€æŠ•è³‡å»ºè­°
â€¢ é•·æœŸæŠ•è³‡è€…ï¼šè©•ä¼°åŸºæœ¬é¢æ˜¯å¦æ”¯æ’è‚¡åƒ¹
â€¢ åƒ¹å€¼æŠ•è³‡ï¼šé—œæ³¨æœ¬ç›Šæ¯”èˆ‡æ®–åˆ©ç‡
â€¢ é¢¨éšªæ§ç®¡ï¼šåˆ†æ•£æŠ•è³‡é™ä½å–®ä¸€æŒè‚¡é¢¨éšª

âš ï¸ æŠ•è³‡å‰è«‹è©³é–±å…¬å¸è²¡å ±ï¼Œå¯©æ…è©•ä¼°ã€‚

#åŸºæœ¬é¢åˆ†æ #æŠ•è³‡å±•æœ› #{stock_name}"""

        else:  # å…¶ä»–è§’è‰²ä½¿ç”¨é€šç”¨æ¨¡æ¿
            title = f"{stock_name}({stock_id}) å¸‚å ´è§€å¯Ÿèˆ‡äº¤æ˜“æƒ³æ³•"
            content = f"""ã€{stock_name}({stock_id}) å¸‚å ´è§€å¯Ÿã€‘

ä¸€ã€è¿‘æœŸèµ°å‹¢
{stock_name}è¿‘æœŸèµ°å‹¢å€¼å¾—é—œæ³¨ï¼Œå¸‚å ´æ³¢å‹•æä¾›ä¸åŒçš„äº¤æ˜“æ©Ÿæœƒã€‚æŠ•è³‡äººå¯æ ¹æ“šè‡ªèº«é¢¨éšªåå¥½ï¼Œé¸æ“‡é©åˆçš„æ“ä½œç­–ç•¥ã€‚

äºŒã€äº¤æ˜“æƒ³æ³•
â€¢ è¶¨å‹¢è·Ÿéš¨ï¼šé †å‹¢è€Œç‚ºï¼Œä¸é€†å‹¢æ“ä½œ
â€¢ é¢¨éšªç®¡ç†ï¼šæ§åˆ¶å€‰ä½ï¼Œè¨­å®šåœæ
â€¢ æƒ…ç·’ç®¡ç†ï¼šé¿å…è¿½é«˜æ®ºä½

ä¸‰ã€æ³¨æ„äº‹é …
è«‹ç•™æ„æ•´é«”å¸‚å ´ç³»çµ±æ€§é¢¨éšªï¼Œä»¥åŠå€‹è‚¡åŸºæœ¬é¢è®ŠåŒ–ã€‚å»ºè­°è¨­å®šåˆç†çš„åœæåœåˆ©é»ï¼Œåš´æ ¼æ§åˆ¶æŒè‚¡æ¯”é‡ã€‚

âš ï¸ æŠ•è³‡æœ‰é¢¨éšªï¼Œè«‹è¬¹æ…è©•ä¼°ã€‚

#å¸‚å ´è§€å¯Ÿ #äº¤æ˜“ç­–ç•¥ #{stock_name}"""

        return {
            "title": title,
            "content": content,
            "content_md": content,
            "commodity_tags": [{"type": "Stock", "key": stock_id, "bullOrBear": 0}],
            "community_topic": None,
            "generation_method": "template_fallback",
            "model_used": "template"
        }

# å…¨åŸŸå¯¦ä¾‹
gpt_generator = GPTContentGenerator()
