"""
çµ±ä¸€çš„ API æœå‹™ - å®Œæ•´ç‰ˆæœ¬
Railway éƒ¨ç½²æ™‚ä½¿ç”¨æ­¤æœå‹™ä½œç‚ºå”¯ä¸€çš„ API å…¥å£
æ•´åˆæ‰€æœ‰å¾®æœå‹™åŠŸèƒ½åˆ°ä¸€å€‹ API

ğŸ”¥ FORCE REBUILD: 2025-10-20-19:50 - Railway cache bust to deploy latest fixes

Recent Updates:
- 2025-10-20 19:50: FORCE REBUILD - Railway stuck on old cache (abd60928)
  * Log cleanup (2dff92db) - 65% verbosity reduction
  * API key strip (d1fa8165) - Fix trailing newline causing "Invalid header"
  * Stock info passing (3e22a469) - Fix generic fallback content
  * Fallback fix (f016e43d) - Stock names in templates
- 2025-10-20: Fixed posting-service imports with robust multi-strategy path resolution
- 2025-10-20: Enabled alternative_versions generation for ALL posting_types
- 2025-01-18: Fixed Railway healthcheck - added /health endpoint alongside /api/health
- 2025-01-18: Separated 6 intraday triggers into individual GET endpoints
- 2025-01-18: Standardized all API endpoints with /api prefix
"""

from fastapi import FastAPI, Query, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import httpx
import json
import sys
import os
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor
import pytz
import traceback
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import asyncio
import asyncpg  # ğŸ”¥ Add asyncpg for KOL Profile query

# ğŸ”¥ CMoney Real-time Stock Price Service
from services.cmoney_realtime import get_cmoney_service
# ğŸ”¥ DTNO Data Service (åŸºæœ¬é¢/æŠ€è¡“é¢/ç±Œç¢¼é¢)
from services.dtno import get_dtno_service

# Timezone utility - Always use Taipei time (GMT+8)
def get_current_time():
    """Returns current time in Asia/Taipei timezone"""
    return datetime.now(pytz.timezone('Asia/Taipei'))

def convert_post_datetimes_to_taipei(post_dict):
    """
    Convert naive UTC datetime fields in post dictionary to Taipei timezone strings.
    Also parse JSON fields from TEXT to dict/list.

    This fixes the issue where database TIMESTAMP columns (without timezone) are returned
    as naive datetime objects, which get serialized as UTC but displayed incorrectly.

    Args:
        post_dict: Dictionary containing post data from database

    Returns:
        Dictionary with datetime fields converted to Taipei timezone ISO strings
        and JSON fields parsed from TEXT to dict/list
    """
    taipei_tz = pytz.timezone('Asia/Taipei')
    utc_tz = pytz.utc

    # Datetime fields that need conversion
    datetime_fields = ['created_at', 'updated_at', 'published_at', 'scheduled_time']

    result = dict(post_dict)

    # Convert datetime fields
    for field in datetime_fields:
        if field in result and result[field] is not None:
            dt = result[field]

            # If datetime is naive (no tzinfo), assume it's UTC
            if isinstance(dt, datetime) and dt.tzinfo is None:
                # Add UTC timezone
                dt_utc = utc_tz.localize(dt)
                # Convert to Taipei timezone
                dt_taipei = dt_utc.astimezone(taipei_tz)
                # Store as ISO format string
                result[field] = dt_taipei.isoformat()
            elif isinstance(dt, datetime):
                # Already has timezone info, just convert to Taipei
                dt_taipei = dt.astimezone(taipei_tz)
                result[field] = dt_taipei.isoformat()

    # ğŸ”¥ FIX: Parse JSON fields from TEXT to dict/list
    json_fields = ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']

    for field in json_fields:
        if field in result and result[field] is not None:
            if isinstance(result[field], str):
                try:
                    parsed = json.loads(result[field])
                    result[field] = parsed
                    # Removed excessive debug logging (was causing 500 logs/sec Railway limit)
                except Exception as e:
                    logger.error(f"âŒ Failed to parse {field}: {e}")
                    result[field] = None

    # ğŸ”¥ FIX: Rename generation_params to generation_config for frontend compatibility
    if 'generation_params' in result:
        result['generation_config'] = result.pop('generation_params')
        # Removed excessive debug logging (was causing 500 logs/sec Railway limit)

    return result

def _get_sorting_label(stock_sorting: dict) -> str:
    """Generate display label for stock sorting"""
    if not stock_sorting or not stock_sorting.get('method') or stock_sorting.get('method') == 'none':
        return "éš¨æ©Ÿæ’åº"

    method_labels = {
        'price_change_pct': 'æ¼²è·Œå¹…',
        'volume': 'æˆäº¤é‡',
        'five_day_return': 'äº”æ—¥æ¼²å¹…',
        'ten_day_return': 'åæ—¥æ¼²å¹…',
        'twenty_day_return': 'äºŒåæ—¥æ¼²å¹…',
        'market_cap': 'å¸‚å€¼',
        'turnover_rate': 'å‘¨è½‰ç‡'
    }

    method = stock_sorting.get('method', 'none')
    direction = stock_sorting.get('direction', 'desc')

    label = method_labels.get(method, method)
    arrow = 'â†“' if direction == 'desc' else 'â†‘'

    return f"{label}{arrow}"

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ”‡ Suppress verbose external library logging (saves ~10 lines per post)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("openai._base_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ğŸ”¥ Initialize APScheduler for automatic schedule execution
scheduler = AsyncIOScheduler(timezone='Asia/Taipei')

# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="Forum Autoposter Unified API",
    description="çµ±ä¸€çš„ API æœå‹™ï¼Œæ•´åˆæ‰€æœ‰å¾®æœå‹™åŠŸèƒ½ã€‚è¨ªå• /docs æŸ¥çœ‹ Swagger UI æ–‡æª”",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# é…ç½® CORS - å…è¨±æ‰€æœ‰ä¾†æºï¼ˆå› ç‚ºæˆ‘å€‘æœƒç”¨ Vercel Proxyï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# ==================== FinLab åˆå§‹åŒ– ====================

import finlab
from finlab import data
import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor

# ==================== GPT å…§å®¹ç”Ÿæˆå™¨åˆå§‹åŒ– ====================

# æ·»åŠ  posting-service åˆ° Python è·¯å¾‘
# ä½¿ç”¨å¤šç¨®æ–¹å¼ç¢ºä¿æ­£ç¢ºæ‰¾åˆ° posting-service ç›®éŒ„
def setup_posting_service_path():
    """Setup posting-service path with multiple fallback strategies"""
    # Strategy 1: Relative to current file (local development)
    path1 = os.path.join(os.path.dirname(__file__), 'posting-service')

    # Strategy 2: Relative to current working directory
    path2 = os.path.join(os.getcwd(), 'posting-service')

    # Strategy 3: Absolute path (Docker WORKDIR)
    path3 = '/app/posting-service'

    for path in [path1, path2, path3]:
        if os.path.exists(path) and os.path.isdir(path):
            if path not in sys.path:
                sys.path.insert(0, path)
            logger.info(f"ğŸ“ posting-service è·¯å¾‘å·²è¨­ç½®: {path}")
            return path

    logger.error(f"âŒ æ‰¾ä¸åˆ° posting-service ç›®éŒ„! å˜—è©¦çš„è·¯å¾‘: {[path1, path2, path3]}")
    return None

posting_service_path = setup_posting_service_path()

# å°å…¥ GPT å…§å®¹ç”Ÿæˆå™¨
try:
    import openai
    from gpt_content_generator import GPTContentGenerator
    gpt_generator = GPTContentGenerator()
    logger.info("âœ… GPT å…§å®¹ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸  GPT å…§å®¹ç”Ÿæˆå™¨å°å…¥å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")
    gpt_generator = None

# å°å…¥å€‹äººåŒ–æ¨¡çµ„
try:
    from personalization_module import enhanced_personalization_processor
    logger.info("âœ… å€‹äººåŒ–æ¨¡çµ„åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸  å€‹äººåŒ–æ¨¡çµ„å°å…¥å¤±æ•—: {e}ï¼Œå°‡è·³éå€‹äººåŒ–è™•ç†")
    enhanced_personalization_processor = None

# å°å…¥ Serper API æœå‹™
try:
    import sys
    import os
    # Add posting-service directory to path (both possible locations)
    current_dir = os.path.dirname(__file__)
    posting_service_paths = [
        os.path.join(current_dir, 'posting-service'),
        os.path.join(os.path.dirname(current_dir), 'posting-service')
    ]

    for path in posting_service_paths:
        if path not in sys.path and os.path.exists(path):
            sys.path.insert(0, path)
            logger.info(f"ğŸ“ æ·»åŠ è·¯å¾‘åˆ° sys.path: {path}")

    from serper_integration import SerperNewsService
    serper_service = SerperNewsService()
    logger.info("âœ… Serper API æœå‹™åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.warning(f"âš ï¸  Serper API æœå‹™å°å…¥å¤±æ•—: {e}ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
    serper_service = None

stock_mapping = {}
db_pool = None  # Connection pool instead of single connection

# ğŸ”¥ FIX: Parse DATABASE_URL into DB_CONFIG for asyncpg connections
DB_CONFIG = None

# ğŸ”¥ Reaction Bot Service and CMoney Client
reaction_bot_service = None
cmoney_reaction_client = None
asyncpg_pool = None  # AsyncPG pool for reaction bot

def get_db_connection():
    """Get a connection from the pool"""
    if db_pool is None:
        raise Exception("Database pool not initialized")
    return db_pool.getconn()

def return_db_connection(conn):
    """Return a connection to the pool"""
    if db_pool and conn:
        db_pool.putconn(conn)

def create_post_records_table():
    """å‰µå»º post_records è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    conn = None
    try:
        if not db_pool:
            logger.error("âŒ æ•¸æ“šåº«é€£æ¥æ± ä¸å­˜åœ¨ï¼Œç„¡æ³•å‰µå»ºè¡¨")
            return

        conn = get_db_connection()

        with conn.cursor() as cursor:
            logger.info("ğŸ” æª¢æŸ¥ post_records è¡¨æ˜¯å¦å­˜åœ¨...")
            # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'post_records'
                );
            """)
            table_exists = cursor.fetchone()[0]
            logger.info(f"ğŸ“Š è¡¨å­˜åœ¨ç‹€æ…‹: {table_exists}")
            
            if not table_exists:
                logger.info("ğŸ“‹ é–‹å§‹å‰µå»º post_records è¡¨...")
                cursor.execute("""
                    CREATE TABLE post_records (
                        post_id VARCHAR PRIMARY KEY,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        session_id BIGINT,
                        kol_serial INTEGER NOT NULL,
                        kol_nickname VARCHAR NOT NULL,
                        kol_persona VARCHAR,
                        stock_code VARCHAR NOT NULL,
                        stock_name VARCHAR NOT NULL,
                        title VARCHAR NOT NULL,
                        content TEXT NOT NULL,
                        content_md TEXT,
                        status VARCHAR DEFAULT 'draft',
                        reviewer_notes TEXT,
                        approved_by VARCHAR,
                        approved_at TIMESTAMP,
                        scheduled_at TIMESTAMP,
                        published_at TIMESTAMP,
                        cmoney_post_id VARCHAR,
                        cmoney_post_url VARCHAR,
                        publish_error TEXT,
                        views BIGINT DEFAULT 0,
                        likes BIGINT DEFAULT 0,
                        comments BIGINT DEFAULT 0,
                        shares BIGINT DEFAULT 0,
                        topic_id VARCHAR,
                        topic_title VARCHAR,
                        technical_analysis TEXT,
                        serper_data TEXT,
                        quality_score FLOAT,
                        ai_detection_score FLOAT,
                        risk_level VARCHAR,
                        generation_params TEXT,
                        commodity_tags TEXT,
                        alternative_versions TEXT
                    );
                """)
                conn.commit()
                logger.info("âœ… post_records è¡¨å‰µå»ºæˆåŠŸ")
            else:
                logger.info("âœ… post_records è¡¨å·²å­˜åœ¨")

    except Exception as e:
        logger.error(f"âŒ å‰µå»º post_records è¡¨å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            return_db_connection(conn)

def create_schedule_tasks_table():
    """å‰µå»º schedule_tasks è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    conn = None
    try:
        if not db_pool:
            logger.error("âŒ æ•¸æ“šåº«é€£æ¥æ± ä¸å­˜åœ¨ï¼Œç„¡æ³•å‰µå»ºè¡¨")
            return

        conn = get_db_connection()

        with conn.cursor() as cursor:
            logger.info("ğŸ” æª¢æŸ¥ schedule_tasks è¡¨æ˜¯å¦å­˜åœ¨...")
            # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_schema = 'public'
                    AND table_name = 'schedule_tasks'
                );
            """)
            table_exists = cursor.fetchone()[0]
            logger.info(f"ğŸ“Š è¡¨å­˜åœ¨ç‹€æ…‹: {table_exists}")

            if not table_exists:
                logger.info("ğŸ“‹ é–‹å§‹å‰µå»º schedule_tasks è¡¨...")
                cursor.execute("""
                    CREATE TABLE schedule_tasks (
                        schedule_id VARCHAR PRIMARY KEY,
                        schedule_name VARCHAR NOT NULL,
                        schedule_description TEXT,
                        status VARCHAR DEFAULT 'active',
                        schedule_type VARCHAR NOT NULL,
                        interval_seconds INTEGER DEFAULT 300,
                        auto_posting BOOLEAN DEFAULT FALSE,
                        max_posts_per_hour INTEGER DEFAULT 2,
                        schedule_config TEXT,
                        trigger_config TEXT,
                        batch_info TEXT,
                        generation_config TEXT,
                        next_run TIMESTAMP,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        run_count INTEGER DEFAULT 0,
                        success_count INTEGER DEFAULT 0,
                        failure_count INTEGER DEFAULT 0,
                        success_rate FLOAT DEFAULT 0.0,
                        last_run TIMESTAMP,
                        last_error TEXT
                    );
                """)
                conn.commit()
                logger.info("âœ… schedule_tasks è¡¨å‰µå»ºæˆåŠŸ")
            else:
                logger.info("âœ… schedule_tasks è¡¨å·²å­˜åœ¨")

    except Exception as e:
        logger.error(f"âŒ å‰µå»º schedule_tasks è¡¨å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        if conn:
            conn.rollback()
        raise
    finally:
        if conn:
            return_db_connection(conn)

# ğŸ”¥ APScheduler Background Job - Check and execute schedules
async def check_schedules():
    """
    Background job that runs every minute to check for schedules ready to execute.

    Flow:
    1. Find schedules where status='active' AND next_run <= NOW()
    2. Execute each schedule (generate posts)
    3. If auto_posting=true: Publish posts with queue intervals
    4. Calculate and update next_run for next execution
    """
    conn = None
    try:
        logger.info("ğŸ” [APScheduler] Checking for schedules ready to execute...")

        conn = get_db_connection()
        if not conn:
            logger.error("âŒ [APScheduler] Failed to get database connection")
            return

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # ğŸ”¥ FIX: Only execute schedules due NOW (within current minute window)
            # If next_run is in the past, update it instead of executing
            now = get_current_time()

            # Define "now" window: current minute (e.g., 18:07:00 to 18:07:59)
            current_minute_start = now.replace(second=0, microsecond=0)
            current_minute_end = current_minute_start + timedelta(minutes=1)

            # First, find and update stale schedules (next_run in the past)
            cursor.execute("""
                SELECT *
                FROM schedule_tasks
                WHERE status = 'active'
                  AND next_run IS NOT NULL
                  AND next_run < %s
            """, (current_minute_start,))

            stale_schedules = cursor.fetchall()

            if stale_schedules:
                logger.info(f"âš ï¸ [APScheduler] Found {len(stale_schedules)} schedule(s) with stale next_run, updating...")
                for stale_schedule in stale_schedules:
                    await update_next_run(stale_schedule['schedule_id'], stale_schedule, is_post_execution=False)

            # Now find schedules due within current minute
            cursor.execute("""
                SELECT *
                FROM schedule_tasks
                WHERE status = 'active'
                  AND next_run IS NOT NULL
                  AND next_run >= %s
                  AND next_run < %s
                ORDER BY next_run ASC
            """, (current_minute_start, current_minute_end))

            ready_schedules = cursor.fetchall()

            if not ready_schedules:
                logger.info("âœ… [APScheduler] No schedules ready to execute")
                return

            logger.info(f"ğŸš€ [APScheduler] Found {len(ready_schedules)} schedule(s) ready to execute NOW")

            # Execute each schedule
            for schedule in ready_schedules:
                try:
                    schedule_id = schedule['schedule_id']
                    schedule_name = schedule['schedule_name']
                    auto_posting = schedule.get('auto_posting', False)
                    interval_seconds = schedule.get('interval_seconds', 300)

                    logger.info(f"ğŸš€ [APScheduler] Executing schedule: {schedule_name} (ID: {schedule_id})")
                    logger.info(f"ğŸ”§ [APScheduler] auto_posting={auto_posting}, interval_seconds={interval_seconds}")

                    # Import necessary modules for execution
                    import httpx

                    # Call the existing execute endpoint internally
                    # We'll construct a request body similar to what the API endpoint expects
                    async with httpx.AsyncClient(timeout=300.0) as client:
                        # Call our own execute endpoint
                        api_url = os.getenv('RAILWAY_PUBLIC_URL', 'http://localhost:8080')
                        response = await client.post(
                            f"{api_url}/api/schedule/execute/{schedule_id}",
                            json={}
                        )

                        if response.status_code == 200:
                            result = response.json()
                            logger.info(f"âœ… [APScheduler] Schedule executed successfully: {schedule_name}")

                            # If auto_posting is enabled, publish posts with intervals
                            if auto_posting and result.get('success'):
                                # ğŸ”¥ FIX: posts is at root level, not in 'data'
                                posts = result.get('posts', [])
                                if posts:
                                    logger.info(f"ğŸ“¤ [APScheduler] Auto-posting enabled - Publishing {len(posts)} posts with {interval_seconds}s intervals")
                                    await publish_posts_with_queue(posts, interval_seconds)
                                else:
                                    logger.warning(f"âš ï¸ [APScheduler] No posts to publish for schedule: {schedule_name}")

                            # Calculate next_run for the next execution
                            await update_next_run(schedule_id, schedule, is_post_execution=True)
                        else:
                            logger.error(f"âŒ [APScheduler] Schedule execution failed: {schedule_name}, status={response.status_code}")

                except Exception as schedule_error:
                    logger.error(f"âŒ [APScheduler] Error executing schedule {schedule.get('schedule_name')}: {schedule_error}")
                    logger.error(traceback.format_exc())
                    # Continue with next schedule even if this one fails
                    continue

    except Exception as e:
        logger.error(f"âŒ [APScheduler] Error in check_schedules: {e}")
        logger.error(traceback.format_exc())
    finally:
        if conn:
            return_db_connection(conn)

async def publish_posts_with_queue(posts: List[Dict], interval_seconds: int):
    """
    Publish posts one by one with intervals between them.

    Args:
        posts: List of post dictionaries from schedule execution
        interval_seconds: Seconds to wait between publishing each post
    """
    try:
        import httpx
        api_url = os.getenv('RAILWAY_PUBLIC_URL', 'http://localhost:8080')

        for idx, post in enumerate(posts):
            try:
                post_id = post.get('post_id')
                stock_code = post.get('stock_code')

                logger.info(f"ğŸ“¤ [Auto-Posting] Publishing post {idx+1}/{len(posts)}: {stock_code} (ID: {post_id})")

                async with httpx.AsyncClient(timeout=120.0) as client:
                    # Call the publish endpoint (single post)
                    response = await client.post(
                        f"{api_url}/api/posts/{post_id}/publish"
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if result.get('success'):
                            logger.info(f"âœ… [Auto-Posting] Post published successfully: {stock_code}")
                        else:
                            logger.error(f"âŒ [Auto-Posting] Failed to publish post: {result.get('error')}")
                    else:
                        logger.error(f"âŒ [Auto-Posting] Publish API returned {response.status_code}")

                # Wait interval before publishing next post (except for last post)
                if idx < len(posts) - 1:
                    logger.info(f"â³ [Auto-Posting] Waiting {interval_seconds}s before next post...")
                    await asyncio.sleep(interval_seconds)

            except Exception as post_error:
                logger.error(f"âŒ [Auto-Posting] Error publishing post {post_id}: {post_error}")
                # Continue with next post even if this one fails
                continue

    except Exception as e:
        logger.error(f"âŒ [Auto-Posting] Error in publish_posts_with_queue: {e}")
        logger.error(traceback.format_exc())

async def update_next_run(schedule_id: str, schedule: Dict, is_post_execution: bool = True):
    """
    Calculate and update next_run time for a schedule based on its schedule_type.

    Args:
        schedule_id: Schedule ID
        schedule: Schedule dictionary from database
        is_post_execution: True if updating after execution, False if updating stale schedule
    """
    conn = None
    try:
        schedule_type = schedule.get('schedule_type', 'daily')
        daily_execution_time = schedule.get('daily_execution_time')
        weekdays_only = schedule.get('weekdays_only', True)
        timezone_str = schedule.get('timezone', 'Asia/Taipei')

        # ğŸ”¥ FIX: Extract daily_execution_time from schedule_config if not at root level
        if not daily_execution_time:
            schedule_config = schedule.get('schedule_config', {})
            if isinstance(schedule_config, str):
                try:
                    schedule_config = json.loads(schedule_config)
                except:
                    schedule_config = {}
            if isinstance(schedule_config, dict):
                posting_time_slots = schedule_config.get('posting_time_slots', [])
                if posting_time_slots and len(posting_time_slots) > 0:
                    daily_execution_time = posting_time_slots[0]
                    logger.info(f"ğŸ” [APScheduler] Extracted daily_execution_time from schedule_config: {daily_execution_time}")

        tz = pytz.timezone(timezone_str)
        now = datetime.now(tz)
        next_run = None

        # ğŸ”¥ FIX: Support both 'daily' and 'weekday_daily' schedule types
        if schedule_type in ['daily', 'weekday_daily'] and daily_execution_time:
            # Parse execution time (format: "HH:MM")
            try:
                hour, minute = map(int, daily_execution_time.split(':'))

                # Calculate next execution time
                next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

                # If execution time already passed today, move to tomorrow
                if next_run <= now:
                    next_run = next_run + timedelta(days=1)

                # ğŸ”¥ FIX: For weekday_daily or weekdays_only, skip weekends
                # Skip to next Monday if next_run falls on weekend
                if schedule_type == 'weekday_daily' or weekdays_only:
                    while next_run.weekday() >= 5:  # 5=Saturday, 6=Sunday
                        next_run = next_run + timedelta(days=1)

                log_prefix = "ğŸ”„" if not is_post_execution else "ğŸ“…"
                logger.info(f"{log_prefix} [APScheduler] Next run for schedule {schedule_id}: {next_run.isoformat()}")

            except Exception as parse_error:
                logger.error(f"âŒ [APScheduler] Failed to parse daily_execution_time '{daily_execution_time}': {parse_error}")
                # ğŸ”¥ FIX: Set default next_run to avoid infinite stale loop
                next_run = now + timedelta(days=1)
                next_run = next_run.replace(hour=9, minute=30, second=0, microsecond=0)
                logger.warning(f"âš ï¸ [APScheduler] Set default next_run to tomorrow 09:30: {next_run.isoformat()}")
        elif schedule_type in ['daily', 'weekday_daily']:
            # Missing daily_execution_time but valid schedule_type
            logger.error(f"âŒ [APScheduler] Schedule {schedule_id} has schedule_type='{schedule_type}' but no daily_execution_time")
            logger.error(f"âŒ [APScheduler] schedule_config: {schedule.get('schedule_config', 'None')}")
            # ğŸ”¥ FIX: Set default next_run to avoid infinite stale loop
            next_run = now + timedelta(days=1)
            next_run = next_run.replace(hour=9, minute=30, second=0, microsecond=0)
            if schedule_type == 'weekday_daily':
                while next_run.weekday() >= 5:
                    next_run = next_run + timedelta(days=1)
            logger.warning(f"âš ï¸ [APScheduler] Set default next_run to tomorrow 09:30: {next_run.isoformat()}")
        else:
            # For other schedule types or empty schedule_type
            if not schedule_type:
                logger.error(f"âŒ [APScheduler] Schedule {schedule_id} has EMPTY schedule_type!")
            else:
                logger.warning(f"âš ï¸ [APScheduler] Schedule type '{schedule_type}' not supported for auto-update")
            # ğŸ”¥ FIX: Set default next_run to avoid infinite stale loop
            next_run = now + timedelta(days=1)
            next_run = next_run.replace(hour=9, minute=30, second=0, microsecond=0)
            logger.warning(f"âš ï¸ [APScheduler] Set default next_run to tomorrow 09:30: {next_run.isoformat()}")

        # Update next_run in database
        conn = get_db_connection()
        if conn:
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE schedule_tasks
                    SET next_run = %s, updated_at = NOW()
                    WHERE schedule_id = %s
                """, (next_run, schedule_id))
                conn.commit()
                logger.info(f"âœ… [APScheduler] Updated next_run for schedule {schedule_id}")

    except Exception as e:
        logger.error(f"âŒ [APScheduler] Error updating next_run for schedule {schedule_id}: {e}")
        logger.error(traceback.format_exc())
    finally:
        if conn:
            return_db_connection(conn)

@app.on_event("startup")
async def startup_event():
    """å•Ÿå‹•æ™‚åˆå§‹åŒ– FinLab å’Œæ•¸æ“šåº«é€£æ¥"""
    global stock_mapping, db_pool

    # æª¢æŸ¥æ‰€æœ‰é—œéµç’°å¢ƒè®Šæ•¸
    logger.info("ğŸ” [å•Ÿå‹•æª¢æŸ¥] é–‹å§‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸...")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FINLAB_API_KEY å­˜åœ¨: {os.getenv('FINLAB_API_KEY') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FORUM_200_EMAIL å­˜åœ¨: {os.getenv('FORUM_200_EMAIL') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FORUM_200_PASSWORD å­˜åœ¨: {os.getenv('FORUM_200_PASSWORD') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] DATABASE_URL å­˜åœ¨: {os.getenv('DATABASE_URL') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] PORT: {os.getenv('PORT', 'æœªè¨­å®š')}")

    # åˆå§‹åŒ–æ•¸æ“šåº«é€£æ¥ - Don't crash startup if DB fails
    try:
        database_url = os.getenv("DATABASE_URL")
        if database_url:
            logger.info(f"ğŸ”— å˜—è©¦é€£æ¥æ•¸æ“šåº«: {database_url[:20]}...")
            # Railway PostgreSQL URL æ ¼å¼è½‰æ›ï¼ˆpostgresql:// -> postgres:// for psycopg2ï¼‰
            if database_url.startswith("postgresql://"):
                database_url = database_url.replace("postgresql://", "postgres://", 1)

            # æ·»åŠ é€£æ¥åƒæ•¸ä»¥è§£æ±º Railway é€£æ¥å•é¡Œ
            import urllib.parse
            parsed_url = urllib.parse.urlparse(database_url)

            # æ§‹å»ºé€£æ¥åƒæ•¸
            connect_kwargs = {
                'host': parsed_url.hostname,
                'port': parsed_url.port or 5432,
                'database': parsed_url.path[1:],  # ç§»é™¤å‰å°æ–œç·š
                'user': parsed_url.username,
                'password': parsed_url.password,
                'connect_timeout': 30,  # 30ç§’é€£æ¥è¶…æ™‚
                'sslmode': 'require',   # Railway éœ€è¦ SSL
                'keepalives_idle': 600, # ä¿æŒé€£æ¥æ´»èº
                'keepalives_interval': 30,
                'keepalives_count': 3
            }

            # ğŸ”¥ FIX: Set DB_CONFIG for asyncpg connections (used in KOL Profile queries)
            global DB_CONFIG
            DB_CONFIG = {
                'host': parsed_url.hostname,
                'port': parsed_url.port or 5432,
                'database': parsed_url.path[1:],
                'user': parsed_url.username,
                'password': parsed_url.password
            }
            logger.info(f"âœ… DB_CONFIG å·²è¨­ç½®: host={DB_CONFIG['host']}, database={DB_CONFIG['database']}")

            # Create connection pool (1-10 concurrent connections)
            # Use minconn=1 for faster startup
            logger.info(f"ğŸ”— å‰µå»ºæ•¸æ“šåº«é€£æ¥æ± ...")
            db_pool = pool.SimpleConnectionPool(
                minconn=1,  # Minimum 1 connection (faster startup)
                maxconn=10,  # Maximum 10 concurrent connections
                **connect_kwargs
            )
            logger.info("âœ… PostgreSQL é€£æ¥æ± å‰µå»ºæˆåŠŸ (1-10 connections)")

            # Don't test pool during startup - let first request validate
            logger.info("âœ… æ•¸æ“šåº«é€£æ¥æ± åˆå§‹åŒ–å®Œæˆ (å»¶é²æ¸¬è©¦åˆ°é¦–æ¬¡ä½¿ç”¨)")

            # Create tables using pool
            try:
                logger.info("ğŸ“‹ é–‹å§‹å‰µå»º post_records è¡¨...")
                create_post_records_table()
                logger.info("âœ… post_records è¡¨å‰µå»ºå®Œæˆ")

                logger.info("ğŸ“‹ é–‹å§‹å‰µå»º schedule_tasks è¡¨...")
                create_schedule_tasks_table()
                logger.info("âœ… schedule_tasks è¡¨å‰µå»ºå®Œæˆ")
            except Exception as table_error:
                logger.error(f"âŒ å‰µå»ºè¡¨å¤±æ•—: {table_error}")
                # Don't fail startup if table creation fails
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° DATABASE_URL ç’°å¢ƒè®Šæ•¸ï¼Œå°‡ç„¡æ³•æŸ¥è©¢è²¼æ–‡æ•¸æ“š")
            db_pool = None
    except Exception as e:
        logger.error(f"âŒ PostgreSQL æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        db_pool = None  # Ensure pool is None on failure

    # FinLab API ç™»å…¥ - Don't crash startup if FinLab login fails
    try:
        api_key = os.getenv("FINLAB_API_KEY")
        if api_key:
            logger.info("ğŸ”‘ å˜—è©¦ç™»å…¥ FinLab API...")
            finlab.login(api_key)
            logger.info("âœ… FinLab API ç™»å…¥æˆåŠŸ")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° FINLAB_API_KEY ç’°å¢ƒè®Šæ•¸")
    except Exception as e:
        logger.error(f"âŒ FinLab API ç™»å…¥å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        # Don't crash - app can still serve other endpoints

    # è¼‰å…¥è‚¡ç¥¨æ˜ å°„è¡¨ï¼ˆå…ˆå˜—è©¦éœæ…‹æ–‡ä»¶ï¼‰
    try:
        stock_mapping_path = '/app/stock_mapping.json'
        if os.path.exists(stock_mapping_path):
            with open(stock_mapping_path, 'r', encoding='utf-8') as f:
                stock_mapping = json.load(f)
            logger.info(f"âœ… è¼‰å…¥éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨æˆåŠŸ: {len(stock_mapping)} æ”¯è‚¡ç¥¨")
        else:
            logger.warning(f"âš ï¸ éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨ä¸å­˜åœ¨: {stock_mapping_path}")
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨å¤±æ•—: {e}")

    # å¾ FinLab å‹•æ…‹è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨Š
    try:
        if api_key:
            logger.info("ğŸ“Š æ­£åœ¨å¾ FinLab è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨Š...")
            company_info = data.get('company_basic_info')
            if company_info is not None and not company_info.empty:
                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
                for stock_id in company_info['stock_id']:
                    stock_data = company_info[company_info['stock_id'] == stock_id].iloc[0]
                    stock_mapping[stock_id] = {
                        'company_name': stock_data.get('å…¬å¸ç°¡ç¨±', stock_data.get('å…¬å¸åç¨±', f'è‚¡ç¥¨{stock_id}')),
                        'industry': stock_data.get('ç”¢æ¥­é¡åˆ¥', 'æœªçŸ¥ç”¢æ¥­')
                    }
                logger.info(f"âœ… å¾ FinLab è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨ŠæˆåŠŸ: {len(stock_mapping)} æ”¯è‚¡ç¥¨")
            else:
                logger.warning("âš ï¸ ç„¡æ³•å¾ FinLab å–å¾—å…¬å¸è³‡è¨Š")
    except Exception as e:
        logger.error(f"âŒ å¾ FinLab è¼‰å…¥å…¬å¸è³‡è¨Šå¤±æ•—: {e}")

    # ğŸ”¥ Initialize and start APScheduler
    try:
        logger.info("ğŸš€ [APScheduler] æ­£åœ¨å•Ÿå‹•æ’ç¨‹å™¨...")

        # Add job to check schedules every minute
        scheduler.add_job(
            check_schedules,
            'interval',
            minutes=1,
            id='check_schedules',
            replace_existing=True,
            max_instances=1  # Prevent overlapping runs
        )

        # Start the scheduler
        scheduler.start()
        logger.info("âœ… [APScheduler] æ’ç¨‹å™¨å•Ÿå‹•æˆåŠŸ - æ¯åˆ†é˜æª¢æŸ¥æ’ç¨‹ä»»å‹™")

    except Exception as scheduler_error:
        logger.error(f"âŒ [APScheduler] æ’ç¨‹å™¨å•Ÿå‹•å¤±æ•—: {scheduler_error}")
        logger.error(traceback.format_exc())
        # Don't crash startup if scheduler fails

    # ğŸ”¥ Initialize Reaction Bot Service
    try:
        global reaction_bot_service, cmoney_reaction_client, asyncpg_pool

        logger.info("ğŸ¤– [Reaction Bot] æ­£åœ¨åˆå§‹åŒ– Reaction Bot æœå‹™...")

        # Create asyncpg connection pool if DB_CONFIG is available
        if DB_CONFIG:
            import asyncpg
            asyncpg_pool = await asyncpg.create_pool(
                host=DB_CONFIG['host'],
                port=DB_CONFIG['port'],
                database=DB_CONFIG['database'],
                user=DB_CONFIG['user'],
                password=DB_CONFIG['password'],
                min_size=1,
                max_size=5
            )
            logger.info("âœ… [Reaction Bot] AsyncPG é€£æ¥æ± å‰µå»ºæˆåŠŸ")

            # Initialize CMoney reaction client
            from cmoney_reaction_client import CMoneyReactionClient
            cmoney_reaction_client = CMoneyReactionClient()
            logger.info("âœ… [Reaction Bot] CMoney å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")

            # Initialize reaction bot service
            from reaction_bot_service import ReactionBotService
            reaction_bot_service = ReactionBotService(
                db_connection=asyncpg_pool,
                cmoney_client=cmoney_reaction_client
            )
            logger.info("âœ… [Reaction Bot] Reaction Bot æœå‹™åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("âš ï¸  [Reaction Bot] DB_CONFIG æœªè¨­ç½®ï¼ŒReaction Bot æœå‹™å°‡ç„¡æ³•ä½¿ç”¨")

    except Exception as reaction_bot_error:
        logger.error(f"âŒ [Reaction Bot] åˆå§‹åŒ–å¤±æ•—: {reaction_bot_error}")
        logger.error(traceback.format_exc())
        reaction_bot_service = None
        cmoney_reaction_client = None
        asyncpg_pool = None
        # Don't crash startup if reaction bot fails

@app.on_event("shutdown")
async def shutdown_event():
    """æ‡‰ç”¨é—œé–‰æ™‚æ¸…ç†è³‡æº"""
    try:
        # Shutdown scheduler
        if scheduler and scheduler.running:
            logger.info("ğŸ›‘ [APScheduler] æ­£åœ¨é—œé–‰æ’ç¨‹å™¨...")
            scheduler.shutdown(wait=False)
            logger.info("âœ… [APScheduler] æ’ç¨‹å™¨å·²é—œé–‰")
    except Exception as e:
        logger.error(f"âŒ [APScheduler] æ’ç¨‹å™¨é—œé–‰å¤±æ•—: {e}")

    try:
        # Close reaction bot resources
        if cmoney_reaction_client:
            logger.info("ğŸ›‘ [Reaction Bot] æ­£åœ¨é—œé–‰ CMoney å®¢æˆ¶ç«¯...")
            cmoney_reaction_client.close()
            logger.info("âœ… [Reaction Bot] CMoney å®¢æˆ¶ç«¯å·²é—œé–‰")

        if asyncpg_pool:
            logger.info("ğŸ›‘ [Reaction Bot] æ­£åœ¨é—œé–‰ AsyncPG é€£æ¥æ± ...")
            await asyncpg_pool.close()
            logger.info("âœ… [Reaction Bot] AsyncPG é€£æ¥æ± å·²é—œé–‰")
    except Exception as e:
        logger.error(f"âŒ [Reaction Bot] é—œé–‰å¤±æ•—: {e}")

def ensure_finlab_login():
    """ç¢ºä¿ FinLab å·²ç™»å…¥"""
    try:
        test_data = data.get('market_transaction_info:æ”¶ç›¤æŒ‡æ•¸')
        if test_data is None:
            api_key = os.getenv("FINLAB_API_KEY")
            if api_key:
                finlab.login(api_key)
                logger.info("ğŸ”„ FinLab API é‡æ–°ç™»å…¥æˆåŠŸ")
            else:
                raise Exception("æœªæ‰¾åˆ° FINLAB_API_KEY")
    except Exception as e:
        logger.error(f"âŒ FinLab ç™»å…¥æª¢æŸ¥å¤±æ•—: {e}")
        raise e

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def get_stock_name(stock_code: str) -> str:
    """æ ¹æ“šè‚¡ç¥¨ä»£è™Ÿç²å–è‚¡ç¥¨åç¨±"""
    if stock_code in stock_mapping:
        return stock_mapping[stock_code].get('company_name', f"è‚¡ç¥¨{stock_code}")

    stock_names = {
        "2330": "å°ç©é›»", "2454": "è¯ç™¼ç§‘", "2317": "é´»æµ·", "2881": "å¯Œé‚¦é‡‘",
        "2882": "åœ‹æ³°é‡‘", "1101": "å°æ³¥", "1102": "äºæ³¥", "1216": "çµ±ä¸€",
        "1303": "å—äº", "1326": "å°åŒ–", "2002": "ä¸­é‹¼", "2308": "å°é”é›»",
        "2377": "å¾®æ˜Ÿ", "2382": "å»£é”", "2408": "å—äºç§‘", "2474": "å¯æˆ",
        "2498": "å®é”é›»", "3008": "å¤§ç«‹å…‰", "3034": "è¯è© ", "3231": "ç·¯å‰µ",
        "3711": "æ—¥æœˆå…‰æŠ•æ§", "4938": "å’Œç¢©", "6505": "å°å¡‘åŒ–", "8046": "å—é›»",
        "9910": "è±æ³°", "2412": "ä¸­è¯é›»", "1301": "å°å¡‘", "2603": "é•·æ¦®"
    }
    return stock_names.get(stock_code, f"è‚¡ç¥¨{stock_code}")

def get_stock_industry(stock_code: str) -> str:
    """æ ¹æ“šè‚¡ç¥¨ä»£è™Ÿç²å–ç”¢æ¥­é¡åˆ¥"""
    if stock_code in stock_mapping:
        return stock_mapping[stock_code].get('industry', 'æœªçŸ¥ç”¢æ¥­')
    return 'æœªçŸ¥ç”¢æ¥­'

def calculate_trading_stats(stock_id: str, latest_date: datetime, close_df: pd.DataFrame) -> dict:
    """è¨ˆç®—éå»äº”å€‹äº¤æ˜“æ—¥çš„çµ±è¨ˆè³‡è¨Š"""
    try:
        trading_days = close_df.index[close_df.index <= latest_date].sort_values(ascending=False)[:5]

        if len(trading_days) < 2:
            return {"up_days": 0, "five_day_change": 0.0}

        up_days = 0
        for i in range(len(trading_days) - 1):
            current_price = close_df.loc[trading_days[i], stock_id]
            previous_price = close_df.loc[trading_days[i + 1], stock_id]

            if not pd.isna(current_price) and not pd.isna(previous_price) and current_price > previous_price:
                up_days += 1

        five_days_ago_price = close_df.loc[trading_days[-1], stock_id]
        latest_price = close_df.loc[trading_days[0], stock_id]

        if not pd.isna(five_days_ago_price) and not pd.isna(latest_price) and five_days_ago_price != 0:
            five_day_change = ((latest_price - five_days_ago_price) / five_days_ago_price) * 100
        else:
            five_day_change = 0.0

        return {
            "up_days": up_days,
            "five_day_change": round(five_day_change, 2)
        }
    except Exception as e:
        logger.error(f"è¨ˆç®— {stock_id} äº¤æ˜“çµ±è¨ˆå¤±æ•—: {e}")
        return {"up_days": 0, "five_day_change": 0.0}

# ==================== å¥åº·æª¢æŸ¥ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾‘"""
    logger.info("æ”¶åˆ°æ ¹è·¯å¾‘è«‹æ±‚")
    return {
        "message": "Forum Autoposter Unified API",
        "status": "running",
        "version": "1.0.0",
        "timestamp": get_current_time().isoformat()
    }

@app.get("/health")
@app.get("/api/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é» - æ”¯æŒ /health å’Œ /api/health å…©å€‹è·¯å¾‘"""
    logger.info("æ”¶åˆ°å¥åº·æª¢æŸ¥è«‹æ±‚")

    # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥ç‹€æ…‹
    db_status = "disconnected"
    if db_pool:
        conn = None
        try:
            conn = get_db_connection()
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                cursor.fetchone()
            db_status = "connected"
        except Exception as e:
            logger.warning(f"æ•¸æ“šåº«å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
            db_status = "error"
        finally:
            if conn:
                return_db_connection(conn)

    # æª¢æŸ¥ FinLab API ç‹€æ…‹
    finlab_status = "connected" if os.getenv("FINLAB_API_KEY") else "disconnected"

    return {
        "status": "healthy",
        "message": "Unified API is running successfully",
        "timestamp": get_current_time().isoformat(),
        "services": {
            "finlab": finlab_status,
            "database": db_status
        },
        "endpoints": {
            "total": 35,
            "working": 30 if db_status == "connected" else 24,
            "database_dependent": 11
        }
    }

@app.get("/api/kols")
async def get_kols():
    """Get all KOL profiles from database"""
    logger.info("æ”¶åˆ° GET /api/kols è«‹æ±‚")

    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT serial, nickname, persona, model_id
                FROM kol_profiles
                ORDER BY serial
            """)
            kols = cursor.fetchall()

            logger.info(f"âœ… æˆåŠŸç²å– {len(kols)} å€‹ KOL è³‡æ–™")

            return {
                "success": True,
                "kols": [dict(kol) for kol in kols],
                "count": len(kols)
            }
    except Exception as e:
        logger.error(f"âŒ ç²å– KOL åˆ—è¡¨å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch KOL profiles: {str(e)}")
    finally:
        if conn:
            return_db_connection(conn)

@app.get("/api/debug/import-status")
async def debug_import_status():
    """Debug endpoint to expose module import status and errors"""
    import traceback

    result = {
        "timestamp": get_current_time().isoformat(),
        "python_version": sys.version,
        "sys_path": sys.path,
        "posting_service_path": os.path.join(os.path.dirname(__file__), 'posting-service'),
        "modules": {}
    }

    # Check if posting-service directory exists
    posting_service_dir = os.path.join(os.path.dirname(__file__), 'posting-service')
    result["posting_service_exists"] = os.path.exists(posting_service_dir)

    if os.path.exists(posting_service_dir):
        result["posting_service_files"] = os.listdir(posting_service_dir)

    # Check GPT generator
    result["modules"]["gpt_generator"] = {
        "imported": gpt_generator is not None,
        "instance": str(type(gpt_generator)) if gpt_generator else None
    }

    # Check personalization processor
    result["modules"]["personalization_processor"] = {
        "imported": enhanced_personalization_processor is not None,
        "instance": str(type(enhanced_personalization_processor)) if enhanced_personalization_processor else None
    }

    # Try importing modules individually to see exact errors
    errors = {}

    # Test openai import
    try:
        import openai as test_openai
        errors["openai"] = {"status": "success", "version": getattr(test_openai, '__version__', 'unknown')}
    except Exception as e:
        errors["openai"] = {"status": "failed", "error": str(e), "traceback": traceback.format_exc()}

    # Test gpt_content_generator import
    try:
        from gpt_content_generator import GPTContentGenerator as TestGPT
        errors["gpt_content_generator"] = {"status": "success"}
    except Exception as e:
        errors["gpt_content_generator"] = {"status": "failed", "error": str(e), "traceback": traceback.format_exc()}

    # Test kol_database_service import
    try:
        from kol_database_service import KOLProfile, KOLDatabaseService
        errors["kol_database_service"] = {"status": "success"}
    except Exception as e:
        errors["kol_database_service"] = {"status": "failed", "error": str(e), "traceback": traceback.format_exc()}

    # Test random_content_generator import
    try:
        from random_content_generator import RandomContentGenerator
        errors["random_content_generator"] = {"status": "success"}
    except Exception as e:
        errors["random_content_generator"] = {"status": "failed", "error": str(e), "traceback": traceback.format_exc()}

    # Test personalization_module import
    try:
        from personalization_module import enhanced_personalization_processor as test_processor
        errors["personalization_module"] = {"status": "success", "has_instance": test_processor is not None}
    except Exception as e:
        errors["personalization_module"] = {"status": "failed", "error": str(e), "traceback": traceback.format_exc()}

    result["import_tests"] = errors

    return result

@app.get("/api/debug/openai-config")
async def debug_openai_config():
    """Debug endpoint to check OpenAI API configuration"""
    import traceback

    result = {
        "timestamp": get_current_time().isoformat(),
        "environment": {
            "OPENAI_API_KEY_exists": os.getenv("OPENAI_API_KEY") is not None,
            "OPENAI_API_KEY_length": len(os.getenv("OPENAI_API_KEY", "")) if os.getenv("OPENAI_API_KEY") else 0,
            "OPENAI_API_KEY_prefix": os.getenv("OPENAI_API_KEY", "")[:10] + "..." if os.getenv("OPENAI_API_KEY") else None,
            "OPENAI_MODEL": os.getenv("OPENAI_MODEL", "not set")
        },
        "gpt_generator": {
            "initialized": gpt_generator is not None,
            "has_api_key": gpt_generator.api_key is not None if gpt_generator else False,
            "api_key_length": len(gpt_generator.api_key) if gpt_generator and gpt_generator.api_key else 0,
            "model": gpt_generator.model if gpt_generator else None
        }
    }

    # Test OpenAI API connection
    if gpt_generator and gpt_generator.api_key:
        try:
            # Try a simple API call
            import openai
            openai.api_key = gpt_generator.api_key

            # Test with a minimal completion
            response = openai.chat.completions.create(
                model=gpt_generator.model,
                messages=[{"role": "user", "content": "Say 'OK' if you can read this."}],
                max_tokens=10
            )

            result["api_test"] = {
                "status": "success",
                "response": response.choices[0].message.content,
                "model_used": response.model
            }
        except Exception as e:
            result["api_test"] = {
                "status": "failed",
                "error": str(e),
                "error_type": type(e).__name__,
                "traceback": traceback.format_exc()[:500]
            }
    else:
        result["api_test"] = {
            "status": "skipped",
            "reason": "No API key or GPT generator not initialized"
        }

    return result

@app.post("/api/database/migrate/add-schedule-columns")
async def migrate_add_schedule_columns():
    """
    Migration: Add trigger_config and schedule_config columns to schedule_tasks table
    This is a one-time migration endpoint
    """
    logger.info("æ”¶åˆ°æ•¸æ“šåº«é·ç§»è«‹æ±‚: add-schedule-columns")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "Database connection not available"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Check if columns already exist
            cursor.execute("""
                SELECT column_name
                FROM information_schema.columns
                WHERE table_name = 'schedule_tasks'
                  AND column_name IN ('trigger_config', 'schedule_config')
            """)
            existing_columns = [row['column_name'] for row in cursor.fetchall()]

            if 'trigger_config' in existing_columns and 'schedule_config' in existing_columns:
                logger.info("âœ… Columns already exist, no migration needed")
                return {
                    "success": True,
                    "message": "Columns already exist, no migration needed",
                    "existing_columns": existing_columns
                }

            logger.info(f"ğŸ“ Existing columns: {existing_columns}")
            logger.info("ğŸ”„ Adding missing columns...")

            # Add columns
            cursor.execute("""
                ALTER TABLE schedule_tasks
                ADD COLUMN IF NOT EXISTS trigger_config JSONB DEFAULT '{}',
                ADD COLUMN IF NOT EXISTS schedule_config JSONB DEFAULT '{}'
            """)

            conn.commit()
            logger.info("âœ… Columns added successfully")

            # Verify columns were added
            cursor.execute("""
                SELECT column_name, data_type
                FROM information_schema.columns
                WHERE table_name = 'schedule_tasks'
                  AND column_name IN ('trigger_config', 'schedule_config')
            """)

            result = cursor.fetchall()
            logger.info(f"ğŸ“Š Verification: {result}")

            return {
                "success": True,
                "message": "Migration completed successfully",
                "columns_added": [dict(row) for row in result]
            }

    except Exception as e:
        logger.error(f"âŒ Migration failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/database/test")
async def test_database():
    """æ¸¬è©¦æ•¸æ“šåº«é€£æ¥ä¸¦åŸ·è¡ŒæŸ¥è©¢"""
    logger.info("æ”¶åˆ°æ•¸æ“šåº«æ¸¬è©¦è«‹æ±‚")

    result = {
        "connection_status": "disconnected",
        "test_query_success": False,
        "tables": [],
        "kol_count": 0,
        "post_count": 0,
        "schedule_count": 0,
        "errors": [],
        "timestamp": get_current_time().isoformat()
    }

    conn = None
    try:
        if not db_pool:
            result["errors"].append("Database pool not initialized")
            return result

        conn = get_db_connection()

        # Test 1: Basic connection test
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                cursor.fetchone()
            result["connection_status"] = "connected"
            result["test_query_success"] = True
        except Exception as e:
            result["errors"].append(f"Connection test failed: {str(e)}")
            return result

        # Test 2: List all tables
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT table_name
                    FROM information_schema.tables
                    WHERE table_schema = 'public'
                    ORDER BY table_name
                """)
                tables = cursor.fetchall()
                result["tables"] = [table[0] for table in tables]
        except Exception as e:
            result["errors"].append(f"Failed to list tables: {str(e)}")

        # Test 3: Count records in key tables
        try:
            with conn.cursor() as cursor:
                # Count KOLs
                cursor.execute("SELECT COUNT(*) FROM kol_profiles")
                result["kol_count"] = cursor.fetchone()[0]

                # Count posts
                cursor.execute("SELECT COUNT(*) FROM post_records")
                result["post_count"] = cursor.fetchone()[0]

                # Count schedules
                cursor.execute("SELECT COUNT(*) FROM posting_schedules")
                result["schedule_count"] = cursor.fetchone()[0]
        except Exception as e:
            result["errors"].append(f"Failed to count records: {str(e)}")

        result["success"] = len(result["errors"]) == 0

    except Exception as e:
        result["errors"].append(f"Database test error: {str(e)}")
    finally:
        if conn:
            return_db_connection(conn)

    return result

@app.post("/api/database/migrate-trigger-type")
async def migrate_trigger_type():
    """
    Database Migration: Add trigger_type column to post_records table

    This endpoint should be called ONCE after deploying the trigger_type fix.
    It adds the missing trigger_type column to the post_records table.
    """
    logger.info("ğŸ”§ é–‹å§‹æ•¸æ“šåº«é·ç§»: æ·»åŠ  trigger_type åˆ—")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "Database pool not initialized",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor() as cursor:
            # Add trigger_type column if it doesn't exist
            cursor.execute("""
                ALTER TABLE post_records
                ADD COLUMN IF NOT EXISTS trigger_type VARCHAR(100);
            """)

            # Add generation_mode column if it doesn't exist
            cursor.execute("""
                ALTER TABLE post_records
                ADD COLUMN IF NOT EXISTS generation_mode VARCHAR(50) DEFAULT 'manual';
            """)

            conn.commit()

        logger.info("âœ… æ•¸æ“šåº«é·ç§»æˆåŠŸ: trigger_type å’Œ generation_mode åˆ—å·²æ·»åŠ ")
        return {
            "success": True,
            "message": "Migration successful: trigger_type and generation_mode columns added to post_records table",
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåº«é·ç§»å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/database/migrate/disable-all-schedules")
async def migrate_disable_all_schedules():
    """
    Migration: Disable all existing schedules and turn off auto_posting

    This is a clean slate migration to:
    1. Set all schedules to status='cancelled'
    2. Disable auto_posting for all schedules
    3. Start fresh with new test schedules
    """
    logger.info("ğŸ”§ é–‹å§‹æ•¸æ“šåº«é·ç§»: ç¦ç”¨æ‰€æœ‰ç¾æœ‰æ’ç¨‹")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "Database pool not initialized",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor() as cursor:
            # Count existing schedules
            cursor.execute("SELECT COUNT(*) FROM schedule_tasks WHERE status != 'cancelled'")
            count_before = cursor.fetchone()[0]

            # Disable all schedules and turn off auto_posting
            cursor.execute("""
                UPDATE schedule_tasks
                SET status = 'cancelled',
                    auto_posting = FALSE,
                    updated_at = NOW()
                WHERE status != 'cancelled'
            """)

            rows_updated = cursor.rowcount
            conn.commit()

        logger.info(f"âœ… æ•¸æ“šåº«é·ç§»æˆåŠŸ: {rows_updated} å€‹æ’ç¨‹å·²ç¦ç”¨")
        return {
            "success": True,
            "message": f"Migration successful: Disabled {rows_updated} schedules and turned off auto_posting",
            "schedules_before": count_before,
            "schedules_updated": rows_updated,
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåº«é·ç§»å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/database/migrate/add-reaction-bot-tables")
async def migrate_add_reaction_bot_tables():
    """
    Migration: Create reaction bot tables

    Creates all required tables for the reaction bot feature:
    - reaction_bot_config
    - reaction_bot_logs
    - reaction_bot_batches
    - reaction_bot_article_queue
    - reaction_bot_stats
    """
    logger.info("ğŸ”§ é–‹å§‹æ•¸æ“šåº«é·ç§»: å‰µå»º Reaction Bot è¡¨")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "Database pool not initialized",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        # Read SQL file
        sql_file_path = os.path.join(os.path.dirname(__file__), 'migrations', 'add_reaction_bot_tables.sql')

        if not os.path.exists(sql_file_path):
            raise Exception(f"SQL file not found: {sql_file_path}")

        with open(sql_file_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()

        # Execute SQL
        with conn.cursor() as cursor:
            cursor.execute(sql_content)
            conn.commit()

        logger.info("âœ… æ•¸æ“šåº«é·ç§»æˆåŠŸ: Reaction Bot è¡¨å·²å‰µå»º")
        return {
            "success": True,
            "message": "Migration successful: Reaction Bot tables created",
            "tables_created": [
                "reaction_bot_config",
                "reaction_bot_logs",
                "reaction_bot_batches",
                "reaction_bot_article_queue",
                "reaction_bot_stats"
            ],
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåº«é·ç§»å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/admin/reconnect-database")
async def reconnect_database():
    """é‡æ–°é€£æ¥æ•¸æ“šåº«ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    global db_connection
    logger.info("æ”¶åˆ°é‡æ–°é€£æ¥æ•¸æ“šåº«è«‹æ±‚")
    
    try:
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            return {
                "success": False,
                "error": "DATABASE_URL not found",
                "timestamp": get_current_time().isoformat()
            }
        
        # é—œé–‰ç¾æœ‰é€£æ¥
        if db_pool:
            try:
                db_connection.close()
                logger.info("å·²é—œé–‰ç¾æœ‰æ•¸æ“šåº«é€£æ¥")
            except:
                pass
        
        # é‡æ–°é€£æ¥
        import urllib.parse
        parsed_url = urllib.parse.urlparse(database_url)
        
        connect_kwargs = {
            'host': parsed_url.hostname,
            'port': parsed_url.port or 5432,
            'database': parsed_url.path[1:],
            'user': parsed_url.username,
            'password': parsed_url.password,
            'connect_timeout': 30,
            'sslmode': 'require',
            'keepalives_idle': 600,
            'keepalives_interval': 30,
            'keepalives_count': 3
        }
        
        db_connection = psycopg2.connect(**connect_kwargs)
        
        # æ¸¬è©¦é€£æ¥
        with conn.cursor() as cursor:
            cursor.execute("SELECT 1")
            cursor.fetchone()
        
        logger.info("âœ… æ•¸æ“šåº«é‡æ–°é€£æ¥æˆåŠŸ")
        
        return {
            "success": True,
            "message": "Database reconnected successfully",
            "timestamp": get_current_time().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æ•¸æ“šåº«é‡æ–°é€£æ¥å¤±æ•—: {e}")
        db_connection = None
        return {
            "success": False,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }

# ==================== OHLC API åŠŸèƒ½ ====================

@app.get("/api/after_hours_limit_up")
async def get_after_hours_limit_up_stocks(
    limit: int = Query(1000, description="è‚¡ç¥¨æ•¸é‡é™åˆ¶"),
    changeThreshold: float = Query(9.5, description="æ¼²è·Œå¹…é–¾å€¼ç™¾åˆ†æ¯”"),
    industries: str = Query("", description="ç”¢æ¥­é¡åˆ¥ç¯©é¸"),
    sortBy: str = Query(None, description="æ’åºæ–¹å¼: five_day_loss, five_day_gain, volume_desc, etc.")
):
    """ç²å–ç›¤å¾Œæ¼²åœè‚¡ç¥¨åˆ—è¡¨ - æ”¯æŒå‹•æ…‹æ¼²è·Œå¹…è¨­å®š"""
    try:
        ensure_finlab_login()

        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–æ”¶ç›¤åƒ¹æ•¸æ“š"}

        close_df = close_df.sort_index()
        if volume_df is not None:
            volume_df = volume_df.sort_index()

        latest_date = close_df.index[-1]
        latest_close = close_df.loc[latest_date]

        previous_dates = close_df.index[close_df.index < latest_date]
        if len(previous_dates) == 0:
            return {"error": f"ç„¡æ³•æ‰¾åˆ° {latest_date.strftime('%Y-%m-%d')} ä¹‹å‰çš„äº¤æ˜“æ—¥æ•¸æ“š"}

        previous_date = previous_dates[-1]
        previous_close = close_df.loc[previous_date]

        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]

        selected_industries = []
        if industries:
            selected_industries = [industry.strip() for industry in industries.split(',') if industry.strip()]

        limit_up_stocks = []

        for stock_id in latest_close.index:
            try:
                if selected_industries:
                    stock_industry = get_stock_industry(stock_id)
                    if stock_industry not in selected_industries:
                        continue

                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]

                if pd.isna(today_price) or pd.isna(yesterday_price) or yesterday_price == 0:
                    continue

                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100

                if change_percent >= changeThreshold:
                    stock_name = get_stock_name(stock_id)
                    stock_industry = get_stock_industry(stock_id)

                    volume = 0
                    volume_amount = 0
                    if latest_volume is not None and stock_id in latest_volume.index:
                        vol = latest_volume[stock_id]
                        if not pd.isna(vol):
                            volume = int(vol)
                            volume_amount = volume * float(today_price)

                    trading_stats = calculate_trading_stats(stock_id, latest_date, close_df)

                    limit_up_stocks.append({
                        'stock_code': stock_id,
                        'stock_name': stock_name,
                        'industry': stock_industry,
                        'current_price': float(today_price),
                        'yesterday_close': float(yesterday_price),
                        'change_amount': float(today_price - yesterday_price),
                        'change_percent': float(change_percent),
                        'volume': volume,
                        'volume_amount': volume_amount,
                        'date': latest_date.strftime('%Y-%m-%d'),
                        'previous_date': previous_date.strftime('%Y-%m-%d'),
                        'up_days_5': trading_stats['up_days'],
                        'five_day_change': trading_stats['five_day_change']
                    })

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        limit_up_stocks.sort(key=lambda x: x['volume'], reverse=True)
        limit_up_stocks = limit_up_stocks[:limit]

        return {
            'success': True,
            'total_count': len(limit_up_stocks),
            'stocks': limit_up_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'changeThreshold': changeThreshold
        }

    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæ¼²åœè‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/after_hours_limit_down")
async def get_after_hours_limit_down_stocks(
    limit: int = Query(1000, description="è‚¡ç¥¨æ•¸é‡é™åˆ¶"),
    changeThreshold: float = Query(-9.5, description="è·Œå¹…é–¾å€¼ç™¾åˆ†æ¯”"),
    industries: str = Query("", description="ç”¢æ¥­é¡åˆ¥ç¯©é¸"),
    sortBy: str = Query(None, description="æ’åºæ–¹å¼: five_day_loss, five_day_gain, volume_desc, etc.")
):
    """ç²å–ç›¤å¾Œè·Œåœè‚¡ç¥¨åˆ—è¡¨"""
    try:
        ensure_finlab_login()

        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–æ”¶ç›¤åƒ¹æ•¸æ“š"}

        close_df = close_df.sort_index()
        if volume_df is not None:
            volume_df = volume_df.sort_index()

        latest_date = close_df.index[-1]
        latest_close = close_df.loc[latest_date]

        previous_dates = close_df.index[close_df.index < latest_date]
        if len(previous_dates) == 0:
            return {"error": f"ç„¡æ³•æ‰¾åˆ° {latest_date.strftime('%Y-%m-%d')} ä¹‹å‰çš„äº¤æ˜“æ—¥æ•¸æ“š"}

        previous_date = previous_dates[-1]
        previous_close = close_df.loc[previous_date]

        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]

        selected_industries = []
        if industries:
            selected_industries = [industry.strip() for industry in industries.split(',') if industry.strip()]

        limit_down_stocks = []

        for stock_id in latest_close.index:
            try:
                if selected_industries:
                    stock_industry = get_stock_industry(stock_id)
                    if stock_industry not in selected_industries:
                        continue

                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]

                if pd.isna(today_price) or pd.isna(yesterday_price) or yesterday_price == 0:
                    continue

                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100

                if change_percent <= changeThreshold:
                    stock_name = get_stock_name(stock_id)
                    stock_industry = get_stock_industry(stock_id)

                    volume = 0
                    volume_amount = 0
                    if latest_volume is not None and stock_id in latest_volume.index:
                        vol = latest_volume[stock_id]
                        if not pd.isna(vol):
                            volume = int(vol)
                            volume_amount = volume * float(today_price)

                    trading_stats = calculate_trading_stats(stock_id, latest_date, close_df)

                    limit_down_stocks.append({
                        'stock_code': stock_id,
                        'stock_name': stock_name,
                        'industry': stock_industry,
                        'current_price': float(today_price),
                        'yesterday_close': float(yesterday_price),
                        'change_amount': float(today_price - yesterday_price),
                        'change_percent': float(change_percent),
                        'volume': volume,
                        'volume_amount': volume_amount,
                        'date': latest_date.strftime('%Y-%m-%d'),
                        'previous_date': previous_date.strftime('%Y-%m-%d'),
                        'up_days_5': trading_stats['up_days'],
                        'five_day_change': trading_stats['five_day_change']
                    })

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        limit_down_stocks.sort(key=lambda x: x['volume'], reverse=True)
        limit_down_stocks = limit_down_stocks[:limit]

        return {
            'success': True,
            'total_count': len(limit_down_stocks),
            'stocks': limit_down_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'changeThreshold': changeThreshold
        }

    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œè·Œåœè‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/after_hours_volume_amount_high")
async def get_after_hours_volume_amount_high(
    limit: int = Query(50, description="è¿”å›çš„è‚¡ç¥¨æ•¸é‡"),
    changeThreshold: float = Query(0.0, description="æ¼²è·Œå¹…é–¾å€¼ï¼ˆ%ï¼‰")
):
    """ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡é«˜çš„è‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡çµ•å°å€¼æ’åºï¼Œç”±å¤§åˆ°å°ï¼‰"""
    logger.info(f"æ”¶åˆ° get_after_hours_volume_amount_high è«‹æ±‚: limit={limit}, changeThreshold={changeThreshold}")

    try:
        ensure_finlab_login()

        # ç²å–è‚¡ç¥¨æ•¸æ“š
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š"}

        close_df = close_df.sort_index()
        volume_df = volume_df.sort_index() if volume_df is not None else None
        
        latest_date = close_df.index[-1]
        previous_date = close_df.index[-2]
        
        latest_close = close_df.loc[latest_date]
        previous_close = close_df.loc[previous_date]
        
        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]
        
        volume_amount_stocks = []
        
        for stock_id in latest_close.index:
            try:
                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]
                
                if pd.isna(today_price) or pd.isna(yesterday_price):
                    continue
                
                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100
                
                # æª¢æŸ¥æ¼²è·Œå¹…é–¾å€¼
                if abs(change_percent) < changeThreshold:
                    continue

                # è¨ˆç®—æˆäº¤é‡‘é¡å’Œæˆäº¤é‡
                volume_amount = 0
                volume = 0
                if latest_volume is not None and stock_id in latest_volume.index:
                    vol = latest_volume[stock_id]
                    if not pd.isna(vol):
                        volume = int(vol)
                        volume_amount = volume * float(today_price)

                # è¨ˆç®—äº”æ—¥çµ±è¨ˆ
                stats = calculate_trading_stats(stock_id, latest_date, close_df)

                volume_amount_stocks.append({
                    'stock_id': stock_id,
                    'stock_name': get_stock_name(stock_id),
                    'industry': get_stock_industry(stock_id),
                    'current_price': float(today_price),
                    'change_amount': float(today_price - yesterday_price),
                    'change_percent': float(change_percent),
                    'volume': volume,
                    'volume_amount': volume_amount,
                    'up_days_5': stats['up_days'],
                    'five_day_change': stats['five_day_change'],
                    'date': latest_date.strftime('%Y-%m-%d'),
                    'previous_date': previous_date.strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                logger.warning(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # æŒ‰æˆäº¤é‡‘é¡æ’åºï¼ˆç”±å¤§åˆ°å°ï¼‰
        volume_amount_stocks.sort(key=lambda x: x['volume_amount'], reverse=True)
        volume_amount_stocks = volume_amount_stocks[:limit]
        
        return {
            'success': True,
            'total_count': len(volume_amount_stocks),
            'stocks': volume_amount_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'sort_by': 'volume_amount_high'
        }
        
    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡é«˜è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/after_hours_volume_amount_low")
async def get_after_hours_volume_amount_low(
    limit: int = Query(50, description="è¿”å›çš„è‚¡ç¥¨æ•¸é‡"),
    changeThreshold: float = Query(0.0, description="æ¼²è·Œå¹…é–¾å€¼ï¼ˆ%ï¼‰")
):
    """ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡ä½çš„è‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡çµ•å°å€¼æ’åºï¼Œç”±å°åˆ°å¤§ï¼‰"""
    logger.info(f"æ”¶åˆ° get_after_hours_volume_amount_low è«‹æ±‚: limit={limit}, changeThreshold={changeThreshold}")

    try:
        ensure_finlab_login()

        # ç²å–è‚¡ç¥¨æ•¸æ“š
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š"}

        close_df = close_df.sort_index()
        volume_df = volume_df.sort_index() if volume_df is not None else None
        
        latest_date = close_df.index[-1]
        previous_date = close_df.index[-2]
        
        latest_close = close_df.loc[latest_date]
        previous_close = close_df.loc[previous_date]
        
        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]
        
        volume_amount_stocks = []
        
        for stock_id in latest_close.index:
            try:
                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]
                
                if pd.isna(today_price) or pd.isna(yesterday_price):
                    continue
                
                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100
                
                # æª¢æŸ¥æ¼²è·Œå¹…é–¾å€¼
                if abs(change_percent) < changeThreshold:
                    continue

                # è¨ˆç®—æˆäº¤é‡‘é¡å’Œæˆäº¤é‡
                volume_amount = 0
                volume = 0
                if latest_volume is not None and stock_id in latest_volume.index:
                    vol = latest_volume[stock_id]
                    if not pd.isna(vol):
                        volume = int(vol)
                        volume_amount = volume * float(today_price)

                # è¨ˆç®—äº”æ—¥çµ±è¨ˆ
                stats = calculate_trading_stats(stock_id, latest_date, close_df)

                volume_amount_stocks.append({
                    'stock_id': stock_id,
                    'stock_name': get_stock_name(stock_id),
                    'industry': get_stock_industry(stock_id),
                    'current_price': float(today_price),
                    'change_amount': float(today_price - yesterday_price),
                    'change_percent': float(change_percent),
                    'volume': volume,
                    'volume_amount': volume_amount,
                    'up_days_5': stats['up_days'],
                    'five_day_change': stats['five_day_change'],
                    'date': latest_date.strftime('%Y-%m-%d'),
                    'previous_date': previous_date.strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                logger.warning(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # æŒ‰æˆäº¤é‡‘é¡æ’åºï¼ˆç”±å°åˆ°å¤§ï¼‰
        volume_amount_stocks.sort(key=lambda x: x['volume_amount'])
        volume_amount_stocks = volume_amount_stocks[:limit]
        
        return {
            'success': True,
            'total_count': len(volume_amount_stocks),
            'stocks': volume_amount_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'sort_by': 'volume_amount_low'
        }
        
    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡ä½è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/after_hours_volume_change_rate_high")
async def get_after_hours_volume_change_rate_high(
    limit: int = Query(50, description="è¿”å›çš„è‚¡ç¥¨æ•¸é‡"),
    changeThreshold: float = Query(0.0, description="æ¼²è·Œå¹…é–¾å€¼ï¼ˆ%ï¼‰")
):
    """ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡è®ŠåŒ–ç‡é«˜çš„è‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡è®ŠåŒ–ç‡æ’åºï¼Œç”±å¤§åˆ°å°ï¼‰"""
    logger.info(f"æ”¶åˆ° get_after_hours_volume_change_rate_high è«‹æ±‚: limit={limit}, changeThreshold={changeThreshold}")

    try:
        ensure_finlab_login()

        # ç²å–è‚¡ç¥¨æ•¸æ“š
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š"}

        close_df = close_df.sort_index()
        volume_df = volume_df.sort_index() if volume_df is not None else None
        
        latest_date = close_df.index[-1]
        previous_date = close_df.index[-2]
        day_before_date = close_df.index[-3]
        
        latest_close = close_df.loc[latest_date]
        previous_close = close_df.loc[previous_date]
        
        latest_volume = None
        previous_volume = None
        if volume_df is not None:
            if latest_date in volume_df.index:
                latest_volume = volume_df.loc[latest_date]
            if previous_date in volume_df.index:
                previous_volume = volume_df.loc[previous_date]
        
        volume_change_stocks = []
        
        for stock_id in latest_close.index:
            try:
                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]
                
                if pd.isna(today_price) or pd.isna(yesterday_price):
                    continue
                
                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100
                
                # æª¢æŸ¥æ¼²è·Œå¹…é–¾å€¼
                if abs(change_percent) < changeThreshold:
                    continue
                
                # è¨ˆç®—æˆäº¤é‡‘é¡è®ŠåŒ–ç‡
                today_volume_amount = 0
                yesterday_volume_amount = 0
                volume_change_rate = 0
                
                if latest_volume is not None and stock_id in latest_volume.index:
                    vol = latest_volume[stock_id]
                    if not pd.isna(vol):
                        today_volume_amount = int(vol) * float(today_price)
                
                if previous_volume is not None and stock_id in previous_volume.index:
                    vol = previous_volume[stock_id]
                    if not pd.isna(vol):
                        yesterday_volume_amount = int(vol) * float(yesterday_price)
                
                if yesterday_volume_amount > 0:
                    volume_change_rate = ((today_volume_amount - yesterday_volume_amount) / yesterday_volume_amount) * 100
                
                # è¨ˆç®—äº”æ—¥çµ±è¨ˆ
                stats = calculate_trading_stats(stock_id, latest_date, close_df)

                # è¨ˆç®—æˆäº¤é‡
                today_volume = 0
                if latest_volume is not None and stock_id in latest_volume.index and not pd.isna(latest_volume[stock_id]):
                    today_volume = int(latest_volume[stock_id])

                volume_change_stocks.append({
                    'stock_id': stock_id,
                    'stock_name': get_stock_name(stock_id),
                    'industry': get_stock_industry(stock_id),
                    'current_price': float(today_price),
                    'change_amount': float(today_price - yesterday_price),
                    'change_percent': float(change_percent),
                    'volume': today_volume,
                    'volume_amount': today_volume_amount,
                    'volume_change_rate': float(volume_change_rate),
                    'up_days_5': stats['up_days'],
                    'five_day_change': stats['five_day_change'],
                    'date': latest_date.strftime('%Y-%m-%d'),
                    'previous_date': previous_date.strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                logger.warning(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # æŒ‰æˆäº¤é‡‘é¡è®ŠåŒ–ç‡æ’åºï¼ˆç”±å¤§åˆ°å°ï¼‰
        volume_change_stocks.sort(key=lambda x: x['volume_change_rate'], reverse=True)
        volume_change_stocks = volume_change_stocks[:limit]
        
        return {
            'success': True,
            'total_count': len(volume_change_stocks),
            'stocks': volume_change_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'sort_by': 'volume_change_rate_high'
        }
        
    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡è®ŠåŒ–ç‡é«˜è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/after_hours_volume_change_rate_low")
async def get_after_hours_volume_change_rate_low(
    limit: int = Query(50, description="è¿”å›çš„è‚¡ç¥¨æ•¸é‡"),
    changeThreshold: float = Query(0.0, description="æ¼²è·Œå¹…é–¾å€¼ï¼ˆ%ï¼‰")
):
    """ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡è®ŠåŒ–ç‡ä½çš„è‚¡ç¥¨ï¼ˆæˆäº¤é‡‘é¡è®ŠåŒ–ç‡æ’åºï¼Œç”±å°åˆ°å¤§ï¼‰"""
    logger.info(f"æ”¶åˆ° get_after_hours_volume_change_rate_low è«‹æ±‚: limit={limit}, changeThreshold={changeThreshold}")

    try:
        ensure_finlab_login()

        # ç²å–è‚¡ç¥¨æ•¸æ“š
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–è‚¡ç¥¨æ•¸æ“š"}

        close_df = close_df.sort_index()
        volume_df = volume_df.sort_index() if volume_df is not None else None
        
        latest_date = close_df.index[-1]
        previous_date = close_df.index[-2]
        day_before_date = close_df.index[-3]
        
        latest_close = close_df.loc[latest_date]
        previous_close = close_df.loc[previous_date]
        
        latest_volume = None
        previous_volume = None
        if volume_df is not None:
            if latest_date in volume_df.index:
                latest_volume = volume_df.loc[latest_date]
            if previous_date in volume_df.index:
                previous_volume = volume_df.loc[previous_date]
        
        volume_change_stocks = []
        
        for stock_id in latest_close.index:
            try:
                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]
                
                if pd.isna(today_price) or pd.isna(yesterday_price):
                    continue
                
                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100
                
                # æª¢æŸ¥æ¼²è·Œå¹…é–¾å€¼
                if abs(change_percent) < changeThreshold:
                    continue
                
                # è¨ˆç®—æˆäº¤é‡‘é¡è®ŠåŒ–ç‡
                today_volume_amount = 0
                yesterday_volume_amount = 0
                volume_change_rate = 0
                
                if latest_volume is not None and stock_id in latest_volume.index:
                    vol = latest_volume[stock_id]
                    if not pd.isna(vol):
                        today_volume_amount = int(vol) * float(today_price)
                
                if previous_volume is not None and stock_id in previous_volume.index:
                    vol = previous_volume[stock_id]
                    if not pd.isna(vol):
                        yesterday_volume_amount = int(vol) * float(yesterday_price)
                
                if yesterday_volume_amount > 0:
                    volume_change_rate = ((today_volume_amount - yesterday_volume_amount) / yesterday_volume_amount) * 100
                
                # è¨ˆç®—äº”æ—¥çµ±è¨ˆ
                stats = calculate_trading_stats(stock_id, latest_date, close_df)

                # è¨ˆç®—æˆäº¤é‡
                today_volume = 0
                if latest_volume is not None and stock_id in latest_volume.index and not pd.isna(latest_volume[stock_id]):
                    today_volume = int(latest_volume[stock_id])

                volume_change_stocks.append({
                    'stock_id': stock_id,
                    'stock_name': get_stock_name(stock_id),
                    'industry': get_stock_industry(stock_id),
                    'current_price': float(today_price),
                    'change_amount': float(today_price - yesterday_price),
                    'change_percent': float(change_percent),
                    'volume': today_volume,
                    'volume_amount': today_volume_amount,
                    'volume_change_rate': float(volume_change_rate),
                    'up_days_5': stats['up_days'],
                    'five_day_change': stats['five_day_change'],
                    'date': latest_date.strftime('%Y-%m-%d'),
                    'previous_date': previous_date.strftime('%Y-%m-%d')
                })
                
            except Exception as e:
                logger.warning(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # æŒ‰æˆäº¤é‡‘é¡è®ŠåŒ–ç‡æ’åºï¼ˆç”±å°åˆ°å¤§ï¼‰
        volume_change_stocks.sort(key=lambda x: x['volume_change_rate'])
        volume_change_stocks = volume_change_stocks[:limit]
        
        return {
            'success': True,
            'total_count': len(volume_change_stocks),
            'stocks': volume_change_stocks,
            'timestamp': get_current_time().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'sort_by': 'volume_change_rate_low'
        }
        
    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæˆäº¤é‡‘é¡è®ŠåŒ–ç‡ä½è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/stock_mapping.json")
async def get_stock_mapping():
    """ç²å–å®Œæ•´è‚¡ç¥¨æ˜ å°„è¡¨ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰"""
    logger.info("æ”¶åˆ° stock_mapping è«‹æ±‚")

    try:
        return {
            "success": True,
            "data": stock_mapping,
            "count": len(stock_mapping),
            "timestamp": get_current_time().isoformat()
        }
    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨æ˜ å°„è¡¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/industries")
async def get_industries():
    """ç²å–æ‰€æœ‰ç”¢æ¥­é¡åˆ¥"""
    logger.info("æ”¶åˆ° industries è«‹æ±‚")

    try:
        industries = set()
        for stock_code, info in stock_mapping.items():
            if 'industry' in info:
                industries.add(info['industry'])

        industries_list = [{"id": ind, "name": ind} for ind in sorted(list(industries))]

        result = {
            "success": True,
            "data": industries_list,
            "count": len(industries_list),
            "timestamp": get_current_time().isoformat()
        }

        logger.info(f"è¿”å› industries æ•¸æ“š: {len(result['data'])} æ¢è¨˜éŒ„")
        return result
    except Exception as e:
        logger.error(f"ç²å–ç”¢æ¥­é¡åˆ¥å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/stocks_by_industry")
async def get_stocks_by_industry(industry: str = Query(..., description="ç”¢æ¥­é¡åˆ¥")):
    """æ ¹æ“šç”¢æ¥­ç²å–è‚¡ç¥¨åˆ—è¡¨"""
    logger.info(f"æ”¶åˆ° stocks_by_industry è«‹æ±‚: industry={industry}")

    try:
        stocks = []
        for stock_code, info in stock_mapping.items():
            if info.get('industry') == industry:
                stocks.append({
                    "stock_id": stock_code,
                    "stock_name": info.get('company_name', stock_code),
                    "industry": industry
                })

        result = {
            "success": True,
            "data": stocks,
            "count": len(stocks),
            "industry": industry,
            "timestamp": get_current_time().isoformat()
        }

        logger.info(f"è¿”å› {industry} ç”¢æ¥­è‚¡ç¥¨: {len(result['data'])} æ”¯")
        return result
    except Exception as e:
        logger.error(f"ç²å–ç”¢æ¥­è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/api/get_ohlc")
async def get_ohlc(stock_id: str = Query(..., description="è‚¡ç¥¨ä»£ç¢¼")):
    """ç²å–ç‰¹å®šè‚¡ç¥¨çš„ OHLC æ•¸æ“š"""
    logger.info(f"æ”¶åˆ° get_ohlc è«‹æ±‚: stock_id={stock_id}")

    try:
        ensure_finlab_login()

        open_df = data.get('price:é–‹ç›¤åƒ¹')
        high_df = data.get('price:æœ€é«˜åƒ¹')
        low_df = data.get('price:æœ€ä½åƒ¹')
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if stock_id not in open_df.columns:
            return {"error": f"Stock ID {stock_id} not found."}

        ohlcv_df = pd.DataFrame({
            'open': open_df[stock_id],
            'high': high_df[stock_id],
            'low': low_df[stock_id],
            'close': close_df[stock_id],
            'volume': volume_df[stock_id]
        })

        ohlcv_df = ohlcv_df.dropna().reset_index()
        ohlcv_df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']

        one_year_ago = datetime.today() - timedelta(days=365)
        ohlcv_df = ohlcv_df[ohlcv_df['date'] >= one_year_ago]

        return json.loads(ohlcv_df.to_json(orient="records", date_format="iso"))
    except Exception as e:
        logger.error(f"ç²å– OHLC æ•¸æ“šå¤±æ•—: {e}")
        return {"error": str(e)}

# ==================== ç›¤ä¸­è§¸ç™¼å™¨åŠŸèƒ½ ====================

# CMoney API é…ç½®
CMONEY_API_BASE = "https://asterisk-chipsapi.cmoney.tw"

def get_forum_200_credentials():
    """å¾ç’°å¢ƒè®Šæ•¸ç²å– forum_200 æ†‘è­‰"""
    email = os.getenv("FORUM_200_EMAIL")
    password = os.getenv("FORUM_200_PASSWORD")
    member_id = os.getenv("FORUM_200_MEMBER_ID", "9505546")  # é è¨­å€¼

    # è¨˜éŒ„ç’°å¢ƒè®Šæ•¸ç‹€æ…‹ï¼ˆä¸è¨˜éŒ„å¯¦éš›å€¼ä»¥ä¿è­·éš±ç§ï¼‰
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_EMAIL å­˜åœ¨: {email is not None}")
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_PASSWORD å­˜åœ¨: {password is not None}")
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_MEMBER_ID: {member_id}")

    if not email or not password:
        # æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯
        missing = []
        if not email:
            missing.append("FORUM_200_EMAIL")
        if not password:
            missing.append("FORUM_200_PASSWORD")
        error_msg = f"ç¼ºå°‘ç’°å¢ƒè®Šæ•¸: {', '.join(missing)}"
        logger.error(f"âŒ {error_msg}")
        raise Exception(error_msg)

    logger.info(f"âœ… [æ†‘è­‰æª¢æŸ¥] æˆåŠŸè¼‰å…¥ forum_200 æ†‘è­‰: {email}")
    return {
        "email": email,
        "password": password,
        "member_id": member_id
    }

_token_cache = {
    "token": None,
    "expires_at": None,
    "created_at": None
}

async def get_dynamic_auth_token() -> str:
    """ä½¿ç”¨ forum_200 KOL æ†‘è­‰å‹•æ…‹å–å¾— CMoney API token"""
    try:
        # Check token cache validity
        if _token_cache["token"] and _token_cache["expires_at"]:
            current_time = get_current_time()
            expires_at = _token_cache["expires_at"]

            # Handle timezone-naive datetime from cmoney_client
            if expires_at.tzinfo is None:
                # Assume naive datetime is in local timezone (Taipei)
                taipei_tz = pytz.timezone('Asia/Taipei')
                expires_at = taipei_tz.localize(expires_at)

            if current_time < expires_at:
                logger.info("âœ… ä½¿ç”¨å¿«å–çš„ CMoney API token")
                return _token_cache["token"]

        logger.info("ğŸ” é–‹å§‹ä½¿ç”¨ forum_200 æ†‘è­‰ç™»å…¥ CMoney...")

        # å°‡ src è·¯å¾‘åŠ å…¥ Python path
        src_path = '/app/src'
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials

        cmoney_client = CMoneyClient()
        forum_credentials = get_forum_200_credentials()
        credentials = LoginCredentials(
            email=forum_credentials["email"],
            password=forum_credentials["password"]
        )

        login_result = await cmoney_client.login(credentials)

        if not login_result or not login_result.token:
            raise Exception("forum_200 ç™»å…¥å¤±æ•—")

        _token_cache["token"] = login_result.token
        _token_cache["expires_at"] = login_result.expires_at
        _token_cache["created_at"] = get_current_time()

        logger.info(f"âœ… forum_200 ç™»å…¥æˆåŠŸï¼Œtoken æœ‰æ•ˆæœŸè‡³: {login_result.expires_at}")
        return login_result.token

    except Exception as e:
        logger.error(f"âŒ å‹•æ…‹å–å¾— CMoney API token å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"èªè­‰å¤±æ•—: {str(e)}")

@app.post("/api/intraday-trigger/execute")
async def get_intraday_trigger_stocks(request: Request):
    """ç²å–ç›¤ä¸­è§¸ç™¼å™¨è‚¡ç¥¨åˆ—è¡¨"""
    try:
        # å¾ POST body ç²å–é…ç½®
        body = await request.json()
        endpoint = body.get("endpoint")
        processing = body.get("processing", [])
        
        logger.info(f"æ”¶åˆ°ç›¤ä¸­è§¸ç™¼å™¨è«‹æ±‚: endpoint={endpoint}, processing={processing}")
        
        if not endpoint:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘ endpoint åƒæ•¸")

        # æº–å‚™è«‹æ±‚åƒæ•¸
        columns = "äº¤æ˜“æ™‚é–“,å‚³è¼¸åºè™Ÿ,å…§å¤–ç›¤æ——æ¨™,å³æ™‚æˆäº¤åƒ¹,å³æ™‚æˆäº¤é‡,æœ€ä½åƒ¹,æœ€é«˜åƒ¹,æ¨™çš„,æ¼²è·Œ,æ¼²è·Œå¹…,ç´¯è¨ˆæˆäº¤ç¸½é¡,ç´¯è¨ˆæˆäº¤é‡,é–‹ç›¤åƒ¹"

        request_data = {
            "AppId": 2,
            "Guid": "583defeb-f7cb-49e7-964d-d0817e944e4f",
            "Processing": processing
        }

        # å‹•æ…‹å–å¾—èªè­‰ token
        logger.info("ğŸ” [ç›¤ä¸­è§¸ç™¼å™¨] é–‹å§‹å‹•æ…‹å–å¾—èªè­‰ token...")
        try:
            auth_token = await get_dynamic_auth_token()
            logger.info(f"âœ… [ç›¤ä¸­è§¸ç™¼å™¨] æˆåŠŸå–å¾— token: {auth_token[:20]}...")
        except Exception as e:
            logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] å‹•æ…‹èªè­‰å¤±æ•—: {e}")
            raise

        # æº–å‚™è«‹æ±‚é ­
        headers = {
            "Accept-Encoding": "gzip",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {auth_token}",
            "cmoneyapi-trace-context": '{"appVersion":"10.111.0","osName":"iOS","platform":1,"manufacturer":"Apple","osVersion":"18.6.2","appId":2,"model":"iPhone15,2"}'
        }

        # ç™¼é€è«‹æ±‚åˆ° CMoney API
        logger.info(f"ğŸŒ [ç›¤ä¸­è§¸ç™¼å™¨] ç™¼é€è«‹æ±‚åˆ° CMoney API: {endpoint}")
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{endpoint}?columns={columns}",
                json=request_data,
                headers=headers
            )

            logger.info(f"ğŸ“¡ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API éŸ¿æ‡‰ç‹€æ…‹: {response.status_code}")

            if response.status_code != 200:
                logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API è«‹æ±‚å¤±æ•—: HTTP {response.status_code}, éŸ¿æ‡‰: {response.text}")
                raise HTTPException(status_code=500, detail=f"CMoney API è«‹æ±‚å¤±æ•—: {response.status_code}")

            # è§£æéŸ¿æ‡‰æ•¸æ“š
            raw_data = response.json()
            logger.info(f"ğŸ“Š [ç›¤ä¸­è§¸ç™¼å™¨] æ”¶åˆ°æ•¸æ“š: {len(raw_data)} ç­†è¨˜éŒ„")

            # æå–è‚¡ç¥¨ä»£ç¢¼ä¸¦æ˜ å°„åˆ°è‚¡ç¥¨åç¨±å’Œç”¢æ¥­
            stock_codes = [item[7] for item in raw_data if len(item) > 7 and item[7]]

            # æ§‹å»ºåŒ…å«è‚¡ç¥¨åç¨±å’Œç”¢æ¥­çš„å®Œæ•´æ•¸æ“š
            stocks_with_info = []
            for stock_code in stock_codes:
                stock_name = get_stock_name(stock_code)
                stock_industry = get_stock_industry(stock_code)
                stocks_with_info.append({
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "industry": stock_industry
                })

            logger.info(f"âœ… [ç›¤ä¸­è§¸ç™¼å™¨] åŸ·è¡ŒæˆåŠŸï¼Œç²å– {len(stocks_with_info)} æ”¯è‚¡ç¥¨")
            # æå–è‚¡ç¥¨åˆ—è¡¨é¿å… f-string åµŒå¥—å•é¡Œ
            stock_list = [f"{s['stock_code']}({s['stock_name']})" for s in stocks_with_info[:5]]
            logger.info(f"ğŸ“‹ [ç›¤ä¸­è§¸ç™¼å™¨] è‚¡ç¥¨åˆ—è¡¨: {stock_list}...")

            return {
                "success": True,
                "stocks": stock_codes,  # è¿”å›è‚¡ç¥¨ä»£ç¢¼å­—ç¬¦ä¸²æ•¸çµ„ï¼Œèˆ‡å‰ç«¯æœŸæœ›æ ¼å¼ä¸€è‡´
                "data": raw_data,  # ä¿ç•™åŸå§‹ CMoney æ•¸æ“š
                "count": len(stock_codes)
            }

    except httpx.TimeoutException:
        logger.error("âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API è«‹æ±‚è¶…æ™‚")
        raise HTTPException(status_code=504, detail="CMoney API è«‹æ±‚è¶…æ™‚")
    except httpx.ConnectError:
        logger.error("âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API é€£æ¥å¤±æ•—")
        raise HTTPException(status_code=503, detail="CMoney API é€£æ¥å¤±æ•—")
    except Exception as e:
        logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] åŸ·è¡Œå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"åŸ·è¡Œå¤±æ•—: {str(e)}")

# Helper function for intraday triggers
async def execute_cmoney_intraday_trigger(processing: list, trigger_name: str):
    """åŸ·è¡Œ CMoney ç›¤ä¸­è§¸ç™¼å™¨çš„é€šç”¨å‡½æ•¸"""
    try:
        endpoint = "https://asterisk-chipsapi.cmoney.tw/AdditionInformationRevisit/api/GetAll/StockCalculation"
        columns = "äº¤æ˜“æ™‚é–“,å‚³è¼¸åºè™Ÿ,å…§å¤–ç›¤æ——æ¨™,å³æ™‚æˆäº¤åƒ¹,å³æ™‚æˆäº¤é‡,æœ€ä½åƒ¹,æœ€é«˜åƒ¹,æ¨™çš„,æ¼²è·Œ,æ¼²è·Œå¹…,ç´¯è¨ˆæˆäº¤ç¸½é¡,ç´¯è¨ˆæˆäº¤é‡,é–‹ç›¤åƒ¹"

        request_data = {
            "AppId": 2,
            "Guid": "583defeb-f7cb-49e7-964d-d0817e944e4f",
            "Processing": processing
        }

        # å‹•æ…‹å–å¾—èªè­‰ token
        auth_token = await get_dynamic_auth_token()

        headers = {
            "Accept-Encoding": "gzip",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {auth_token}",
            "cmoneyapi-trace-context": '{"appVersion":"10.111.0","osName":"iOS","platform":1,"manufacturer":"Apple","osVersion":"18.6.2","appId":2,"model":"iPhone15,2"}'
        }

        # ç™¼é€è«‹æ±‚åˆ° CMoney API
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{endpoint}?columns={columns}",
                json=request_data,
                headers=headers
            )

            if response.status_code != 200:
                raise HTTPException(status_code=500, detail=f"CMoney API è«‹æ±‚å¤±æ•—: {response.status_code}")

            raw_data = response.json()
            # CMoney API columns: äº¤æ˜“æ™‚é–“,å‚³è¼¸åºè™Ÿ,å…§å¤–ç›¤æ——æ¨™,å³æ™‚æˆäº¤åƒ¹,å³æ™‚æˆäº¤é‡,æœ€ä½åƒ¹,æœ€é«˜åƒ¹,æ¨™çš„,æ¼²è·Œ,æ¼²è·Œå¹…,ç´¯è¨ˆæˆäº¤ç¸½é¡,ç´¯è¨ˆæˆäº¤é‡,é–‹ç›¤åƒ¹
            # Index mapping: 0=äº¤æ˜“æ™‚é–“, 1=å‚³è¼¸åºè™Ÿ, 2=å…§å¤–ç›¤æ——æ¨™, 3=å³æ™‚æˆäº¤åƒ¹, 4=å³æ™‚æˆäº¤é‡, 5=æœ€ä½åƒ¹, 6=æœ€é«˜åƒ¹, 7=æ¨™çš„, 8=æ¼²è·Œ, 9=æ¼²è·Œå¹…, 10=ç´¯è¨ˆæˆäº¤ç¸½é¡, 11=ç´¯è¨ˆæˆäº¤é‡, 12=é–‹ç›¤åƒ¹

            stocks_with_info = []
            for item in raw_data:
                if len(item) >= 13 and item[7]:  # Ensure we have all fields
                    stock_code = item[7]

                    # Calculate 5-day trading statistics
                    # Note: Historical data not available in intraday context, using defaults
                    stats = {"up_days": 0, "five_day_change": 0.0}

                    stocks_with_info.append({
                        "stock_code": stock_code,
                        "stock_name": get_stock_name(stock_code),
                        "industry": get_stock_industry(stock_code),
                        "current_price": float(item[3]) if item[3] else 0.0,  # å³æ™‚æˆäº¤åƒ¹
                        "open_price": float(item[12]) if item[12] else 0.0,  # é–‹ç›¤åƒ¹
                        "high_price": float(item[6]) if item[6] else 0.0,  # æœ€é«˜åƒ¹
                        "low_price": float(item[5]) if item[5] else 0.0,  # æœ€ä½åƒ¹
                        "change_amount": float(item[8]) if item[8] else 0.0,  # æ¼²è·Œ
                        "change_percent": float(item[9]) if item[9] else 0.0,  # æ¼²è·Œå¹…
                        "volume": int(item[11]) if item[11] else 0,  # ç´¯è¨ˆæˆäº¤é‡
                        "volume_amount": float(item[10]) if item[10] else 0.0,  # ç´¯è¨ˆæˆäº¤ç¸½é¡
                        "trade_time": item[0] if item[0] else "",  # äº¤æ˜“æ™‚é–“
                        # Add 5-day statistics
                        "up_days_5": stats['up_days'],  # äº”æ—¥ä¸Šæ¼²å¤©æ•¸
                        "five_day_change": stats['five_day_change']  # äº”æ—¥æ¼²è·Œå¹…
                    })

            logger.info(f"âœ… [{trigger_name}] ç²å– {len(stocks_with_info)} æ”¯è‚¡ç¥¨")

            return {
                "success": True,
                "total_count": len(stocks_with_info),
                "stocks": stocks_with_info,
                "timestamp": get_current_time().isoformat(),
                "trigger_type": trigger_name
            }

    except Exception as e:
        logger.error(f"âŒ [{trigger_name}] åŸ·è¡Œå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"{trigger_name} åŸ·è¡Œå¤±æ•—: {str(e)}")

# 6 å€‹ç¨ç«‹çš„ç›¤ä¸­è§¸ç™¼å™¨ç«¯é»

@app.get("/api/intraday/gainers-by-amount")
async def get_intraday_gainers_by_amount(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šæ¼²å¹…æ’åº+æˆäº¤é¡

    æŒ‰æˆäº¤é¡æ’åºï¼Œç¯©é¸å‡ºæ¼²å¹…æœ€å¤§çš„è‚¡ç¥¨ï¼ˆæœªæ¼²åœï¼‰
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºç•¶æ—¥å¼·å‹¢ä¸Šæ¼²ä¸”æˆäº¤æ´»èºçš„è‚¡ç¥¨
    """
    processing = [
        {"ParameterJson":"{ \"TargetPropertyNamePath\" : [ \"TotalTransactionAmount\"]}","ProcessType":"DescOrder"},
        {"ProcessType":"EqualValueFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"Commodity\", \"IsChipsKPopularStocksSortSubject\"], \"Value\": true}"},
        {"ProcessType":"LessThanColumnsFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"StrikePrice\"], \"ComparePropertyNamePath\": [\"Commodity\" , \"LimitUp\"]}"},
        {"ProcessType":"MoreThanValueFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"ChangeRange\"], \"Value\": 0 }"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"ChangeRange\"]}","ProcessType":"DescOrder"},
        {"ProcessType":"ThenDescOrder","ParameterJson":"{\"TargetPropertyNamePath\": [\"TotalVolume\"]}"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"CommKey\"]}","ProcessType":"ThenAscOrder"},
        {"ProcessType":"TakeCount","ParameterJson":f"{{\"Count\":{limit}}}"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "æ¼²å¹…æ’åº+æˆäº¤é¡")

@app.get("/api/intraday/volume-leaders")
async def get_intraday_volume_leaders(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šæˆäº¤é‡æ’åº

    æŒ‰æˆäº¤é‡æ’åºï¼Œæ‰¾å‡ºç•¶æ—¥æˆäº¤é‡æœ€å¤§çš„ç†±é–€è‚¡ç¥¨
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºå¸‚å ´é—œæ³¨åº¦æœ€é«˜ã€äº¤æ˜“æœ€æ´»èºçš„è‚¡ç¥¨
    """
    processing = [
        {"ProcessType":"EqualValueFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"Commodity\", \"IsChipsKPopularStocksSortSubject\"], \"Value\": true}"},
        {"ProcessType":"DescOrder","ParameterJson":"{\"TargetPropertyNamePath\" :[\"TotalVolume\"]}"},
        {"ProcessType":"ThenDescOrder","ParameterJson":"{\"TargetPropertyNamePath\" :[\"ChangeRange\"]}"},
        {"ParameterJson":"{\"TargetPropertyNamePath\" :[\"CommKey\"]}","ProcessType":"ThenAscOrder"},
        {"ParameterJson":f"{{\"Count\":{limit}}}","ProcessType":"TakeCount"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "æˆäº¤é‡æ’åº")

@app.get("/api/intraday/amount-leaders")
async def get_intraday_amount_leaders(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šæˆäº¤é¡æ’åº

    æŒ‰æˆäº¤é¡æ’åºï¼Œæ‰¾å‡ºç•¶æ—¥æˆäº¤é‡‘é¡æœ€å¤§çš„è‚¡ç¥¨
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºè³‡é‡‘æµå…¥æœ€å¤šã€å¤§æˆ¶é—œæ³¨çš„è‚¡ç¥¨
    """
    processing = [
        {"ProcessType":"EqualValueFilter","ParameterJson":"{\"TargetPropertyNamePath\":[\"Commodity\", \"IsChipsKPopularStocksSortSubject\"], \"Value\": true}"},
        {"ProcessType":"DescOrder","ParameterJson":"{\"TargetPropertyNamePath\":[\"TotalTransactionAmount\"]}"},
        {"ParameterJson":"{\"TargetPropertyNamePath\":[\"TotalVolume\" ]}","ProcessType":"ThenDescOrder"},
        {"ProcessType":"ThenDescOrder","ParameterJson":"{\"TargetPropertyNamePath\":[\"ChangeRange\"]}"},
        {"ProcessType":"TakeCount","ParameterJson":f"{{\"Count\":{limit}}}"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "æˆäº¤é¡æ’åº")

@app.get("/api/intraday/limit-down")
async def get_intraday_limit_down(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šè·Œåœç¯©é¸

    ç¯©é¸ç•¶æ—¥è·Œåœçš„è‚¡ç¥¨
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºç•¶æ—¥è¡¨ç¾æœ€å¼±å‹¢çš„è‚¡ç¥¨
    """
    processing = [
        {"ProcessType":"EqualValueFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"Commodity\", \"IsChipsKPopularStocksSortSubject\"], \"Value\": true}"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"StrikePrice\"], \"ComparePropertyNamePath\": [\"Commodity\", \"LimitDown\"]}","ProcessType":"EqualColumnsFilter"},
        {"ProcessType":"AscOrder","ParameterJson":"{\"TargetPropertyNamePath\":[\"ChangeRange\"]}"},
        {"ProcessType":"ThenDescOrder","ParameterJson":"{\"TargetPropertyNamePath\":[\"TotalVolume\"]}"},
        {"ParameterJson":f"{{\"Count\":{limit}}}","ProcessType":"TakeCount"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "è·Œåœç¯©é¸")

@app.get("/api/intraday/limit-up")
async def get_intraday_limit_up(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šæ¼²åœç¯©é¸

    ç¯©é¸ç•¶æ—¥æ¼²åœçš„è‚¡ç¥¨
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºç•¶æ—¥è¡¨ç¾æœ€å¼·å‹¢çš„è‚¡ç¥¨
    """
    processing = [
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"Commodity\", \"IsChipsKPopularStocksSortSubject\"], \"Value\": true}","ProcessType":"EqualValueFilter"},
        {"ProcessType":"EqualColumnsFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"StrikePrice\"], \"ComparePropertyNamePath\": [\"Commodity\", \"LimitUp\"]}"},
        {"ProcessType":"DescOrder","ParameterJson":"{\"TargetPropertyNamePath\": [\"ChangeRange\"]}"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"TotalVolume\"]}","ProcessType":"ThenDescOrder"},
        {"ProcessType":"TakeCount","ParameterJson":f"{{\"Count\":{limit}}}"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "æ¼²åœç¯©é¸")

@app.get("/api/intraday/limit-down-by-amount")
async def get_intraday_limit_down_by_amount(limit: int = Query(20, description="è¿”å›è‚¡ç¥¨æ•¸é‡")):
    """
    ç›¤ä¸­è§¸ç™¼å™¨ï¼šè·Œåœç¯©é¸+æˆäº¤é¡

    æŒ‰æˆäº¤é¡æ’åºï¼Œç¯©é¸å‡ºè·Œå¹…å¤§ä¸”æˆäº¤æ´»èºçš„è‚¡ç¥¨ï¼ˆæ¥è¿‘è·Œåœï¼‰
    é©ç”¨å ´æ™¯ï¼šæ‰¾å‡ºç•¶æ—¥å¼±å‹¢ä¸‹è·Œä¸”è³£å£“å¤§çš„è‚¡ç¥¨
    """
    processing = [
        {"ParameterJson":"{ \"TargetPropertyNamePath\" : [ \"TotalTransactionAmount\"]}","ProcessType":"DescOrder"},
        {"ParameterJson":"{\"TargetPropertyNamePath\":[\"Commodity\", \"IsChipsKPopularStocksSortSubject\" ], \"Value\": true}","ProcessType":"EqualValueFilter"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"StrikePrice\"], \"ComparePropertyNamePath\": [\"Commodity\", \"LimitDown\"]}","ProcessType":"MoreThanColumnsFilter"},
        {"ProcessType":"LessThanValueFilter","ParameterJson":"{\"TargetPropertyNamePath\": [\"ChangeRange\"], \"Value\": 0 }"},
        {"ParameterJson":"{\"TargetPropertyNamePath\": [\"ChangeRange\"]}","ProcessType":"AscOrder"},
        {"ProcessType":"ThenDescOrder","ParameterJson":"{\"TargetPropertyNamePath\": [\"TotalVolume\"]}"},
        {"ProcessType":"ThenAscOrder","ParameterJson":"{\"TargetPropertyNamePath\": [\"CommKey\"]}"},
        {"ParameterJson":f"{{\"Count\":{limit}}}","ProcessType":"TakeCount"}
    ]
    return await execute_cmoney_intraday_trigger(processing, "è·Œåœç¯©é¸+æˆäº¤é¡")

# ==================== Posting Service åŠŸèƒ½ ====================

@app.post("/api/posting")
async def create_posting(request: Request):
    """å‰µå»ºè²¼æ–‡"""
    logger.info("æ”¶åˆ° create_posting è«‹æ±‚")

    try:
        body = await request.json()
        logger.info(f"è²¼æ–‡å…§å®¹: {body}")

        # ğŸ”¥ Validate kol_serial is provided
        kol_serial_raw = body.get('kol_serial')
        if not kol_serial_raw:
            logger.error(f"âŒ Missing kol_serial in /api/posting request: {body}")
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "message": "kol_serial is required",
                    "timestamp": get_current_time().isoformat()
                }
            )

        try:
            kol_serial = int(kol_serial_raw)
        except (ValueError, TypeError):
            logger.error(f"âŒ Invalid kol_serial format: {kol_serial_raw}")
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "message": f"Invalid kol_serial format: {kol_serial_raw}",
                    "timestamp": get_current_time().isoformat()
                }
            )

        # ç”Ÿæˆå”¯ä¸€çš„ post_id
        import uuid
        post_id = str(uuid.uuid4())

        # æº–å‚™æ’å…¥æ•¸æ“š
        post_data = {
            'post_id': post_id,
            'created_at': get_current_time(),
            'updated_at': get_current_time(),
            'session_id': body.get('session_id', 1),
            'kol_serial': kol_serial,
            'kol_nickname': body.get('kol_nickname', f'KOL-{kol_serial}'),
            'kol_persona': body.get('kol_persona', 'åˆ†æå¸«'),
            'stock_code': body.get('stock_code', ''),
            'stock_name': body.get('stock_name', ''),
            'title': body.get('title', ''),
            'content': body.get('content', ''),
            'content_md': body.get('content_md', ''),
            'status': body.get('status', 'draft'),
            'reviewer_notes': body.get('reviewer_notes'),
            'approved_by': body.get('approved_by'),
            'approved_at': body.get('approved_at'),
            'scheduled_at': body.get('scheduled_at'),
            'published_at': body.get('published_at'),
            'cmoney_post_id': body.get('cmoney_post_id'),
            'cmoney_post_url': body.get('cmoney_post_url'),
            'publish_error': body.get('publish_error'),
            'views': body.get('views', 0),
            'likes': body.get('likes', 0),
            'comments': body.get('comments', 0),
            'shares': body.get('shares', 0),
            'topic_id': body.get('topic_id'),
            'topic_title': body.get('topic_title'),
            'technical_analysis': json.dumps(body.get('technical_analysis', {})) if body.get('technical_analysis') else None,
            'serper_data': json.dumps(body.get('serper_data', {})) if body.get('serper_data') else None,
            'quality_score': body.get('quality_score'),
            'ai_detection_score': body.get('ai_detection_score'),
            'risk_level': body.get('risk_level'),
            'generation_params': json.dumps(body.get('generation_params', {})) if body.get('generation_params') else None,
            'commodity_tags': json.dumps(body.get('commodity_tags', [])) if body.get('commodity_tags') else None,
            'alternative_versions': json.dumps(body.get('alternative_versions', {})) if body.get('alternative_versions') else None
        }
        
        # æ’å…¥åˆ°æ•¸æ“šåº«
        if db_pool:
            with conn.cursor() as cursor:
                insert_sql = """
                    INSERT INTO post_records (
                        post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                        kol_persona, stock_code, stock_name, title, content, content_md, 
                        status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                        published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                        views, likes, comments, shares, topic_id, topic_title, 
                        technical_analysis, serper_data, quality_score, ai_detection_score, 
                        risk_level, generation_params, commodity_tags, alternative_versions
                    ) VALUES (
                        %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                        %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                        %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                        %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                        %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                        %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                        %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                        %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                        %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                    )
                """
                
                cursor.execute(insert_sql, post_data)
                conn.commit()
                logger.info(f"âœ… è²¼æ–‡å·²æ’å…¥æ•¸æ“šåº«: {post_id}")
        else:
            logger.warning("âš ï¸ æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨ï¼Œç„¡æ³•ä¿å­˜è²¼æ–‡")

        result = {
            "success": True,
            "message": "è²¼æ–‡å‰µå»ºæˆåŠŸ",
            "post_id": post_id,
            "data": body,
            "timestamp": get_current_time().isoformat()
        }

        logger.info("è²¼æ–‡å‰µå»ºæˆåŠŸ")
        return result

    except Exception as e:
        logger.error(f"è²¼æ–‡å‰µå»ºå¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"è²¼æ–‡å‰µå»ºå¤±æ•—: {str(e)}",
                "timestamp": get_current_time().isoformat()
            }
        )

@app.post("/api/manual-posting")
async def manual_posting(request: Request):
    """æ‰‹å‹•è²¼æ–‡ - ç”Ÿæˆå…§å®¹ä¸¦å¯«å…¥æ•¸æ“šåº«"""
    logger.info("æ”¶åˆ° manual_posting è«‹æ±‚")

    conn = None
    try:
        body = await request.json()
        logger.info(f"æ‰‹å‹•è²¼æ–‡åƒæ•¸: stock_code={body.get('stock_code')}, kol_serial={body.get('kol_serial')}, session_id={body.get('session_id')}")

        # æå–åƒæ•¸
        stock_code = body.get('stock_code', '')
        stock_name = body.get('stock_name', stock_code)

        # ğŸ”¥ FIX: Validate kol_serial is provided (no default to 200!)
        kol_serial_raw = body.get('kol_serial')
        if not kol_serial_raw:
            logger.error(f"âŒ Missing kol_serial in request body: {body}")
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "message": "kol_serial is required. Please select a KOL in step 5.",
                    "timestamp": get_current_time().isoformat()
                }
            )

        try:
            kol_serial = int(kol_serial_raw)
            logger.info(f"âœ… Using KOL serial: {kol_serial}")
        except (ValueError, TypeError):
            logger.error(f"âŒ Invalid kol_serial format: {kol_serial_raw}")
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "message": f"Invalid kol_serial format: {kol_serial_raw}. Must be a number.",
                    "timestamp": get_current_time().isoformat()
                }
            )

        kol_persona = body.get('kol_persona', 'technical')
        session_id = body.get('session_id')
        trigger_type = body.get('trigger_type', 'custom_stocks')
        generation_mode = body.get('generation_mode', 'manual')  # ğŸ”¥ NEW: Extract generation_mode (manual/scheduled/self_learning)
        posting_type = body.get('posting_type', 'analysis')
        max_words = body.get('max_words', 200)

        # ğŸ”¥ HOTFIX: Cap max_words for personalized type to prevent 502 timeouts
        if posting_type == 'personalized' and max_words > 200:
            logger.warning(f"âš ï¸  Personalized type max_words capped: {max_words} â†’ 200 (prevent timeout)")
            max_words = 200

        # ğŸ”¥ æ–°å¢ï¼šæ¨¡å‹ ID é¸æ“‡é‚è¼¯
        model_id_override = body.get('model_id_override')  # æ‰¹é‡è¦†è“‹æ¨¡å‹
        use_kol_default_model = body.get('use_kol_default_model', True)  # é è¨­ä½¿ç”¨ KOL æ¨¡å‹

        # ğŸ” DEBUG: å°å‡ºå‰ç«¯å‚³ä¾†çš„åƒæ•¸
        logger.info(f"ğŸ” DEBUG trigger_type: {repr(trigger_type)} (type: {type(trigger_type).__name__})")
        logger.info(f"ğŸ” DEBUG posting_type: {repr(posting_type)} (type: {type(posting_type).__name__})")
        logger.info(f"ğŸ” DEBUG model_id_override: {repr(model_id_override)} (type: {type(model_id_override).__name__})")
        logger.info(f"ğŸ” DEBUG use_kol_default_model: {repr(use_kol_default_model)} (type: {type(use_kol_default_model).__name__})")

        # ç¢ºå®šä½¿ç”¨çš„æ¨¡å‹
        chosen_model_id = None

        # ğŸ”¥ æŸ¥è©¢å®Œæ•´ KOL Profileï¼ˆä¸åª model_idï¼‰
        kol_profile = None

        # å„ªå…ˆç´š 1: æ‰¹é‡è¦†è“‹æ¨¡å‹ï¼ˆå¦‚æœè¨­å®šä¸”ä¸ä½¿ç”¨ KOL é è¨­ï¼‰
        if not use_kol_default_model and model_id_override:
            chosen_model_id = model_id_override
            logger.info(f"ğŸ¤– ä½¿ç”¨æ‰¹é‡è¦†è“‹æ¨¡å‹: {chosen_model_id}")
        else:
            chosen_model_id = "gpt-4o-mini"  # é è¨­å€¼

        # å„ªå…ˆç´š 2: å¾æ•¸æ“šåº«ç²å– KOL å®Œæ•´è³‡æ–™
        try:
            conn = await asyncpg.connect(
                host=DB_CONFIG['host'],
                port=DB_CONFIG['port'],
                database=DB_CONFIG['database'],
                user=DB_CONFIG['user'],
                password=DB_CONFIG['password']
            )

            # ğŸ”¥ æŸ¥è©¢å®Œæ•´ KOL Profile (åŒ…å« prompt è¨­å®š)
            kol_row = await conn.fetchrow("""
                SELECT serial, nickname, persona, model_id,
                       prompt_persona, prompt_style, prompt_guardrails, prompt_skeleton
                FROM kol_profiles
                WHERE serial = $1
            """, str(kol_serial))

            await conn.close()

            if kol_row:
                # ğŸ”¥ æ§‹å»º kol_profile dict (åŒ…å«å®Œæ•´ prompt è¨­å®š)
                kol_profile = {
                    'serial': kol_row['serial'],
                    'nickname': kol_row['nickname'],
                    'persona': kol_row['persona'],
                    'writing_style': kol_row['prompt_style'] or '',  # ä½¿ç”¨ prompt_style ä½œç‚º writing_style
                    'prompt_persona': kol_row['prompt_persona'] or '',
                    'prompt_guardrails': kol_row['prompt_guardrails'] or '',
                    'prompt_skeleton': kol_row['prompt_skeleton'] or ''
                }

                # æ¨¡å‹é¸æ“‡é‚è¼¯ï¼ˆä¿æŒä¸è®Šï¼‰
                if use_kol_default_model and kol_row['model_id']:
                    chosen_model_id = kol_row['model_id']
                    logger.info(f"ğŸ¤– ä½¿ç”¨ KOL é è¨­æ¨¡å‹: {chosen_model_id} (KOL: {kol_row['nickname']})")
                else:
                    logger.info(f"ğŸ¤– ä½¿ç”¨é è¨­æ¨¡å‹: {chosen_model_id}")
            else:
                logger.warning(f"âš ï¸  KOL serial {kol_serial} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é™ç´š profile")
                # é™ç´šè™•ç†
                kol_profile = {
                    'serial': str(kol_serial),
                    'nickname': 'åˆ†æå¸«',
                    'persona': kol_persona
                }

        except Exception as db_error:
            logger.warning(f"âš ï¸  ç„¡æ³•ç²å– KOL Profile: {db_error}ï¼Œä½¿ç”¨é™ç´š profile")
            # é™ç´šè™•ç†
            kol_profile = {
                'serial': str(kol_serial),
                'nickname': 'åˆ†æå¸«',
                'persona': kol_persona,
                'writing_style': '',
                'tone_settings': ''
            }

        # ğŸ”¥ Phase 1.5: ç²å–å³æ™‚è‚¡åƒ¹è³‡è¨Š (å¯é¸)
        realtime_price_data = {}
        # ğŸ”¥ NEW: Check if enable_realtime_price is True (default: True)
        enable_realtime_price = body.get('enable_realtime_price', True)

        if enable_realtime_price:
            try:
                logger.info(f"ğŸ’° é–‹å§‹æŠ“å– {stock_name}({stock_code}) å³æ™‚è‚¡åƒ¹...")
                cmoney_service = get_cmoney_service()
                realtime_price_data = await cmoney_service.get_realtime_stock_price(
                    stock_code=stock_code,
                    stock_name=stock_name
                )

                if realtime_price_data.get('is_realtime'):
                    logger.info(f"âœ… æˆåŠŸç²å–å³æ™‚è‚¡åƒ¹: {stock_name} ç•¶å‰åƒ¹æ ¼ {realtime_price_data.get('current_price')} å…ƒ ({realtime_price_data.get('price_change_pct'):+.2f}%)")
                else:
                    logger.warning(f"âš ï¸  ç„¡æ³•ç²å–å³æ™‚è‚¡åƒ¹ï¼Œå°‡ä½¿ç”¨é è¨­æ•¸æ“š")
            except Exception as price_error:
                logger.error(f"âŒ ç²å–å³æ™‚è‚¡åƒ¹å¤±æ•—: {price_error}")
                realtime_price_data = {
                    "error": str(price_error),
                    "is_realtime": False
                }
        else:
            logger.info(f"â­ï¸  è·³éå³æ™‚è‚¡åƒ¹æŠ“å– (enable_realtime_price=False)")
            realtime_price_data = {
                "is_realtime": False,
                "disabled": True
            }

        # ğŸ”¥ Phase 2: èª¿ç”¨ Serper API ç²å–æ–°èæ•¸æ“š
        serper_analysis = {}
        if serper_service:
            try:
                logger.info(f"ğŸ” é–‹å§‹æœå°‹ {stock_name}({stock_code}) ç›¸é—œæ–°è...")
                # å¾å‰ç«¯ç²å–æ–°èé…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
                news_config = body.get('news_config', {})
                search_keywords = news_config.get('search_keywords')
                time_range = news_config.get('time_range', 'd1')  # é è¨­éå»1å¤©

                # ğŸ”¥ FIX: æå–æ–°èé€£çµè¨­å®š (enable_news_links, max_links)
                enable_news_links = news_config.get('enable_news_links', True)  # é è¨­é–‹å•Ÿ
                news_max_links = news_config.get('max_links', 5)  # é è¨­5å€‹é€£çµ

                logger.info(f"ğŸ“° æ–°èé€£çµè¨­å®š: enable={enable_news_links}, max_links={news_max_links}")

                serper_analysis = serper_service.get_comprehensive_stock_analysis(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    search_keywords=search_keywords,
                    time_range=time_range,
                    trigger_type=trigger_type
                )

                # ğŸ”¥ FIX: å°‡æ–°èé€£çµè¨­å®šæ³¨å…¥åˆ° serper_analysisï¼Œä¾› personalization_module ä½¿ç”¨
                serper_analysis['enable_news_links'] = enable_news_links
                serper_analysis['news_max_links'] = news_max_links

                news_count = len(serper_analysis.get('news_items', []))
                logger.info(f"âœ… Serper API èª¿ç”¨æˆåŠŸï¼Œæ‰¾åˆ° {news_count} å‰‡æ–°è")
            except Exception as serper_error:
                logger.warning(f"âš ï¸  Serper API èª¿ç”¨å¤±æ•—: {serper_error}ï¼Œç¹¼çºŒä½¿ç”¨ç©ºæ•¸æ“š")
                serper_analysis = {}
        else:
            logger.info("â„¹ï¸  Serper æœå‹™æœªåˆå§‹åŒ–ï¼Œè·³éæ–°èæœå°‹")

        # ğŸ”¥ Phase 3: DTNO æ•¸æ“š (åŸºæœ¬é¢/æŠ€è¡“é¢/ç±Œç¢¼é¢)
        dtno_data = {}
        data_sources = body.get('data_sources', {})
        sub_categories = data_sources.get('subCategories', [])

        if sub_categories:
            try:
                logger.info(f"ğŸ“Š é–‹å§‹æŠ“å– DTNO æ•¸æ“š: {sub_categories}")
                dtno_service = get_dtno_service()
                dtno_data = await dtno_service.fetch_by_subcategories(stock_code, sub_categories)
                logger.info(f"âœ… DTNO æ•¸æ“šæŠ“å–æˆåŠŸ: {len(dtno_data)} å€‹åˆ†é¡")
            except Exception as dtno_error:
                logger.warning(f"âš ï¸  DTNO æ•¸æ“šæŠ“å–å¤±æ•—: {dtno_error}ï¼Œç¹¼çºŒä½¿ç”¨ç©ºæ•¸æ“š")
                dtno_data = {}
        else:
            logger.info("â„¹ï¸  æœªé¸æ“‡ DTNO æ•¸æ“šæºï¼Œè·³é")

        # ğŸ”¥ FIX: Check if user provided custom title and content (for manual posting)
        custom_title = body.get('title')
        custom_content = body.get('content')

        if custom_title and custom_content:
            # User provided custom content - use it directly, skip GPT generation
            logger.info(f"âœ… ä½¿ç”¨ç”¨æˆ¶è‡ªå®šç¾©å…§å®¹: title={custom_title[:30]}..., content_length={len(custom_content)}")
            title = custom_title
            content = custom_content
        elif gpt_generator:
            # No custom content - generate with GPT
            logger.info(f"ä½¿ç”¨ GPT ç”Ÿæˆå™¨ç”Ÿæˆå…§å®¹: stock_code={stock_code}, kol={kol_profile.get('nickname')}, model={chosen_model_id}")
            try:
                gpt_result = gpt_generator.generate_stock_analysis(
                    stock_id=stock_code,
                    stock_name=stock_name,
                    kol_profile=kol_profile,  # ğŸ”¥ å‚³å®Œæ•´ profile
                    posting_type=posting_type,  # ğŸ”¥ ä½¿ç”¨ posting_type æ±ºå®š prompt æ¨¡æ¿
                    trigger_type=trigger_type,  # ğŸ”¥ æ–°å¢
                    serper_analysis=serper_analysis,  # âœ… å‚³å…¥çœŸå¯¦ Serper æ•¸æ“š
                    realtime_price_data=realtime_price_data,  # ğŸ”¥ å‚³å…¥å³æ™‚è‚¡åƒ¹è³‡è¨Š
                    ohlc_data=None,
                    technical_indicators=None,
                    dtno_data=dtno_data if dtno_data else None,  # ğŸ”¥ NEW: DTNO æ•¸æ“š
                    content_length="medium",
                    max_words=max_words,
                    model=chosen_model_id  # ğŸ”¥ å‚³éé¸å®šçš„æ¨¡å‹
                )

                # ğŸ” DEBUG: å°å‡º gpt_result çš„å®Œæ•´å…§å®¹
                logger.info(f"ğŸ” DEBUG gpt_result keys: {list(gpt_result.keys())}")
                logger.info(f"ğŸ” DEBUG gpt_result title: {gpt_result.get('title', 'None')}")
                logger.info(f"ğŸ” DEBUG gpt_result content é•·åº¦: {len(gpt_result.get('content', ''))}")
                logger.info(f"ğŸ” DEBUG gpt_result content å‰ 100 å­—: {gpt_result.get('content', '')[:100]}")

                title = gpt_result.get('title', f"{stock_name}({stock_code}) åˆ†æ")
                content = gpt_result.get('content', '')
                logger.info(f"âœ… GPT å…§å®¹ç”ŸæˆæˆåŠŸ: title={title[:30]}...")

                # ğŸ”¥ FIX: æ•´åˆæ–°èé€£çµåˆ°å…§å®¹æœ«å°¾
                if serper_analysis and serper_analysis.get('enable_news_links', True):
                    news_items = serper_analysis.get('news_items', [])
                    news_max_links = serper_analysis.get('news_max_links', 5)

                    if news_items:
                        # æª¢æŸ¥å…§å®¹ä¸­æ˜¯å¦å·²ç¶“æœ‰æ–°èä¾†æº
                        if "æ–°èä¾†æº:" not in content and "ğŸ“° æ–°èä¾†æº:" not in content:
                            logger.info(f"ğŸ“° é–‹å§‹æ•´åˆ {len(news_items)} å‰‡æ–°èä¾†æº (max: {news_max_links})")

                            news_sources = []
                            for i, news in enumerate(news_items[:news_max_links]):
                                title_text = news.get('title', '')
                                link = news.get('link', '')

                                if title_text and link:
                                    news_sources.append(f"{i+1}. {title_text}\n   ğŸ”— {link}")
                                elif title_text:
                                    news_sources.append(f"{i+1}. {title_text}")

                            if news_sources:
                                news_section = "\n\nğŸ“° æ–°èä¾†æºï¼š\n" + "\n".join(news_sources)
                                content += news_section
                                logger.info(f"âœ… æˆåŠŸé™„åŠ  {len(news_sources)} å‰‡æ–°èé€£çµ")
                        else:
                            logger.info("âš ï¸  å…§å®¹ä¸­å·²åŒ…å«æ–°èä¾†æºï¼Œè·³éé‡è¤‡æ·»åŠ ")
                    else:
                        logger.info("â„¹ï¸  ç„¡æ–°èæ•¸æ“šå¯é™„åŠ ")
                else:
                    if serper_analysis:
                        logger.info(f"â„¹ï¸  æ–°èé€£çµå·²åœç”¨ (enable_news_links={serper_analysis.get('enable_news_links')})")
                    else:
                        logger.info("â„¹ï¸  ç„¡ serper_analysis æ•¸æ“š")

            except Exception as gpt_error:
                logger.error(f"âŒ GPT ç”Ÿæˆå¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ¿: {gpt_error}")
                title = f"{stock_name}({stock_code}) æŠ€è¡“åˆ†æèˆ‡æ“ä½œç­–ç•¥"
                content = f"""ã€{stock_name}({stock_code}) æ·±åº¦åˆ†æã€‘

ä¸€ã€æŠ€è¡“é¢åˆ†æ
å¾æŠ€è¡“æŒ‡æ¨™ä¾†çœ‹ï¼Œ{stock_name}ç›®å‰å‘ˆç¾å‡ºå€¼å¾—é—œæ³¨çš„è¨Šè™Ÿã€‚RSIæŒ‡æ¨™é¡¯ç¤ºè‚¡åƒ¹å‹•èƒ½è®ŠåŒ–ï¼ŒMACDæŒ‡æ¨™å‰‡åæ˜ çŸ­ä¸­æœŸè¶¨å‹¢ã€‚æˆäº¤é‡æ–¹é¢ï¼Œè¿‘æœŸé‡èƒ½æœ‰æ‰€æ”¾å¤§ï¼Œé¡¯ç¤ºå¸‚å ´é—œæ³¨åº¦æå‡ã€‚

äºŒã€åŸºæœ¬é¢è§€å¯Ÿ
{stock_name}ä½œç‚ºç”¢æ¥­ä¸­çš„é‡è¦æˆå“¡ï¼Œç‡Ÿé‹ç‹€æ³å€¼å¾—æŒçºŒè¿½è¹¤ã€‚æŠ•è³‡äººæ‡‰é—œæ³¨å…¬å¸è²¡å ±æ•¸æ“šã€ç‡Ÿæ”¶è¡¨ç¾ï¼Œä»¥åŠç”¢æ¥­æ•´é«”æ™¯æ°£è®ŠåŒ–ã€‚

ä¸‰ã€æ“ä½œå»ºè­°
çŸ­ç·šæ“ä½œè€…å¯è§€å¯Ÿé—œéµåƒ¹ä½çªç ´æƒ…æ³ï¼Œé…åˆé‡èƒ½è®ŠåŒ–åšé€²å‡ºåˆ¤æ–·ã€‚ä¸­é•·ç·šæŠ•è³‡è€…å‰‡éœ€è©•ä¼°åŸºæœ¬é¢æ˜¯å¦æ”¯æ’ç›®å‰è‚¡åƒ¹æ°´æº–ã€‚

å››ã€é¢¨éšªæé†’
- æ³¨æ„æ•´é«”å¸‚å ´ç³»çµ±æ€§é¢¨éšª
- ç•™æ„ç”¢æ¥­ç«¶çˆ­æ…‹å‹¢è®ŠåŒ–
- è¨­å®šåˆç†åœæåœåˆ©é»
- åš´æ ¼æ§åˆ¶æŒè‚¡æ¯”é‡

ä»¥ä¸Šåˆ†æåƒ…ä¾›åƒè€ƒï¼ŒæŠ•è³‡éœ€è¬¹æ…è©•ä¼°è‡ªèº«é¢¨éšªæ‰¿å—èƒ½åŠ›ã€‚"""
        else:
            logger.warning("âš ï¸  GPT ç”Ÿæˆå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")
            title = f"{stock_name}({stock_code}) æŠ€è¡“åˆ†æèˆ‡æ“ä½œç­–ç•¥"
            content = f"""ã€{stock_name}({stock_code}) æ·±åº¦åˆ†æã€‘

ä¸€ã€æŠ€è¡“é¢åˆ†æ
å¾æŠ€è¡“æŒ‡æ¨™ä¾†çœ‹ï¼Œ{stock_name}ç›®å‰å‘ˆç¾å‡ºå€¼å¾—é—œæ³¨çš„è¨Šè™Ÿã€‚RSIæŒ‡æ¨™é¡¯ç¤ºè‚¡åƒ¹å‹•èƒ½è®ŠåŒ–ï¼ŒMACDæŒ‡æ¨™å‰‡åæ˜ çŸ­ä¸­æœŸè¶¨å‹¢ã€‚æˆäº¤é‡æ–¹é¢ï¼Œè¿‘æœŸé‡èƒ½æœ‰æ‰€æ”¾å¤§ï¼Œé¡¯ç¤ºå¸‚å ´é—œæ³¨åº¦æå‡ã€‚

äºŒã€åŸºæœ¬é¢è§€å¯Ÿ
{stock_name}ä½œç‚ºç”¢æ¥­ä¸­çš„é‡è¦æˆå“¡ï¼Œç‡Ÿé‹ç‹€æ³å€¼å¾—æŒçºŒè¿½è¹¤ã€‚æŠ•è³‡äººæ‡‰é—œæ³¨å…¬å¸è²¡å ±æ•¸æ“šã€ç‡Ÿæ”¶è¡¨ç¾ï¼Œä»¥åŠç”¢æ¥­æ•´é«”æ™¯æ°£è®ŠåŒ–ã€‚

ä¸‰ã€æ“ä½œå»ºè­°
çŸ­ç·šæ“ä½œè€…å¯è§€å¯Ÿé—œéµåƒ¹ä½çªç ´æƒ…æ³ï¼Œé…åˆé‡èƒ½è®ŠåŒ–åšé€²å‡ºåˆ¤æ–·ã€‚ä¸­é•·ç·šæŠ•è³‡è€…å‰‡éœ€è©•ä¼°åŸºæœ¬é¢æ˜¯å¦æ”¯æ’ç›®å‰è‚¡åƒ¹æ°´æº–ã€‚

å››ã€é¢¨éšªæé†’
- æ³¨æ„æ•´é«”å¸‚å ´ç³»çµ±æ€§é¢¨éšª
- ç•™æ„ç”¢æ¥­ç«¶çˆ­æ…‹å‹¢è®ŠåŒ–
- è¨­å®šåˆç†åœæåœåˆ©é»
- åš´æ ¼æ§åˆ¶æŒè‚¡æ¯”é‡

ä»¥ä¸Šåˆ†æåƒ…ä¾›åƒè€ƒï¼ŒæŠ•è³‡éœ€è¬¹æ…è©•ä¼°è‡ªèº«é¢¨éšªæ‰¿å—èƒ½åŠ›ã€‚"""

        # âœ… ç§»é™¤éš¨æ©Ÿç‰ˆæœ¬ç”Ÿæˆ - çµ±ä¸€ä½¿ç”¨ Prompt æ¨¡æ¿ç³»çµ±
        # Prompt æ¨¡æ¿ç³»çµ±å·²æ ¹æ“š posting_type ç”Ÿæˆä¸åŒé¢¨æ ¼å…§å®¹
        alternative_versions = []
        logger.info(f"âœ… ä½¿ç”¨ Prompt æ¨¡æ¿ç³»çµ±ç”Ÿæˆå…§å®¹: posting_type={posting_type}")

        # ç”Ÿæˆ UUID ä½œç‚º post_id
        import uuid
        post_id = str(uuid.uuid4())

        # æº–å‚™æ•¸æ“šåº«å¯«å…¥æ•¸æ“š
        now = get_current_time()

        # ğŸ”¥ FIX: Use commodityTags from request if provided (for manual posting)
        custom_commodity_tags = body.get('commodityTags')
        if custom_commodity_tags:
            # User provided custom commodityTags - use them directly
            logger.info(f"âœ… ä½¿ç”¨ç”¨æˆ¶è‡ªå®šç¾© commodityTags: {custom_commodity_tags}")
            commodity_tags_data = custom_commodity_tags
        else:
            # No custom tags - generate default tags
            logger.info(f"ç”Ÿæˆé è¨­ commodityTags: stock_code={stock_code}")
            commodity_tags_data = [
                {"type": "Market", "key": "TWA00", "bullOrBear": 0},
                {"type": "Stock", "key": stock_code, "bullOrBear": 0}
            ]

        # ğŸ”¥ FIX: Extract topic from request (supports both workflows)
        # 1. Check for communityTopic object (æ‰‹å‹•ç™¼æ–‡ manual posting)
        # 2. Fall back to direct topic_id/topic_title fields (ç™¼æ–‡ç”Ÿæˆå™¨ batch generation)
        custom_community_topic = body.get('communityTopic')
        if custom_community_topic:
            topic_id = custom_community_topic.get('id')
            topic_title = custom_community_topic.get('title') or body.get('topic_title')
            logger.info(f"âœ… ä½¿ç”¨ communityTopic (æ‰‹å‹•ç™¼æ–‡): id={topic_id}, title={topic_title}")
        else:
            # Batch generation sends topic_id and topic_title directly
            topic_id = body.get('topic_id')
            topic_title = body.get('topic_title')
            if topic_id:
                logger.info(f"âœ… ä½¿ç”¨ topic_id/topic_title (ç™¼æ–‡ç”Ÿæˆå™¨): id={topic_id}, title={topic_title}")

        # ğŸ”¥ NEW: Extract trending topic fields
        has_trending_topic = body.get('has_trending_topic', False)
        topic_content = body.get('topic_content')
        if has_trending_topic:
            logger.info(f"ğŸ“° This is a trending topic post: {topic_title}")

        # ç”Ÿæˆåƒæ•¸è¨˜éŒ„
        full_triggers_config_from_request = body.get('full_triggers_config', {})
        logger.info(f"ğŸ” DEBUG: Received full_triggers_config from request: {json.dumps(full_triggers_config_from_request, ensure_ascii=False)[:200]}...")

        generation_params = {
            "method": "manual",
            "kol_persona": kol_persona,
            "content_style": body.get('content_style', 'chart_analysis'),
            "target_audience": body.get('target_audience', 'active_traders'),
            "trigger_type": trigger_type,
            "posting_type": posting_type,
            "created_at": now.isoformat(),
            # ğŸ”¥ FIX: Store full triggers config for schedule recreation
            "full_triggers_config": full_triggers_config_from_request
        }

        logger.info(f"ğŸ” DEBUG: generation_params to store: {json.dumps(generation_params, ensure_ascii=False)[:200]}...")

        # ç¢ºèªæ•¸æ“šåº«é€£æ¥å¯ç”¨
        if not db_pool:
            logger.error("æ•¸æ“šåº«é€£æ¥æ± ä¸å¯ç”¨")
            return JSONResponse(
                status_code=500,
                content={
                    "success": False,
                    "message": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                    "timestamp": now.isoformat()
                }
            )

        # å¯«å…¥æ•¸æ“šåº«
        conn = get_db_connection()
        conn.rollback()  # æ¸…é™¤ä»»ä½•å¤±æ•—çš„äº‹å‹™

        with conn.cursor() as cursor:
            insert_query = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id,
                    kol_serial, kol_nickname, kol_persona,
                    stock_code, stock_name,
                    title, content, content_md,
                    status, commodity_tags, generation_params, alternative_versions, trigger_type, generation_mode,
                    topic_id, topic_title, has_trending_topic, topic_content
                ) VALUES (
                    %s, %s, %s, %s,
                    %s, %s, %s,
                    %s, %s,
                    %s, %s, %s,
                    %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s
                )
            """

            # æº–å‚™ alternative_versions JSON
            alternative_versions_json = json.dumps(alternative_versions, ensure_ascii=False) if alternative_versions else None

            cursor.execute(insert_query, (
                post_id, now, now, session_id,
                kol_serial, f"KOL-{kol_serial}", kol_persona,
                stock_code, stock_name,
                title, content, content,
                'draft',  # é è¨­ç‚ºè‰ç¨¿ç‹€æ…‹
                json.dumps(commodity_tags_data, ensure_ascii=False),
                json.dumps(generation_params, ensure_ascii=False),
                alternative_versions_json,  # æ·»åŠ æ›¿ä»£ç‰ˆæœ¬
                trigger_type,  # æ·»åŠ  trigger_type
                generation_mode,  # ğŸ”¥ NEW: æ·»åŠ  generation_mode
                topic_id,  # ğŸ”¥ FIX: Add topic_id
                topic_title,  # ğŸ”¥ FIX: Add topic_title
                has_trending_topic,  # ğŸ”¥ NEW: Add has_trending_topic
                topic_content  # ğŸ”¥ NEW: Add topic_content
            ))

            conn.commit()
            logger.info(f"âœ… æˆåŠŸå¯«å…¥æ•¸æ“šåº«: post_id={post_id}, session_id={session_id}")

        # è¿”å›æˆåŠŸéŸ¿æ‡‰
        result = {
            "success": True,
            "message": "æ‰‹å‹•è²¼æ–‡ç”ŸæˆæˆåŠŸ",
            "post_id": post_id,
            "content": {
                "title": title,
                "content": content,
                "content_md": content
            },
            "stock_info": {
                "stock_code": stock_code,
                "stock_name": stock_name
            },
            "kol_info": {
                "kol_serial": kol_serial,
                "kol_nickname": f"KOL-{kol_serial}",
                "kol_persona": kol_persona
            },
            "commodity_tags": commodity_tags_data,
            "alternative_versions": alternative_versions,  # ğŸ”¥ FIX: Include alternative versions in response
            "timestamp": now.isoformat()
        }

        logger.info("âœ… æ‰‹å‹•è²¼æ–‡ç”ŸæˆæˆåŠŸ")
        return result

    except Exception as e:
        logger.error(f"âŒ æ‰‹å‹•è²¼æ–‡å¤±æ•—: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()

        if conn:
            try:
                conn.rollback()
            except:
                pass

        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æ‰‹å‹•è²¼æ–‡å¤±æ•—: {str(e)}",
                "error_type": type(e).__name__,
                "timestamp": get_current_time().isoformat()
            }
        )
    finally:
        if conn:
            return_db_connection(conn)

# ==================== Performance Testing Endpoint ====================

@app.post("/api/debug/performance-test")
async def performance_test(request: Request):
    """æ€§èƒ½æ¸¬è©¦ - è¨ˆæ™‚æ¯å€‹ç”Ÿæˆæ­¥é©Ÿçš„è€—æ™‚"""
    import time

    timings = {}
    total_start = time.time()

    try:
        # Step 1: Parse Request
        step_start = time.time()
        body = await request.json()
        timings['1_parse_request'] = round((time.time() - step_start) * 1000, 2)  # ms

        # Extract parameters
        stock_code = body.get('stock_code', '2330')
        stock_name = body.get('stock_name', 'å°ç©é›»')
        kol_serial = int(body.get('kol_serial', 208))
        kol_persona = body.get('kol_persona', 'technical')
        session_id = body.get('session_id', int(time.time() * 1000))
        trigger_type = body.get('trigger_type', 'performance_test')
        posting_type = body.get('posting_type', 'interaction')
        max_words = body.get('max_words', 150)
        model_id_override = body.get('model_id_override')
        use_kol_default_model = body.get('use_kol_default_model', True)

        # Step 2: Model Selection + KOL Profile Query (with DB query)
        step_start = time.time()
        chosen_model_id = None
        kol_profile = None

        if not use_kol_default_model and model_id_override:
            chosen_model_id = model_id_override
        else:
            chosen_model_id = "gpt-4o-mini"  # é è¨­å€¼

        # ğŸ”¥ æŸ¥è©¢å®Œæ•´ KOL Profile
        try:
            conn = await asyncpg.connect(
                host=DB_CONFIG['host'],
                port=DB_CONFIG['port'],
                database=DB_CONFIG['database'],
                user=DB_CONFIG['user'],
                password=DB_CONFIG['password']
            )

            # ğŸ”¥ æŸ¥è©¢å®Œæ•´ KOL Profile (removed writing_style, tone_settings - columns don't exist)
            kol_row = await conn.fetchrow("""
                SELECT serial, nickname, persona, model_id
                FROM kol_profiles
                WHERE serial = $1
            """, str(kol_serial))

            await conn.close()

            if kol_row:
                kol_profile = {
                    'serial': kol_row['serial'],
                    'nickname': kol_row['nickname'],
                    'persona': kol_row['persona'],
                    'writing_style': kol_row['writing_style'] or '',
                    'tone_settings': kol_row['tone_settings'] or ''
                }

                if use_kol_default_model and kol_row['model_id']:
                    chosen_model_id = kol_row['model_id']
            else:
                # é™ç´šè™•ç†
                kol_profile = {
                    'serial': str(kol_serial),
                    'nickname': 'åˆ†æå¸«',
                    'persona': kol_persona
                }

        except Exception as db_error:
            # é™ç´šè™•ç†
            kol_profile = {
                'serial': str(kol_serial),
                'nickname': 'åˆ†æå¸«',
                'persona': kol_persona,
                'writing_style': '',
                'tone_settings': ''
            }

        timings['2_model_selection_db'] = round((time.time() - step_start) * 1000, 2)

        # Step 2.5: Serper API Call (for news data)
        step_start = time.time()
        serper_analysis = {}
        if serper_service:
            try:
                serper_analysis = serper_service.get_comprehensive_stock_analysis(
                    stock_code=stock_code,
                    stock_name=stock_name,
                    search_keywords=None,
                    time_range='d1',
                    trigger_type=trigger_type
                )
                # ğŸ”¥ FIX: ç‚ºæ¸¬è©¦ç«¯é»ä¹Ÿæ·»åŠ æ–°èé€£çµè¨­å®šï¼ˆä½¿ç”¨é è¨­å€¼ï¼‰
                serper_analysis['enable_news_links'] = True  # æ¸¬è©¦ç«¯é»é è¨­é–‹å•Ÿ
                serper_analysis['news_max_links'] = 5  # æ¸¬è©¦ç«¯é»é è¨­5å€‹
            except Exception as e:
                pass
        timings['2_5_serper_api'] = round((time.time() - step_start) * 1000, 2)

        # Step 3: GPT Content Generation
        step_start = time.time()
        title = ""
        content = ""

        if gpt_generator:
            try:
                gpt_result = gpt_generator.generate_stock_analysis(
                    stock_id=stock_code,
                    stock_name=stock_name,
                    kol_profile=kol_profile,  # ğŸ”¥ å‚³å®Œæ•´ profile
                    posting_type=posting_type,  # ğŸ”¥ ä½¿ç”¨ posting_type æ±ºå®š prompt æ¨¡æ¿
                    trigger_type=trigger_type,  # ğŸ”¥ æ–°å¢
                    serper_analysis=serper_analysis,  # âœ… å‚³å…¥çœŸå¯¦ Serper æ•¸æ“š
                    ohlc_data=None,  # ğŸ”¥ Phase 2 æ¥å…¥
                    technical_indicators=None,  # ğŸ”¥ Phase 2 æ¥å…¥
                    content_length="medium",
                    max_words=max_words,
                    model=chosen_model_id
                )
                title = gpt_result.get('title', f"{stock_name}({stock_code}) åˆ†æ")
                content = gpt_result.get('content', '')
            except Exception as gpt_error:
                title = f"{stock_name}({stock_code}) æ¸¬è©¦æ¨™é¡Œ"
                content = f"æ¸¬è©¦å…§å®¹ - GPT å¤±æ•—: {gpt_error}"

        timings['3_gpt_generation'] = round((time.time() - step_start) * 1000, 2)

        # âœ… ç§»é™¤éš¨æ©Ÿç‰ˆæœ¬ç”Ÿæˆ - çµ±ä¸€ä½¿ç”¨ Prompt æ¨¡æ¿ç³»çµ±
        # Prompt æ¨¡æ¿ç³»çµ±å·²æ ¹æ“š posting_type ç”Ÿæˆä¸åŒé¢¨æ ¼å…§å®¹
        alternative_versions = []

        timings['4_alternative_versions'] = round((time.time() - step_start) * 1000, 2)

        # Step 5: Database Write
        step_start = time.time()

        import uuid
        post_id = str(uuid.uuid4())
        now = get_current_time()

        commodity_tags_data = [
            {"type": "Market", "key": "TWA00", "bullOrBear": 0},
            {"type": "Stock", "key": stock_code, "bullOrBear": 0}
        ]

        generation_params = {
            "method": "performance_test",
            "kol_persona": kol_persona,
            "trigger_type": trigger_type,
            "posting_type": posting_type,
            "created_at": now.isoformat()
        }

        conn = get_db_connection()
        conn.rollback()

        with conn.cursor() as cursor:
            insert_query = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id,
                    kol_serial, kol_nickname, kol_persona,
                    stock_code, stock_name,
                    title, content, content_md,
                    status, commodity_tags, generation_params, alternative_versions, trigger_type
                ) VALUES (
                    %s, %s, %s, %s,
                    %s, %s, %s,
                    %s, %s,
                    %s, %s, %s,
                    %s, %s, %s, %s, %s
                )
            """

            alternative_versions_json = json.dumps(alternative_versions, ensure_ascii=False) if alternative_versions else None

            cursor.execute(insert_query, (
                post_id, now, now, session_id,
                kol_serial, f"KOL-{kol_serial}", kol_persona,
                stock_code, stock_name,
                title, content, content,
                'draft',
                json.dumps(commodity_tags_data, ensure_ascii=False),
                json.dumps(generation_params, ensure_ascii=False),
                alternative_versions_json,
                trigger_type
            ))

            conn.commit()

        return_db_connection(conn)

        timings['5_database_write'] = round((time.time() - step_start) * 1000, 2)

        # Calculate total
        timings['total_time'] = round((time.time() - total_start) * 1000, 2)

        # Calculate percentages
        total_ms = timings['total_time']
        percentages = {}
        for key in timings:
            if key != 'total_time':
                percentages[key] = round((timings[key] / total_ms) * 100, 1) if total_ms > 0 else 0

        return {
            "success": True,
            "post_id": post_id,
            "model_used": chosen_model_id,
            "timings_ms": timings,
            "percentages": percentages,
            "breakdown": {
                "1_parse_request": {
                    "time_ms": timings['1_parse_request'],
                    "percentage": percentages.get('1_parse_request', 0),
                    "description": "è§£æè«‹æ±‚åƒæ•¸"
                },
                "2_model_selection_db": {
                    "time_ms": timings['2_model_selection_db'],
                    "percentage": percentages.get('2_model_selection_db', 0),
                    "description": "æ¨¡å‹é¸æ“‡ï¼ˆå«æ•¸æ“šåº«æŸ¥è©¢ï¼‰"
                },
                "3_gpt_generation": {
                    "time_ms": timings['3_gpt_generation'],
                    "percentage": percentages.get('3_gpt_generation', 0),
                    "description": "GPT å…§å®¹ç”Ÿæˆï¼ˆOpenAI API èª¿ç”¨ï¼‰"
                },
                "4_alternative_versions": {
                    "time_ms": timings['4_alternative_versions'],
                    "percentage": percentages.get('4_alternative_versions', 0),
                    "description": "ç”Ÿæˆ 4 å€‹æ›¿ä»£ç‰ˆæœ¬"
                },
                "5_database_write": {
                    "time_ms": timings['5_database_write'],
                    "percentage": percentages.get('5_database_write', 0),
                    "description": "å¯«å…¥æ•¸æ“šåº«"
                }
            },
            "summary": {
                "total_time_ms": timings['total_time'],
                "total_time_seconds": round(timings['total_time'] / 1000, 2),
                "slowest_step": max([(k, v) for k, v in timings.items() if k != 'total_time'], key=lambda x: x[1])[0],
                "slowest_time_ms": max([v for k, v in timings.items() if k != 'total_time'])
            },
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": str(e),
                "timings_ms": timings,
                "timestamp": get_current_time().isoformat()
            }
        )

# ==================== Dashboard API åŠŸèƒ½ ====================

@app.get("/api/dashboard/system-monitoring")
async def get_system_monitoring():
    """ç²å–ç³»çµ±ç›£æ§æ•¸æ“š"""
    logger.info("æ”¶åˆ° system-monitoring è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "cpu_usage": 45.2,
            "memory_usage": 67.8,
            "disk_usage": 23.1,
            "active_connections": 156,
            "uptime": "5 days, 12 hours",
            "last_restart": "2025-10-10T08:30:00Z"
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info("è¿”å›ç³»çµ±ç›£æ§æ•¸æ“š")
    return result

@app.get("/api/dashboard/content-management")
async def get_content_management():
    """ç²å–å…§å®¹ç®¡ç†æ•¸æ“š"""
    logger.info("æ”¶åˆ° content-management è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "total_posts": 1250,
            "published_posts": 1180,
            "draft_posts": 70,
            "scheduled_posts": 45,
            "failed_posts": 5
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info("è¿”å›å…§å®¹ç®¡ç†æ•¸æ“š")
    return result

@app.get("/api/dashboard/interaction-analysis")
async def get_interaction_analysis():
    """ç²å–äº’å‹•åˆ†ææ•¸æ“š"""
    logger.info("æ”¶åˆ° interaction-analysis è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "total_interactions": 15680,
            "likes": 8920,
            "comments": 2340,
            "shares": 4420,
            "engagement_rate": 12.5,
            "top_performing_posts": [
                {"post_id": "post_001", "interactions": 1250},
                {"post_id": "post_002", "interactions": 980},
                {"post_id": "post_003", "interactions": 750}
            ]
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info("è¿”å›äº’å‹•åˆ†ææ•¸æ“š")
    return result

# ==================== Posts API åŠŸèƒ½ ====================

@app.get("/api/posts")
async def get_posts(
    skip: int = Query(0, description="è·³éçš„è¨˜éŒ„æ•¸"),
    limit: int = Query(100, description="è¿”å›çš„è¨˜éŒ„æ•¸ï¼Œé»˜èª100æ¢"),
    status: str = Query(None, description="ç‹€æ…‹ç¯©é¸"),
    session_id: int = Query(None, description="Session IDç¯©é¸")
):
    """ç²å–è²¼æ–‡åˆ—è¡¨ï¼ˆå¾ PostgreSQL æ•¸æ“šåº«ï¼‰"""
    logger.info(f"æ”¶åˆ° get_posts è«‹æ±‚: skip={skip}, limit={limit}, status={status}, session_id={session_id}")

    conn = None
    try:
        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥æ± ç‹€æ…‹
        if not db_pool:
            logger.error("âŒ æ•¸æ“šåº«é€£æ¥æ± ä¸å­˜åœ¨ï¼Œç„¡æ³•æŸ¥è©¢è²¼æ–‡æ•¸æ“š")
            return {
                "success": False,
                "posts": [],
                "count": 0,
                "skip": skip,
                "limit": limit,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        # Get connection from pool
        conn = get_db_connection()

        # Reset connection if in failed state
        conn.rollback()  # Clear any failed transactions

        # æŸ¥è©¢ post_records è¡¨
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # é¦–å…ˆæª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_schema = 'public'
                    AND table_name = 'post_records'
                ) as exists;
            """)
            result = cursor.fetchone()
            table_exists = result['exists'] if result else False
            logger.info(f"ğŸ“Š post_records è¡¨å­˜åœ¨ç‹€æ…‹: {table_exists}")

            if not table_exists:
                logger.error("âŒ post_records è¡¨ä¸å­˜åœ¨")
                return {
                    "success": False,
                    "posts": [],
                    "count": 0,
                    "skip": skip,
                    "limit": limit,
                    "error": "post_records è¡¨ä¸å­˜åœ¨",
                    "timestamp": get_current_time().isoformat()
                }

            # ç²å–ç¸½æ•¸ï¼ˆåœ¨æŸ¥è©¢å‰å…ˆæª¢æŸ¥ï¼‰
            count_query = "SELECT COUNT(*) as count FROM post_records"
            count_params = []
            where_clauses = []

            if status:
                where_clauses.append("status = %s")
                count_params.append(status)

            if session_id is not None:
                where_clauses.append("session_id = %s")
                count_params.append(session_id)

            if where_clauses:
                count_query += " WHERE " + " AND ".join(where_clauses)

            cursor.execute(count_query, count_params)
            total_count = cursor.fetchone()['count']
            logger.info(f"ğŸ“Š æ•¸æ“šåº«ä¸­ç¸½è²¼æ–‡æ•¸ (filtered): {total_count}")

            # æ§‹å»ºæŸ¥è©¢
            query = "SELECT * FROM post_records"
            params = []

            if where_clauses:
                query += " WHERE " + " AND ".join(where_clauses)
                params.extend(count_params)

            query += " ORDER BY created_at DESC LIMIT %s OFFSET %s"
            params.extend([limit, skip])

            logger.info(f"ğŸ” åŸ·è¡Œ SQL æŸ¥è©¢: {query} with params: {params}")
            cursor.execute(query, params)
            posts = cursor.fetchall()

            conn.commit()  # Commit after all reads
            logger.info(f"âœ… æŸ¥è©¢åˆ° {len(posts)} æ¢è²¼æ–‡æ•¸æ“šï¼Œç¸½æ•¸: {total_count}")

            # ğŸ”¥ FIX: Convert naive UTC datetimes to Taipei timezone
            posts_with_timezone = [convert_post_datetimes_to_taipei(dict(post)) for post in posts]

            # ğŸ” DEBUG: Log full_triggers_config content for first post
            if posts_with_timezone:
                first_post = posts_with_timezone[0]
                if 'generation_config' in first_post and first_post['generation_config']:
                    has_ftc = 'full_triggers_config' in first_post['generation_config']
                    logger.info(f"ğŸ” DEBUG: First post generation_config has full_triggers_config: {has_ftc}")
                    if has_ftc:
                        ftc_content = first_post['generation_config']['full_triggers_config']
                        logger.info(f"ğŸ” DEBUG: full_triggers_config content: {json.dumps(ftc_content, ensure_ascii=False)[:300]}...")

            return {
                "success": True,
                "posts": posts_with_timezone,
                "count": total_count,
                "skip": skip,
                "limit": limit,
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        if conn:
            conn.rollback()  # Rollback on error
        logger.error(f"âŒ æŸ¥è©¢è²¼æ–‡æ•¸æ“šå¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        return {
            "success": False,
            "posts": [],
            "count": 0,
            "error": str(e),
            "error_type": type(e).__name__,
            "error_details": f"{type(e).__name__}: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/kol/{serial}/refresh-interactions")
async def refresh_kol_interactions(serial: str):
    """
    åˆ·æ–°ç‰¹å®š KOL çš„æ‰€æœ‰è²¼æ–‡äº’å‹•æ•¸æ“š
    å¾CMoney APIç²å–æœ€æ–°çš„likes, comments, sharesç­‰æ•¸æ“šä¸¦æ›´æ–°åˆ°æ•¸æ“šåº«
    """
    logger.info(f"æ”¶åˆ° refresh-kol-interactions è«‹æ±‚ - KOL Serial: {serial}")

    if not db_pool:
        return {"success": False, "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨", "timestamp": get_current_time().isoformat()}

    conn = None
    try:
        # Get connection from pool
        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        # Fetch all published posts for this KOL with article_id
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT post_id, cmoney_post_id as article_id, kol_serial, email, password
                FROM post_records pr
                LEFT JOIN kol_profiles kp ON pr.kol_serial = kp.serial::integer
                WHERE pr.kol_serial = %s
                  AND pr.status = 'published'
                  AND pr.cmoney_post_id IS NOT NULL
                ORDER BY pr.created_at DESC
            """, (int(serial),))
            posts = cursor.fetchall()
            conn.commit()

        if not posts:
            return {
                "success": True,
                "updated_count": 0,
                "failed_count": 0,
                "total_posts": 0,
                "message": "No published posts found for this KOL",
                "timestamp": get_current_time().isoformat()
            }

        # Get KOL credentials from first post
        email = posts[0]['email']
        password = posts[0]['password']

        if not email or not password:
            return {"success": False, "error": "KOL credentials not found", "timestamp": get_current_time().isoformat()}

        logger.info(f"Found {len(posts)} published posts for KOL {serial}")

        # Login to CMoney to get access token
        async with httpx.AsyncClient() as client:
            login_response = await client.post(
                "https://www.cmoney.tw/member/login/jsonp.aspx",
                data={"a": email, "b": password, "remember": 0},
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                timeout=30.0
            )

            if login_response.status_code != 200:
                return {"success": False, "error": "CMoney login failed", "timestamp": get_current_time().isoformat()}

            # Extract access token
            token_data = login_response.json()
            access_token = token_data.get("Data", {}).get("access_token")

            if not access_token:
                return {"success": False, "error": "Failed to get access token", "timestamp": get_current_time().isoformat()}

            # Refresh interaction data for each post
            updated_count = 0
            failed_count = 0

            for post in posts:
                article_id = post['article_id']
                post_id = post['post_id']

                try:
                    # Fetch interaction data from CMoney
                    interaction_response = await client.get(
                        f"https://forumservice.cmoney.tw/api/Article/{article_id}",
                        headers={
                            "Authorization": f"Bearer {access_token}",
                            "X-Version": "2.0",
                            "accept": "application/json"
                        },
                        timeout=15.0
                    )

                    if interaction_response.status_code == 200:
                        data = interaction_response.json()

                        # Extract interaction metrics
                        likes = data.get("ArticleReplyDto", {}).get("SatisfiedCount", 0)
                        comments = data.get("CommentCount", 0)
                        shares = data.get("ArticleReplyDto", {}).get("ShareCount", 0)
                        bookmarks = data.get("ArticleReplyDto", {}).get("CollectedCount", 0)

                        # Extract emoji counts
                        emojis = data.get("ArticleReplyDto", {}).get("PersonEmotionArticleDetailDto", {})
                        emoji_data = {
                            "like": emojis.get("Like", 0),
                            "dislike": emojis.get("Dislike", 0),
                            "laugh": emojis.get("Laugh", 0),
                            "money": emojis.get("Money", 0),
                            "shock": emojis.get("Shock", 0),
                            "cry": emojis.get("Cry", 0),
                            "think": emojis.get("Think", 0),
                            "angry": emojis.get("Angry", 0)
                        }

                        # Update database
                        with conn.cursor() as update_cursor:
                            update_cursor.execute("""
                                UPDATE post_records
                                SET likes = %s, comments = %s, shares = %s, bookmarks = %s,
                                    emoji_data = %s, last_interaction_update = CURRENT_TIMESTAMP
                                WHERE post_id = %s
                            """, (likes, comments, shares, bookmarks, json.dumps(emoji_data), post_id))

                        updated_count += 1
                        logger.info(f"Updated post {post_id}: {likes} likes, {comments} comments, {shares} shares")

                except Exception as e:
                    logger.error(f"Failed to update post {post_id}: {e}")
                    failed_count += 1

            conn.commit()

        logger.info(f"Refresh complete for KOL {serial}: {updated_count} updated, {failed_count} failed")

        return {
            "success": True,
            "updated_count": updated_count,
            "failed_count": failed_count,
            "total_posts": len(posts),
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ Refresh KOL interactions failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e), "timestamp": get_current_time().isoformat()}
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/posts/refresh-all")
async def refresh_all_interactions():
    """
    åˆ·æ–°æ‰€æœ‰è²¼æ–‡çš„äº’å‹•æ•¸æ“š
    å¾CMoney APIç²å–æœ€æ–°çš„likes, comments, sharesç­‰æ•¸æ“šä¸¦æ›´æ–°åˆ°æ•¸æ“šåº«
    """
    logger.info("æ”¶åˆ° refresh-all è«‹æ±‚")

    if not db_pool:
        return {"success": False, "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨", "timestamp": get_current_time().isoformat()}

    conn = None
    try:
        # Get connection from pool
        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        # Get KOL credentials from environment
        email = os.getenv("FORUM_200_EMAIL")
        password = os.getenv("FORUM_200_PASSWORD")

        if not email or not password:
            return {"success": False, "error": "KOL credentials not configured", "timestamp": get_current_time().isoformat()}

        # Fetch all published posts with article_id
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT post_id, article_id, kol_serial
                FROM post_records
                WHERE status = 'published' AND article_id IS NOT NULL
                ORDER BY created_at DESC
            """)
            posts = cursor.fetchall()
            conn.commit()

        logger.info(f"Found {len(posts)} published posts to refresh")

        # Login to CMoney to get access token
        async with httpx.AsyncClient() as client:
            login_response = await client.post(
                "https://www.cmoney.tw/member/login/jsonp.aspx",
                data={"a": email, "b": password, "remember": 0},
                headers={"Content-Type": "application/x-www-form-urlencoded"},
                timeout=30.0
            )

            if login_response.status_code != 200:
                return {"success": False, "error": "CMoney login failed", "timestamp": get_current_time().isoformat()}

            # Extract access token
            token_data = login_response.json()
            access_token = token_data.get("Data", {}).get("access_token")

            if not access_token:
                return {"success": False, "error": "Failed to get access token", "timestamp": get_current_time().isoformat()}

            # Refresh interaction data for each post
            updated_count = 0
            failed_count = 0

            for post in posts:
                article_id = post['article_id']
                post_id = post['post_id']

                try:
                    # Fetch interaction data from CMoney
                    interaction_response = await client.get(
                        f"https://forumservice.cmoney.tw/api/Article/{article_id}",
                        headers={
                            "Authorization": f"Bearer {access_token}",
                            "X-Version": "2.0",
                            "accept": "application/json"
                        },
                        timeout=15.0
                    )

                    if interaction_response.status_code == 200:
                        data = interaction_response.json()

                        # Extract interaction metrics
                        likes = data.get("ArticleReplyDto", {}).get("SatisfiedCount", 0)
                        comments = data.get("CommentCount", 0)
                        shares = data.get("ArticleReplyDto", {}).get("ShareCount", 0)
                        bookmarks = data.get("ArticleReplyDto", {}).get("CollectedCount", 0)

                        # Extract emoji counts
                        emojis = data.get("ArticleReplyDto", {}).get("PersonEmotionArticleDetailDto", {})
                        emoji_data = {
                            "like": emojis.get("Like", 0),
                            "dislike": emojis.get("Dislike", 0),
                            "laugh": emojis.get("Laugh", 0),
                            "money": emojis.get("Money", 0),
                            "shock": emojis.get("Shock", 0),
                            "cry": emojis.get("Cry", 0),
                            "think": emojis.get("Think", 0),
                            "angry": emojis.get("Angry", 0)
                        }

                        # Update database
                        with conn.cursor() as update_cursor:
                            update_cursor.execute("""
                                UPDATE post_records
                                SET likes = %s, comments = %s, shares = %s, bookmarks = %s,
                                    emoji_data = %s, last_interaction_update = CURRENT_TIMESTAMP
                                WHERE post_id = %s
                            """, (likes, comments, shares, bookmarks, json.dumps(emoji_data), post_id))

                        updated_count += 1

                except Exception as e:
                    logger.error(f"Failed to update post {post_id}: {e}")
                    failed_count += 1

            conn.commit()

        logger.info(f"Refresh complete: {updated_count} updated, {failed_count} failed")

        return {
            "success": True,
            "updated_count": updated_count,
            "failed_count": failed_count,
            "total_posts": len(posts),
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ Refresh all failed: {e}")
        return {"success": False, "error": str(e), "timestamp": get_current_time().isoformat()}
    finally:
        if conn:
            return_db_connection(conn)

# ==================== Post Management API (Approve/Publish/Edit) ====================

@app.post("/api/posts/{post_id}/approve")
async def approve_post(post_id: str, request: Request):
    """å¯©æ ¸é€šéè²¼æ–‡"""
    logger.info(f"æ”¶åˆ° approve_post è«‹æ±‚ - Post ID: {post_id}")

    conn = None
    try:
        body = await request.json()
        reviewer_notes = body.get('reviewer_notes', '')
        approved_by = body.get('approved_by', 'system')
        edited_title = body.get('edited_title')
        edited_content = body.get('edited_content')

        logger.info(f"å¯©æ ¸åƒæ•¸: approved_by={approved_by}, has_edits={bool(edited_title or edited_content)}")

        conn = get_db_connection()
        with conn.cursor() as cursor:
            # Build update query
            update_fields = [
                "status = 'approved'",
                "reviewer_notes = %s",
                "approved_by = %s",
                "approved_at = %s"
            ]
            params = [reviewer_notes, approved_by, get_current_time()]

            # Add edited title/content if provided
            if edited_title:
                update_fields.append("title = %s")
                params.append(edited_title)
            if edited_content:
                update_fields.append("content = %s")
                params.append(edited_content)

            params.append(post_id)

            query = f"""
                UPDATE post_records
                SET {', '.join(update_fields)}
                WHERE post_id = %s
            """

            cursor.execute(query, params)
            conn.commit()

            if cursor.rowcount == 0:
                raise ValueError(f"Post {post_id} not found")

            logger.info(f"âœ… è²¼æ–‡å¯©æ ¸æˆåŠŸ - Post ID: {post_id}")

        return {"success": True, "message": "è²¼æ–‡å¯©æ ¸é€šé"}

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ å¯©æ ¸è²¼æ–‡å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/posts/{post_id}/publish")
async def publish_post(post_id: str):
    """ç™¼å¸ƒè²¼æ–‡åˆ° CMoney"""
    logger.info(f"æ”¶åˆ° publish_post è«‹æ±‚ - Post ID: {post_id}")

    conn = None
    try:
        # Get post from database
        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("SELECT * FROM post_records WHERE post_id = %s", (post_id,))
            post = cursor.fetchone()

            if not post:
                raise ValueError(f"Post {post_id} not found")

            # ğŸ”¥ FIX: Get KOL credentials based on kol_serial from post record
            kol_serial = post.get('kol_serial')
            if not kol_serial:
                raise ValueError(f"Post {post_id} missing kol_serial")

            logger.info(f"ğŸ” Fetching credentials for KOL {kol_serial}")

            # Query kol_profiles for email and password
            # ğŸ”¥ FIX: Cast kol_serial to string (serial column is VARCHAR)
            cursor.execute("""
                SELECT email, password, nickname
                FROM kol_profiles
                WHERE serial = %s
            """, (str(kol_serial),))

            kol_profile = cursor.fetchone()

            if not kol_profile:
                raise ValueError(f"KOL {kol_serial} not found in kol_profiles")

            if not kol_profile['email'] or not kol_profile['password']:
                raise ValueError(f"KOL {kol_serial} missing email or password")

            logger.info(f"âœ… Found KOL credentials: {kol_profile['nickname']} ({kol_profile['email']})")

            # Get CMoney credentials
            from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials

            cmoney_client = CMoneyClient()
            credentials = LoginCredentials(
                email=kol_profile['email'],
                password=kol_profile['password']
            )

            # Login with selected KOL's credentials
            logger.info(f"ğŸ” ç™»å…¥ CMoney API as {kol_profile['nickname']} ({kol_profile['email']})...")
            login_result = await cmoney_client.login(credentials)

            if not login_result or not login_result.token:
                raise ValueError("CMoney login failed")

            # Publish post
            logger.info(f"ğŸ“¤ ç™¼å¸ƒè²¼æ–‡åˆ° CMoney: {post['title'][:50]}...")

            from src.clients.cmoney.cmoney_client import ArticleData

            # Prepare article data with commodity tags and community topic
            article_data = ArticleData(
                title=post['title'],
                text=post['content']
            )

            # Add community topic if exists
            if post.get('topic_id'):
                article_data.communityTopic = {"id": post['topic_id']}

            # ğŸ”¥ FIX: Use commodity_tags from database if exists (user's custom tags)
            if post.get('commodity_tags'):
                # Parse JSON string to list if needed
                import json
                commodity_tags_from_db = post['commodity_tags']
                if isinstance(commodity_tags_from_db, str):
                    commodity_tags_from_db = json.loads(commodity_tags_from_db)

                logger.info(f"âœ… ä½¿ç”¨è³‡æ–™åº«ä¸­çš„ commodityTags: {commodity_tags_from_db}")
                article_data.commodity_tags = commodity_tags_from_db
            elif post.get('stock_code') and post['stock_code'] != '0000':
                # Fallback: generate from stock_code if no custom tags
                logger.info(f"ç”Ÿæˆé è¨­ commodityTags from stock_code: {post['stock_code']}")
                article_data.commodity_tags = [
                    {
                        "type": "Stock",
                        "key": post['stock_code'],
                        "bullOrBear": 0  # 0 = neutral, can be set based on trigger type
                    }
                ]

            publish_result = await cmoney_client.publish_article(
                access_token=login_result.token,
                article=article_data
            )

            if not publish_result or not publish_result.success:
                error_msg = publish_result.error_message if publish_result else "Unknown error"
                raise ValueError(f"Failed to publish to CMoney: {error_msg}")

            # Update database
            cursor.execute("""
                UPDATE post_records
                SET status = 'published',
                    published_at = %s,
                    cmoney_post_id = %s,
                    cmoney_post_url = %s
                WHERE post_id = %s
            """, (
                get_current_time(),
                publish_result.post_id,
                publish_result.post_url,
                post_id
            ))
            conn.commit()

            logger.info(f"âœ… è²¼æ–‡ç™¼å¸ƒæˆåŠŸ - Article ID: {publish_result.post_id}")

        return {
            "success": True,
            "post_id": publish_result.post_id,
            "post_url": publish_result.post_url
        }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ ç™¼å¸ƒè²¼æ–‡å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if conn:
            return_db_connection(conn)

@app.put("/api/posts/{post_id}/content")
async def update_post_content(post_id: str, request: Request):
    """æ›´æ–°è²¼æ–‡å…§å®¹ï¼ˆä¸æ”¹è®Šç‹€æ…‹ï¼‰"""
    logger.info(f"æ”¶åˆ° update_post_content è«‹æ±‚ - Post ID: {post_id}")

    conn = None
    try:
        body = await request.json()
        title = body.get('title')
        content = body.get('content')
        content_md = body.get('content_md')

        if not title and not content:
            raise ValueError("Must provide at least title or content")

        conn = get_db_connection()
        with conn.cursor() as cursor:
            update_fields = []
            params = []

            if title:
                update_fields.append("title = %s")
                params.append(title)
            if content:
                update_fields.append("content = %s")
                params.append(content)
            if content_md:
                update_fields.append("content_md = %s")
                params.append(content_md)

            # Always update updated_at
            update_fields.append("updated_at = %s")
            params.append(get_current_time())

            params.append(post_id)

            query = f"""
                UPDATE post_records
                SET {', '.join(update_fields)}
                WHERE post_id = %s
            """

            cursor.execute(query, params)
            conn.commit()

            if cursor.rowcount == 0:
                raise ValueError(f"Post {post_id} not found")

            logger.info(f"âœ… è²¼æ–‡å…§å®¹æ›´æ–°æˆåŠŸ - Post ID: {post_id}")

        return {"success": True, "message": "è²¼æ–‡å…§å®¹å·²æ›´æ–°"}

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ æ›´æ–°è²¼æ–‡å…§å®¹å¤±æ•—: {e}")
        return {"success": False, "error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.get("/api/posts/{post_id}/versions")
async def get_post_versions(post_id: str):
    """ç²å–è²¼æ–‡çš„å…¶ä»–ç”Ÿæˆç‰ˆæœ¬ï¼ˆå¾ alternative_versions JSON æ¬„ä½ï¼‰"""
    logger.info(f"æ”¶åˆ° get_post_versions è«‹æ±‚ - Post ID: {post_id}")

    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Get the post and its alternative_versions JSON field
            cursor.execute("""
                SELECT alternative_versions
                FROM post_records
                WHERE post_id = %s
            """, (post_id,))

            post = cursor.fetchone()
            if not post:
                logger.warning(f"âš ï¸ è²¼æ–‡ä¸å­˜åœ¨ - Post ID: {post_id}")
                return {"success": False, "versions": [], "error": "Post not found"}

            # Parse alternative_versions JSON
            alternative_versions = post.get('alternative_versions')

            if not alternative_versions:
                logger.info(f"ğŸ“¦ è²¼æ–‡æ²’æœ‰å…¶ä»–ç‰ˆæœ¬ - Post ID: {post_id}")
                return {
                    "success": True,
                    "versions": [],
                    "total": 0
                }

            # alternative_versions is already a dict/list if psycopg2 parsed it, or string if not
            if isinstance(alternative_versions, str):
                import json
                alternative_versions = json.loads(alternative_versions)

            # alternative_versions is a list of dicts with title, content, angle, etc.
            versions_list = []
            for idx, version in enumerate(alternative_versions):
                versions_list.append({
                    'version_number': idx + 2,  # Main post is version 1, alternatives are 2, 3, 4...
                    'title': version.get('title', ''),
                    'content': version.get('content', ''),
                    'angle': version.get('angle', 'æ¨™æº–åˆ†æ'),
                    'quality_score': version.get('quality_score'),
                    'ai_detection_score': version.get('ai_detection_score'),
                    'risk_level': version.get('risk_level')
                })

            logger.info(f"âœ… æ‰¾åˆ° {len(versions_list)} å€‹æ›¿ä»£ç‰ˆæœ¬")

            return {
                "success": True,
                "versions": versions_list,
                "total": len(versions_list)
            }

    except Exception as e:
        logger.error(f"âŒ ç²å–è²¼æ–‡ç‰ˆæœ¬å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {"success": False, "versions": [], "error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.delete("/api/posts/{post_id}")
async def delete_post(post_id: str):
    """åˆªé™¤è²¼æ–‡ï¼ˆè»Ÿåˆªé™¤ï¼‰"""
    logger.info(f"ğŸ—‘ï¸ é–‹å§‹åˆªé™¤è²¼æ–‡ - Post ID: {post_id}")

    conn = None
    try:
        conn = get_db_connection()
        with conn.cursor() as cursor:
            # æª¢æŸ¥è²¼æ–‡æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT post_id, status, cmoney_post_id, kol_serial
                FROM post_records
                WHERE post_id = %s
            """, (post_id,))

            existing_post = cursor.fetchone()
            if not existing_post:
                logger.error(f"âŒ è²¼æ–‡ä¸å­˜åœ¨ - Post ID: {post_id}")
                raise HTTPException(status_code=404, detail=f"è²¼æ–‡ä¸å­˜åœ¨: {post_id}")

            # è»Ÿåˆªé™¤ï¼šæ›´æ–°ç‹€æ…‹ç‚º 'deleted'
            cursor.execute("""
                UPDATE post_records
                SET status = 'deleted',
                    updated_at = %s
                WHERE post_id = %s
            """, (get_current_time(), post_id))

            conn.commit()

            logger.info(f"âœ… è²¼æ–‡è»Ÿåˆªé™¤æˆåŠŸ - Post ID: {post_id}")
            return {"success": True, "message": "è²¼æ–‡å·²åˆªé™¤"}

    except HTTPException:
        raise
    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ åˆªé™¤è²¼æ–‡å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"åˆªé™¤è²¼æ–‡å¤±æ•—: {str(e)}")
    finally:
        if conn:
            return_db_connection(conn)

# ==================== Trending API åŠŸèƒ½ ====================

@app.get("/api/trending")
async def get_trending_topics(limit: int = Query(10, description="è¿”å›çµæœæ•¸é‡")):
    """ç²å–ç†±é–€è©±é¡Œï¼ˆfrom real CMoney APIï¼‰"""
    logger.info(f"æ”¶åˆ° trending è«‹æ±‚: limit={limit}")

    try:
        # ğŸ”¥ FIX: Replace mock data with real CMoney API call
        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials

        # Initialize CMoney client
        cmoney_client = CMoneyClient()
        forum_credentials = get_forum_200_credentials()
        credentials = LoginCredentials(
            email=forum_credentials["email"],
            password=forum_credentials["password"]
        )

        # Login to get access token
        logger.info("ğŸ” Logging in to CMoney API...")
        login_result = await cmoney_client.login(credentials)

        if not login_result or not login_result.token:
            raise Exception("CMoney ç™»å…¥å¤±æ•—")

        access_token = login_result.token
        logger.info(f"âœ… CMoney ç™»å…¥æˆåŠŸï¼Œtoken æœ‰æ•ˆæœŸè‡³: {login_result.expires_at}")

        # Get trending topics from CMoney API
        logger.info("ğŸ“Š ç²å– CMoney ç†±é–€è©±é¡Œ...")
        cmoney_topics = await cmoney_client.get_trending_topics(access_token)

        # Transform CMoney Topic objects to TrendingTopic interface
        topics = []
        for topic in cmoney_topics[:limit]:
            topic_id = str(topic.id) if hasattr(topic, 'id') else f"topic_{len(topics)+1}"

            # ğŸ”¥ FIX: Fetch topic details to get relatedStockSymbols
            stock_ids = []
            topic_description = None
            pinned_article_context = None
            try:
                logger.info(f"ğŸ” Fetching details for topic: {topic_id}")
                topic_detail = await cmoney_client.get_topic_detail(access_token, topic_id)

                # Extract stock_ids from relatedStockSymbols in detail response
                related_stocks = topic_detail.get('relatedStockSymbols', [])
                if isinstance(related_stocks, list):
                    for stock_obj in related_stocks:
                        if isinstance(stock_obj, dict) and 'key' in stock_obj:
                            stock_ids.append(str(stock_obj['key']))
                        elif isinstance(stock_obj, str):
                            stock_ids.append(str(stock_obj))

                # Get description from detail if available
                topic_description = topic_detail.get('description') or topic_detail.get('name')

                logger.info(f"âœ… Topic {topic_id} has {len(stock_ids)} related stocks: {stock_ids}")

                # ğŸ”¥ NEW: Fetch pinned article for better context
                try:
                    pinned_article = await cmoney_client.get_topic_pinned_article(access_token, topic_id)
                    if pinned_article.get('has_pinned'):
                        article_text = pinned_article.get('text', '')

                        # ğŸ”¥ If pinned article doesn't have full text, fetch using article ID
                        if not article_text or len(article_text) < 50:
                            article_id = pinned_article.get('article_id')
                            if article_id:
                                logger.info(f"ğŸ“„ Pinned article incomplete, fetching full content for article {article_id}")
                                article_detail = await cmoney_client.get_article_detail(access_token, article_id)
                                if article_detail:
                                    article_text = article_detail.get('text', article_text)

                        pinned_article_context = {
                            'title': pinned_article.get('title', ''),
                            'text': article_text[:1000]  # Limit to 1000 chars for better context
                        }
                        logger.info(f"ğŸ“Œ Found pinned article for topic {topic_id}: {pinned_article.get('title', 'N/A')} ({len(article_text)} chars)")
                except Exception as pe:
                    logger.warning(f"âš ï¸ Could not fetch pinned article for topic {topic_id}: {pe}")

            except Exception as e:
                logger.warning(f"âš ï¸ Failed to fetch details for topic {topic_id}: {e}")
                # Fallback: try to extract from raw_data if available
                if hasattr(topic, 'raw_data') and topic.raw_data:
                    related_stocks = topic.raw_data.get('relatedStockSymbols', [])
                    if isinstance(related_stocks, list):
                        for stock_obj in related_stocks:
                            if isinstance(stock_obj, dict) and 'key' in stock_obj:
                                stock_ids.append(str(stock_obj['key']))

            # Calculate engagement score (mock for now, can be enhanced later)
            engagement_score = 100.0 - (len(topics) * 5.0)  # Decreasing scores

            # ğŸ”¥ FIX: Use 'name' field as primary title (matches CMoney API)
            topic_title = topic.name if hasattr(topic, 'name') else (topic.title if hasattr(topic, 'title') else f"è©±é¡Œ {len(topics)+1}")
            if not topic_description:
                topic_description = f"ç†±é–€è¨è«–è©±é¡Œï¼š{topic_title}"

            topics.append({
                "id": topic_id,
                "title": topic_title,
                "content": topic_description,
                "stock_ids": stock_ids,
                "category": "å¸‚å ´ç†±è­°",
                "engagement_score": engagement_score,
                "pinned_article_context": pinned_article_context  # ğŸ”¥ NEW: Include pinned article context
            })

            logger.info(f"ğŸ“Š è§£æè©±é¡Œ: {topic_title} | ç›¸é—œè‚¡ç¥¨: {stock_ids} | ç½®é ‚æ–‡ç« : {'æœ‰' if pinned_article_context else 'ç„¡'}")

        result = {
            "topics": topics,
            "timestamp": get_current_time().isoformat()
        }

        logger.info(f"âœ… è¿”å› {len(result['topics'])} å€‹ CMoney ç†±é–€è©±é¡Œ")
        return result

    except Exception as e:
        logger.error(f"âŒ ç²å– CMoney ç†±é–€è©±é¡Œå¤±æ•—: {e}")
        logger.error(f"éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")

        # Fallback: Return empty list if CMoney API fails
        result = {
            "topics": [],
            "timestamp": get_current_time().isoformat(),
            "error": f"CMoney API éŒ¯èª¤: {str(e)}"
        }

        logger.warning(f"âš ï¸  è¿”å›ç©ºåˆ—è¡¨ï¼ˆCMoney API å¤±æ•—ï¼‰")
        return result

@app.get("/api/extract-keywords")
async def extract_keywords(text: str = Query(..., description="è¦æå–é—œéµå­—çš„æ–‡æœ¬")):
    """æå–é—œéµå­—"""
    logger.info(f"æ”¶åˆ° extract-keywords è«‹æ±‚: text={text[:50]}...")

    keywords = ["AI", "äººå·¥æ™ºæ…§", "ç§‘æŠ€", "å‰µæ–°", "ç™¼å±•"]

    result = {
        "success": True,
        "data": {
            "keywords": keywords,
            "confidence_scores": [0.95, 0.88, 0.82, 0.76, 0.65],
            "original_text": text
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info(f"æå–åˆ° {len(keywords)} å€‹é—œéµå­—")
    return result

@app.get("/api/search-stocks-by-keywords")
async def search_stocks_by_keywords(keywords: str = Query(..., description="é—œéµå­—")):
    """æ ¹æ“šé—œéµå­—æœç´¢è‚¡ç¥¨"""
    logger.info(f"æ”¶åˆ° search-stocks-by-keywords è«‹æ±‚: keywords={keywords}")

    stocks = [
        {"stock_id": "2330", "stock_name": "å°ç©é›»", "relevance_score": 0.95},
        {"stock_id": "2454", "stock_name": "è¯ç™¼ç§‘", "relevance_score": 0.88},
        {"stock_id": "2317", "stock_name": "é´»æµ·", "relevance_score": 0.82}
    ]

    result = {
        "success": True,
        "data": {
            "stocks": stocks,
            "keywords": keywords,
            "total_found": len(stocks)
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info(f"æ‰¾åˆ° {len(stocks)} æ”¯ç›¸é—œè‚¡ç¥¨")
    return result

@app.get("/api/analyze-topic")
async def analyze_topic(topic: str = Query(..., description="è¦åˆ†æçš„è©±é¡Œ")):
    """åˆ†æè©±é¡Œ"""
    logger.info(f"æ”¶åˆ° analyze-topic è«‹æ±‚: topic={topic}")

    result = {
        "success": True,
        "data": {
            "topic": topic,
            "sentiment": "positive",
            "sentiment_score": 0.75,
            "key_points": [
                "æŠ€è¡“å‰µæ–°æŒçºŒæ¨é€²",
                "å¸‚å ´éœ€æ±‚ç©©å®šå¢é•·",
                "æ”¿ç­–æ”¯æŒåŠ›åº¦åŠ å¤§"
            ],
            "related_stocks": ["2330", "2454", "2317"]
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info(f"å®Œæˆè©±é¡Œåˆ†æ: {topic}")
    return result

@app.get("/api/generate-content")
async def generate_content(
    topic: str = Query(..., description="è©±é¡Œ"),
    style: str = Query("professional", description="å…§å®¹é¢¨æ ¼")
):
    """ç”Ÿæˆå…§å®¹"""
    logger.info(f"æ”¶åˆ° generate-content è«‹æ±‚: topic={topic}, style={style}")

    result = {
        "success": True,
        "data": {
            "content": f"é—œæ–¼{topic}çš„å°ˆæ¥­åˆ†æï¼šå¸‚å ´è¶¨å‹¢é¡¯ç¤ºè©²é ˜åŸŸå…·æœ‰å¼·å‹çš„æˆé•·æ½›åŠ›ï¼Œå»ºè­°æŠ•è³‡äººå¯†åˆ‡é—œæ³¨ç›¸é—œæ¨™çš„ã€‚",
            "topic": topic,
            "style": style,
            "word_count": 45,
            "generated_at": get_current_time().isoformat()
        },
        "timestamp": get_current_time().isoformat()
    }

    logger.info(f"ç”Ÿæˆå…§å®¹å®Œæˆ: {topic}")
    return result

# ==================== æ•¸æ“šåº«ç®¡ç†åŠŸèƒ½ ====================

@app.post("/admin/import-1788-posts")
async def import_1788_posts():
    """å°å…¥ 1788 ç­† post_records æ•¸æ“šï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    conn = None
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}

        # è®€å– JSON æ•¸æ“šæ–‡ä»¶
        json_file_path = '/app/post_records_1788.json'
        if not os.path.exists(json_file_path):
            return {"error": "post_records_1788.json æ–‡ä»¶ä¸å­˜åœ¨"}

        with open(json_file_path, 'r', encoding='utf-8') as f:
            records = json.load(f)

        logger.info(f"ğŸ“Š å¾ JSON æ–‡ä»¶åŠ è¼‰ {len(records)} ç­†è¨˜éŒ„")

        conn = get_db_connection()
        conn.rollback()  # Clear failed transactions

        with conn.cursor() as cursor:
            # æ¸…ç©ºç¾æœ‰æ•¸æ“š
            cursor.execute("DELETE FROM post_records")
            logger.info("ğŸ—‘ï¸ æ¸…ç©ºç¾æœ‰æ•¸æ“š")
            
            # æ‰¹é‡æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            # è½‰æ›è¨˜éŒ„æ ¼å¼
            records_dict = []
            for record in records:
                record_dict = dict(record)
                
                # è™•ç† datetime å­—ç¬¦ä¸²
                for datetime_field in ['created_at', 'updated_at', 'approved_at', 'scheduled_at', 'published_at']:
                    if record_dict.get(datetime_field):
                        if isinstance(record_dict[datetime_field], str):
                            try:
                                record_dict[datetime_field] = datetime.fromisoformat(record_dict[datetime_field].replace('Z', '+00:00'))
                            except:
                                record_dict[datetime_field] = None
                        elif not isinstance(record_dict[datetime_field], datetime):
                            record_dict[datetime_field] = None
                    else:
                        record_dict[datetime_field] = None
                
                # è™•ç† JSON å­—æ®µ - ç¢ºä¿æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                for json_field in ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']:
                    if record_dict.get(json_field):
                        if isinstance(record_dict[json_field], (dict, list)):
                            # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½‰æ›ç‚º JSON å­—ç¬¦ä¸²
                            try:
                                record_dict[json_field] = json.dumps(record_dict[json_field], ensure_ascii=False)
                            except:
                                record_dict[json_field] = None
                        elif isinstance(record_dict[json_field], str):
                            # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸è®Š
                            pass
                        else:
                            record_dict[json_field] = None
                    else:
                        record_dict[json_field] = None
                
                records_dict.append(record_dict)
            
            # æ‰¹é‡æ’å…¥
            logger.info(f"ğŸ“¥ é–‹å§‹æ’å…¥ {len(records_dict)} ç­†è¨˜éŒ„...")
            cursor.executemany(insert_sql, records_dict)
            conn.commit()
            
            logger.info(f"âœ… æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„")
            
            # é©—è­‰å°å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ å°å…¥ 1788 ç­†è¨˜éŒ„å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {"error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/test/insert-sample-data")
async def insert_sample_data():
    """æ’å…¥æ¨£æœ¬æ•¸æ“šåˆ° post_records è¡¨ï¼ˆæ¸¬è©¦åŠŸèƒ½ï¼‰"""
    conn = None
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}

        conn = get_db_connection()
        conn.rollback()  # Clear failed transactions

        with conn.cursor() as cursor:
            # å‰µå»ºæ¨£æœ¬è¨˜éŒ„
            sample_records = [
                {
                    'post_id': 'test-001',
                    'created_at': get_current_time(),
                    'updated_at': get_current_time(),
                    'session_id': 1,
                    'kol_serial': 200,
                    'kol_nickname': 'KOL-200',
                    'kol_persona': 'æŠ€è¡“åˆ†æå°ˆå®¶',
                    'stock_code': '2330',
                    'stock_name': 'å°ç©é›»',
                    'title': 'å°ç©é›»(2330) æŠ€è¡“é¢åˆ†æèˆ‡æ“ä½œå»ºè­°',
                    'content': 'å°ç©é›»ä»Šæ—¥è¡¨ç¾å¼·å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™é¡¯ç¤ºå¤šé ­è¶¨å‹¢æ˜ç¢ºã€‚å»ºè­°é—œæ³¨580å…ƒæ”¯æ’ä½ï¼Œçªç ´600å…ƒå¯è€ƒæ…®åŠ ç¢¼ã€‚',
                    'content_md': '## å°ç©é›»(2330) æŠ€è¡“é¢åˆ†æ\n\nå°ç©é›»ä»Šæ—¥è¡¨ç¾å¼·å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™é¡¯ç¤ºå¤šé ­è¶¨å‹¢æ˜ç¢ºã€‚',
                    'status': 'draft',
                    'reviewer_notes': None,
                    'approved_by': None,
                    'approved_at': None,
                    'scheduled_at': None,
                    'published_at': None,
                    'cmoney_post_id': None,
                    'cmoney_post_url': None,
                    'publish_error': None,
                    'views': 0,
                    'likes': 0,
                    'comments': 0,
                    'shares': 0,
                    'topic_id': 'tech_analysis',
                    'topic_title': 'æŠ€è¡“åˆ†æ',
                    'technical_analysis': '{"rsi": 65, "macd": "bullish", "support": 580}',
                    'serper_data': '{"search_volume": 1000, "trend": "up"}',
                    'quality_score': 8.5,
                    'ai_detection_score': 0.95,
                    'risk_level': 'medium',
                    'generation_params': '{"model": "gpt-4", "temperature": 0.7}',
                    'commodity_tags': '["åŠå°é«”", "ç§‘æŠ€è‚¡", "é¾é ­è‚¡"]',
                    'alternative_versions': '{"version_1": "çŸ­ç·šæ“ä½œ", "version_2": "é•·ç·šæŠ•è³‡"}'
                },
                {
                    'post_id': 'test-002',
                    'created_at': get_current_time(),
                    'updated_at': get_current_time(),
                    'session_id': 1,
                    'kol_serial': 201,
                    'kol_nickname': 'KOL-201',
                    'kol_persona': 'åŸºæœ¬é¢åˆ†æå¸«',
                    'stock_code': '2317',
                    'stock_name': 'é´»æµ·',
                    'title': 'é´»æµ·(2317) è²¡å ±åˆ†æèˆ‡æŠ•è³‡åƒ¹å€¼è©•ä¼°',
                    'content': 'é´»æµ·æœ€æ–°è²¡å ±é¡¯ç¤ºç‡Ÿæ”¶æˆé•·ç©©å®šï¼Œç²åˆ©èƒ½åŠ›æŒçºŒæ”¹å–„ã€‚æœ¬ç›Šæ¯”åˆç†ï¼Œé©åˆé•·æœŸæŠ•è³‡ã€‚',
                    'content_md': '## é´»æµ·(2317) è²¡å ±åˆ†æ\n\né´»æµ·æœ€æ–°è²¡å ±é¡¯ç¤ºç‡Ÿæ”¶æˆé•·ç©©å®šã€‚',
                    'status': 'approved',
                    'reviewer_notes': 'å…§å®¹å“è³ªè‰¯å¥½ï¼Œå»ºè­°ç™¼å¸ƒ',
                    'approved_by': 'admin',
                    'approved_at': get_current_time(),
                    'scheduled_at': None,
                    'published_at': None,
                    'cmoney_post_id': None,
                    'cmoney_post_url': None,
                    'publish_error': None,
                    'views': 150,
                    'likes': 12,
                    'comments': 3,
                    'shares': 2,
                    'topic_id': 'fundamental_analysis',
                    'topic_title': 'åŸºæœ¬é¢åˆ†æ',
                    'technical_analysis': '{"pe_ratio": 12.5, "pb_ratio": 1.2, "roe": 8.5}',
                    'serper_data': '{"search_volume": 800, "trend": "stable"}',
                    'quality_score': 9.0,
                    'ai_detection_score': 0.98,
                    'risk_level': 'low',
                    'generation_params': '{"model": "gpt-4", "temperature": 0.5}',
                    'commodity_tags': '["é›»å­è£½é€ ", "ä»£å·¥", "è˜‹æœæ¦‚å¿µè‚¡"]',
                    'alternative_versions': '{"version_1": "ä¿å®ˆæŠ•è³‡", "version_2": "åƒ¹å€¼æŠ•è³‡"}'
                }
            ]
            
            # æ’å…¥è¨˜éŒ„
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            cursor.executemany(insert_sql, sample_records)
            conn.commit()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(sample_records)} ç­†æ¨£æœ¬è¨˜éŒ„")
            
            # é©—è­‰æ’å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸæ’å…¥ {len(sample_records)} ç­†æ¨£æœ¬è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ æ’å…¥æ¨£æœ¬æ•¸æ“šå¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {"error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/admin/create-post-records-table")
async def create_table_manually():
    """æ‰‹å‹•å‰µå»º post_records è¡¨ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        create_post_records_table()
        return {
            "success": True,
            "message": "post_records è¡¨å‰µå»ºæˆåŠŸ",
            "timestamp": get_current_time().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ æ‰‹å‹•å‰µå»ºè¡¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/drop-and-recreate-post-records-table")
async def drop_and_recreate_table():
    """åˆªé™¤ä¸¦é‡æ–°å‰µå»º post_records è¡¨ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    conn = None
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}

        conn = get_db_connection()
        conn.rollback()  # Clear failed transactions

        with conn.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            conn.commit()
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")

            # é‡æ–°å‰µå»ºè¡¨
            cursor.execute("""
                CREATE TABLE post_records (
                    post_id VARCHAR PRIMARY KEY,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id INTEGER,
                    kol_serial INTEGER NOT NULL,
                    kol_nickname VARCHAR NOT NULL,
                    kol_persona VARCHAR,
                    stock_code VARCHAR NOT NULL,
                    stock_name VARCHAR NOT NULL,
                    title VARCHAR NOT NULL,
                    content TEXT NOT NULL,
                    content_md TEXT,
                    status VARCHAR DEFAULT 'draft',
                    reviewer_notes TEXT,
                    approved_by VARCHAR,
                    approved_at TIMESTAMP,
                    scheduled_at TIMESTAMP,
                    published_at TIMESTAMP,
                    cmoney_post_id VARCHAR,
                    cmoney_post_url VARCHAR,
                    publish_error TEXT,
                    views BIGINT DEFAULT 0,
                    likes BIGINT DEFAULT 0,
                    comments BIGINT DEFAULT 0,
                    shares BIGINT DEFAULT 0,
                    topic_id VARCHAR,
                    topic_title VARCHAR,
                    technical_analysis TEXT,
                    serper_data TEXT,
                    quality_score FLOAT,
                    ai_detection_score FLOAT,
                    risk_level VARCHAR,
                    generation_params TEXT,
                    commodity_tags TEXT,
                    alternative_versions TEXT
                );
            """)
            conn.commit()
            logger.info("âœ… é‡æ–°å‰µå»º post_records è¡¨æˆåŠŸ")

        return {
            "success": True,
            "message": "post_records è¡¨å·²åˆªé™¤ä¸¦é‡æ–°å‰µå»º",
            "timestamp": get_current_time().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤ä¸¦é‡æ–°å‰µå»ºè¡¨å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {"error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/admin/reset-database")
async def reset_database():
    """é‡ç½®æ•¸æ“šåº«ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    conn = None
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}

        conn = get_db_connection()
        conn.rollback()  # Clear failed transactions

        with conn.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            conn.commit()
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")

        return {
            "success": True,
            "message": "æ•¸æ“šåº«å·²é‡ç½®ï¼Œè¡¨å·²åˆªé™¤",
            "timestamp": get_current_time().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ é‡ç½®æ•¸æ“šåº«å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {"error": str(e)}
    finally:
        if conn:
            return_db_connection(conn)

@app.get("/admin/debug-database")
async def debug_database():
    """èª¿è©¦æ•¸æ“šåº«é€£æ¥å’Œè¡¨ç‹€æ…‹ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        debug_info = {
            "timestamp": get_current_time().isoformat(),
            "database_connection": None,
            "table_exists": False,
            "table_count": 0,
            "sample_data": None,
            "error": None
        }
        
        if not db_pool:
            debug_info["error"] = "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"
            return debug_info
        
        # æ¸¬è©¦æ•¸æ“šåº«é€£æ¥
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
                cursor.fetchone()
            debug_info["database_connection"] = "âœ… é€£æ¥æ­£å¸¸"
        except Exception as e:
            debug_info["database_connection"] = f"âŒ é€£æ¥å¤±æ•—: {str(e)}"
            debug_info["error"] = str(e)
            return debug_info
        
        # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        try:
            with conn.cursor() as cursor:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = 'post_records'
                    );
                """)
                table_exists = cursor.fetchone()[0]
                debug_info["table_exists"] = table_exists
                
                if table_exists:
                    # ç²å–è¨˜éŒ„æ•¸
                    cursor.execute("SELECT COUNT(*) FROM post_records")
                    count = cursor.fetchone()[0]
                    debug_info["table_count"] = count
                    
                    # ç²å–æ¨£æœ¬æ•¸æ“š
                    cursor.execute("SELECT post_id, title, status, created_at FROM post_records LIMIT 3")
                    sample_data = cursor.fetchall()
                    debug_info["sample_data"] = [dict(row) for row in sample_data]
                else:
                    debug_info["error"] = "post_records è¡¨ä¸å­˜åœ¨"
                    
        except Exception as e:
            debug_info["error"] = f"æŸ¥è©¢è¡¨ä¿¡æ¯å¤±æ•—: {str(e)}"
        
        return debug_info
        
    except Exception as e:
        return {
            "timestamp": get_current_time().isoformat(),
            "error": f"èª¿è©¦å¤±æ•—: {str(e)}"
        }

@app.post("/admin/fix-database")
async def fix_database():
    """ä¿®å¾©æ•¸æ“šåº«ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_pool:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        # å‰µå»ºæ–°çš„é€£æ¥ä¾†é¿å…äº‹å‹™å•é¡Œ
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–æ•¸æ“šåº«é€£æ¥ä¿¡æ¯
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            return {"error": "DATABASE_URL ç’°å¢ƒè®Šæ•¸ä¸å­˜åœ¨"}
        
        # å‰µå»ºæ–°é€£æ¥
        new_conn = psycopg2.connect(database_url)
        new_conn.autocommit = True
        
        with new_conn.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")
            
            # é‡æ–°å‰µå»ºè¡¨
            cursor.execute("""
                CREATE TABLE post_records (
                    post_id VARCHAR PRIMARY KEY,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id BIGINT,
                    kol_serial INTEGER NOT NULL,
                    kol_nickname VARCHAR NOT NULL,
                    kol_persona VARCHAR,
                    stock_code VARCHAR NOT NULL,
                    stock_name VARCHAR NOT NULL,
                    title VARCHAR NOT NULL,
                    content TEXT NOT NULL,
                    content_md TEXT,
                    status VARCHAR DEFAULT 'draft',
                    reviewer_notes TEXT,
                    approved_by VARCHAR,
                    approved_at TIMESTAMP,
                    scheduled_at TIMESTAMP,
                    published_at TIMESTAMP,
                    cmoney_post_id VARCHAR,
                    cmoney_post_url VARCHAR,
                    publish_error TEXT,
                    views BIGINT DEFAULT 0,
                    likes BIGINT DEFAULT 0,
                    comments BIGINT DEFAULT 0,
                    shares BIGINT DEFAULT 0,
                    topic_id VARCHAR,
                    topic_title VARCHAR,
                    technical_analysis TEXT,
                    serper_data TEXT,
                    quality_score FLOAT,
                    ai_detection_score FLOAT,
                    risk_level VARCHAR,
                    generation_params TEXT,
                    commodity_tags TEXT,
                    alternative_versions TEXT
                );
            """)
            logger.info("âœ… é‡æ–°å‰µå»º post_records è¡¨æˆåŠŸ")
            
        new_conn.close()
        
        return {
            "success": True,
            "message": "æ•¸æ“šåº«å·²ä¿®å¾©ï¼Œè¡¨å·²é‡æ–°å‰µå»º",
            "timestamp": get_current_time().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ä¿®å¾©æ•¸æ“šåº«å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/import-post-records")
async def import_post_records():
    """å°å…¥ post_records æ•¸æ“šï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        # è®€å– JSON æ•¸æ“šæ–‡ä»¶
        json_file_path = '/app/post_records_1788.json'
        if not os.path.exists(json_file_path):
            return {"error": "post_records_1788.json æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
        
        logger.info(f"ğŸ“Š å¾ JSON æ–‡ä»¶åŠ è¼‰ {len(records)} ç­†è¨˜éŒ„")
        
        with conn.cursor() as cursor:
            # æ¸…ç©ºç¾æœ‰æ•¸æ“š
            cursor.execute("DELETE FROM post_records")
            logger.info("ğŸ—‘ï¸ æ¸…ç©ºç¾æœ‰æ•¸æ“š")
            
            # æ‰¹é‡æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            # è½‰æ›è¨˜éŒ„æ ¼å¼
            records_dict = []
            for record in records:
                record_dict = dict(record)
                
                # è™•ç† datetime å­—ç¬¦ä¸²
                for datetime_field in ['created_at', 'updated_at', 'approved_at', 'scheduled_at', 'published_at']:
                    if record_dict.get(datetime_field):
                        if isinstance(record_dict[datetime_field], str):
                            try:
                                record_dict[datetime_field] = datetime.fromisoformat(record_dict[datetime_field].replace('Z', '+00:00'))
                            except:
                                record_dict[datetime_field] = None
                        elif not isinstance(record_dict[datetime_field], datetime):
                            record_dict[datetime_field] = None
                    else:
                        record_dict[datetime_field] = None
                
                # è™•ç† JSON å­—æ®µ - è½‰æ›ç‚º TEXT
                for json_field in ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']:
                    if record_dict.get(json_field):
                        if isinstance(record_dict[json_field], (dict, list)):
                            # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½‰æ›ç‚º JSON å­—ç¬¦ä¸²
                            try:
                                record_dict[json_field] = json.dumps(record_dict[json_field], ensure_ascii=False)
                            except:
                                record_dict[json_field] = None
                        elif isinstance(record_dict[json_field], str):
                            # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸è®Š
                            pass
                        else:
                            record_dict[json_field] = None
                    else:
                        record_dict[json_field] = None
                
                records_dict.append(record_dict)
            
            # æ‰¹é‡æ’å…¥
            cursor.executemany(insert_sql, records_dict)
            conn.commit()
            
            logger.info(f"âœ… æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„")
            
            # é©—è­‰å°å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": get_current_time().isoformat()
            }
            
    except Exception as e:
        logger.error(f"âŒ å°å…¥ post_records å¤±æ•—: {e}")
        return {"error": str(e)}

# ==================== KOL API åŠŸèƒ½ ====================

@app.get("/api/kol/list")
async def get_kol_list():
    """ç²å– KOL åˆ—è¡¨ï¼ˆå«çœŸå¯¦çµ±è¨ˆæ•¸æ“šï¼‰"""
    logger.info("æ”¶åˆ° get_kol_list è«‹æ±‚")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨ï¼Œè¿”å›ç©ºæ•¸æ“š")
            return {
                "success": False,
                "data": [],
                "count": 0,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # ä½¿ç”¨ LEFT JOIN è¨ˆç®—æ¯å€‹ KOL çš„çœŸå¯¦çµ±è¨ˆæ•¸æ“š
            query = """
                SELECT
                    k.*,
                    COALESCE(COUNT(p.post_id), 0) as total_posts,
                    COALESCE(COUNT(CASE WHEN p.status = 'published' THEN 1 END), 0) as published_posts,
                    COALESCE(AVG(CASE
                        WHEN p.status = 'published' AND (p.likes + p.comments + p.shares) > 0
                        THEN (p.likes + p.comments + p.shares) * 1.0
                        ELSE NULL
                    END), 0) as avg_interaction_rate
                FROM kol_profiles k
                LEFT JOIN post_records p ON k.serial::integer = p.kol_serial
                GROUP BY k.id, k.serial, k.nickname, k.member_id, k.persona, k.status, k.owner,
                         k.email, k.password, k.whitelist, k.notes, k.post_times, k.target_audience,
                         k.interaction_threshold, k.content_types, k.common_terms, k.colloquial_terms,
                         k.tone_style, k.typing_habit, k.backstory, k.expertise, k.data_source,
                         k.prompt_persona, k.prompt_style, k.prompt_guardrails, k.prompt_skeleton,
                         k.prompt_cta, k.prompt_hashtags, k.signature, k.emoji_pack, k.model_id,
                         k.template_variant, k.model_temp, k.max_tokens, k.title_openers,
                         k.title_signature_patterns, k.title_tail_word, k.title_banned_words,
                         k.title_style_examples, k.title_retry_max, k.tone_formal, k.tone_emotion,
                         k.tone_confidence, k.tone_urgency, k.tone_interaction, k.question_ratio,
                         k.content_length, k.interaction_starters, k.require_finlab_api, k.allow_hashtags,
                         k.created_time, k.last_updated, k.best_performing_post, k.humor_probability,
                         k.humor_enabled, k.content_style_probabilities, k.analysis_depth_probabilities,
                         k.content_length_probabilities
                ORDER BY k.serial
            """
            cursor.execute(query)
            kols = cursor.fetchall()

            logger.info(f"æŸ¥è©¢åˆ° {len(kols)} å€‹ KOL é…ç½®ï¼ˆå«çµ±è¨ˆæ•¸æ“šï¼‰")

            return {
                "success": True,
                "data": [dict(kol) for kol in kols],
                "count": len(kols),
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢ KOL åˆ—è¡¨å¤±æ•—: {e}")
        return {
            "success": False,
            "data": [],
            "count": 0,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/kol/{serial}")
async def get_kol_detail(serial: str):
    """ç²å–å–®å€‹ KOL çš„è©³ç´°è³‡æ–™ï¼ˆå«çµ±è¨ˆæ•¸æ“šï¼‰"""
    logger.info(f"æ”¶åˆ° get_kol_detail è«‹æ±‚ - Serial: {serial}")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # æŸ¥è©¢å–®å€‹ KOL çš„è³‡æ–™ï¼ŒåŒ…å«çµ±è¨ˆæ•¸æ“š
            query = """
                SELECT
                    k.*,
                    COALESCE(COUNT(p.post_id), 0) as total_posts,
                    COALESCE(COUNT(CASE WHEN p.status = 'published' THEN 1 END), 0) as published_posts,
                    COALESCE(AVG(CASE
                        WHEN p.status = 'published' AND (p.likes + p.comments + p.shares) > 0
                        THEN (p.likes + p.comments + p.shares) * 1.0
                        ELSE NULL
                    END), 0) as avg_interaction_rate
                FROM kol_profiles k
                LEFT JOIN post_records p ON k.serial::integer = p.kol_serial
                WHERE k.serial = %s
                GROUP BY k.id, k.serial, k.nickname, k.member_id, k.persona, k.status, k.owner,
                         k.email, k.password, k.whitelist, k.notes, k.post_times, k.target_audience,
                         k.interaction_threshold, k.content_types, k.common_terms, k.colloquial_terms,
                         k.tone_style, k.typing_habit, k.backstory, k.expertise, k.data_source,
                         k.prompt_persona, k.prompt_style, k.prompt_guardrails, k.prompt_skeleton,
                         k.prompt_cta, k.prompt_hashtags, k.signature, k.emoji_pack, k.model_id,
                         k.template_variant, k.model_temp, k.max_tokens, k.title_openers,
                         k.title_signature_patterns, k.title_tail_word, k.title_banned_words,
                         k.title_style_examples, k.title_retry_max, k.tone_formal, k.tone_emotion,
                         k.tone_confidence, k.tone_urgency, k.tone_interaction, k.question_ratio,
                         k.content_length, k.interaction_starters, k.require_finlab_api, k.allow_hashtags,
                         k.created_time, k.last_updated, k.best_performing_post, k.humor_probability,
                         k.humor_enabled, k.content_style_probabilities, k.analysis_depth_probabilities,
                         k.content_length_probabilities
            """
            cursor.execute(query, (serial,))
            kol = cursor.fetchone()

            if not kol:
                logger.warning(f"æ‰¾ä¸åˆ° serial ç‚º {serial} çš„ KOL")
                return {
                    "success": False,
                    "error": f"æ‰¾ä¸åˆ° serial ç‚º {serial} çš„ KOL"
                }

            logger.info(f"æŸ¥è©¢åˆ° KOL: {kol['nickname']}")
            return dict(kol)

    except Exception as e:
        logger.error(f"æŸ¥è©¢ KOL è©³æƒ…å¤±æ•—: {e}")
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/kol/weekly-posts")
async def get_weekly_posts():
    """ç²å–æœ¬é€±ç™¼æ–‡ç¸½æ•¸"""
    logger.info("æ”¶åˆ° get_weekly_posts è«‹æ±‚")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨ï¼Œè¿”å›ç©ºæ•¸æ“š")
            return {
                "success": False,
                "weekly_posts": 0,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor() as cursor:
            # è¨ˆç®—æœ¬é€±ç™¼æ–‡æ•¸ï¼ˆå¾é€±ä¸€é–‹å§‹ï¼‰
            query = """
                SELECT COUNT(*) as weekly_posts
                FROM post_records
                WHERE created_at >= date_trunc('week', CURRENT_DATE)
                  AND status = 'published'
            """
            cursor.execute(query)
            result = cursor.fetchone()
            weekly_posts = result[0] if result else 0

            logger.info(f"æœ¬é€±ç™¼æ–‡æ•¸: {weekly_posts}")

            return {
                "success": True,
                "weekly_posts": weekly_posts,
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢æœ¬é€±ç™¼æ–‡æ•¸å¤±æ•—: {e}")
        return {
            "success": False,
            "weekly_posts": 0,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/dashboard/kols/{serial}/posts")
async def get_kol_posts(serial: str, page: int = 1, page_size: int = 10):
    """ç²å– KOL çš„ç™¼æ–‡æ­·å²"""
    logger.info(f"æ”¶åˆ° get_kol_posts è«‹æ±‚ - Serial: {serial}, Page: {page}, PageSize: {page_size}")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Calculate offset
            offset = (page - 1) * page_size

            # Count total posts for this KOL
            count_query = """
                SELECT COUNT(*) as count
                FROM post_records
                WHERE kol_serial = %s
            """
            cursor.execute(count_query, (int(serial),))
            total = cursor.fetchone()['count']

            # Get paginated posts
            query = """
                SELECT
                    post_id,
                    title,
                    content,
                    status,
                    stock_code,
                    created_at,
                    published_at,
                    likes,
                    comments,
                    shares,
                    cmoney_post_url
                FROM post_records
                WHERE kol_serial = %s
                ORDER BY created_at DESC
                LIMIT %s OFFSET %s
            """
            cursor.execute(query, (int(serial), page_size, offset))
            posts = cursor.fetchall()

            logger.info(f"æŸ¥è©¢åˆ° {len(posts)} ç¯‡è²¼æ–‡ï¼Œç¸½æ•¸: {total}")

            return {
                "success": True,
                "data": {
                    "posts": [dict(post) for post in posts],
                    "pagination": {
                        "current_page": page,
                        "page_size": page_size,
                        "total_items": total,
                        "total_pages": (total + page_size - 1) // page_size
                    }
                }
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢ KOL ç™¼æ–‡æ­·å²å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/dashboard/kols/{serial}/interactions")
async def get_kol_interactions(serial: str):
    """ç²å– KOL çš„äº’å‹•æ•¸æ“šå’Œè¶¨å‹¢"""
    logger.info(f"æ”¶åˆ° get_kol_interactions è«‹æ±‚ - Serial: {serial}")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Get interaction trend data (daily aggregation for last 30 days)
            query = """
                SELECT
                    DATE(created_at) as date,
                    COUNT(*) as post_count,
                    COALESCE(SUM(likes), 0) as total_likes,
                    COALESCE(SUM(comments), 0) as total_comments,
                    COALESCE(SUM(shares), 0) as total_shares,
                    COALESCE(AVG(likes), 0) as avg_likes,
                    COALESCE(AVG(comments), 0) as avg_comments,
                    COALESCE(AVG(shares), 0) as avg_shares
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                  AND created_at >= CURRENT_DATE - INTERVAL '30 days'
                GROUP BY DATE(created_at)
                ORDER BY DATE(created_at) DESC
            """
            cursor.execute(query, (int(serial),))
            trend_data = cursor.fetchall()

            logger.info(f"æŸ¥è©¢åˆ° {len(trend_data)} å¤©çš„äº’å‹•æ•¸æ“š")

            return {
                "success": True,
                "data": {
                    "interaction_trend": [dict(row) for row in trend_data]
                }
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢ KOL äº’å‹•æ•¸æ“šå¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.get("/api/kol/{serial}/stats")
async def get_kol_stats(serial: str):
    """ç²å– KOL çš„å®Œæ•´çµ±è¨ˆæ•¸æ“šï¼ˆåŒ…å«åœ–è¡¨æ‰€éœ€æ•¸æ“šï¼‰"""
    logger.info(f"æ”¶åˆ° get_kol_stats è«‹æ±‚ - Serial: {serial}")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # 1. æ ¸å¿ƒæŒ‡æ¨™
            cursor.execute("""
                SELECT
                    COUNT(*) as total_posts,
                    COUNT(CASE WHEN status = 'published' THEN 1 END) as published_posts,
                    COUNT(CASE WHEN status = 'draft' THEN 1 END) as draft_posts,
                    COALESCE(AVG(CASE WHEN status = 'published' THEN likes END), 0) as avg_likes,
                    COALESCE(AVG(CASE WHEN status = 'published' THEN comments END), 0) as avg_comments,
                    COALESCE(AVG(CASE WHEN status = 'published' THEN shares END), 0) as avg_shares,
                    COALESCE(SUM(CASE WHEN status = 'published' THEN likes + comments + shares END), 0) as total_interactions
                FROM post_records
                WHERE kol_serial = %s
            """, (int(serial),))
            core_metrics = cursor.fetchone()

            # 2. ç™¼æ–‡è¶¨å‹¢ï¼ˆæœ€è¿‘3å€‹æœˆï¼ŒæŒ‰æ—¥åˆ†çµ„ï¼‰
            cursor.execute("""
                SELECT
                    DATE(created_at) as date,
                    COUNT(*) as count
                FROM post_records
                WHERE kol_serial = %s
                  AND created_at >= CURRENT_DATE - INTERVAL '3 months'
                GROUP BY DATE(created_at)
                ORDER BY DATE(created_at) ASC
            """, (int(serial),))
            posting_trend = cursor.fetchall()

            # 3. äº’å‹•è¶¨å‹¢ï¼ˆæœ€è¿‘3å€‹æœˆï¼ŒæŒ‰æ—¥åˆ†çµ„ï¼‰
            cursor.execute("""
                SELECT
                    DATE(created_at) as date,
                    COUNT(*) as post_count,
                    COALESCE(SUM(likes), 0) as total_likes,
                    COALESCE(SUM(comments), 0) as total_comments,
                    COALESCE(SUM(shares), 0) as total_shares,
                    COALESCE(AVG(likes), 0) as avg_likes,
                    COALESCE(AVG(comments), 0) as avg_comments,
                    COALESCE(AVG(shares), 0) as avg_shares
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                  AND created_at >= CURRENT_DATE - INTERVAL '3 months'
                GROUP BY DATE(created_at)
                ORDER BY DATE(created_at) ASC
            """, (int(serial),))
            interaction_trend = cursor.fetchall()

            # 4. Top 10 è¡¨ç¾æœ€ä½³æ–‡ç« 
            cursor.execute("""
                SELECT
                    post_id,
                    title,
                    likes,
                    comments,
                    shares,
                    (likes + comments + shares) as total_interactions,
                    created_at,
                    cmoney_post_url
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                ORDER BY (likes + comments + shares) DESC
                LIMIT 10
            """, (int(serial),))
            top_posts = cursor.fetchall()

            # 5. Bottom 10 è¡¨ç¾æœ€å·®æ–‡ç« 
            cursor.execute("""
                SELECT
                    post_id,
                    title,
                    likes,
                    comments,
                    shares,
                    (likes + comments + shares) as total_interactions,
                    created_at,
                    cmoney_post_url
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                ORDER BY (likes + comments + shares) ASC
                LIMIT 10
            """, (int(serial),))
            bottom_posts = cursor.fetchall()

            # 6. è©±é¡Œçµ±è¨ˆï¼ˆTopicï¼‰
            cursor.execute("""
                SELECT
                    topic_title,
                    COUNT(*) as count,
                    COALESCE(AVG(likes + comments + shares), 0) as avg_interaction
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                  AND topic_title IS NOT NULL
                GROUP BY topic_title
                ORDER BY count DESC
                LIMIT 20
            """, (int(serial),))
            topic_stats = cursor.fetchall()

            # 7. è‚¡ç¥¨çµ±è¨ˆï¼ˆStockï¼‰
            cursor.execute("""
                SELECT
                    stock_code,
                    stock_name,
                    COUNT(*) as count,
                    COALESCE(AVG(likes + comments + shares), 0) as avg_interaction
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                  AND stock_code IS NOT NULL
                  AND stock_code != '0000'
                GROUP BY stock_code, stock_name
                ORDER BY count DESC
                LIMIT 20
            """, (int(serial),))
            stock_stats = cursor.fetchall()

            # 8. æ™‚é–“ç†±åŠ›åœ–ï¼ˆå°æ™‚ + æ˜ŸæœŸï¼‰
            cursor.execute("""
                SELECT
                    EXTRACT(HOUR FROM created_at) as hour,
                    EXTRACT(DOW FROM created_at) as day_of_week,
                    COALESCE(AVG(likes + comments + shares), 0) as avg_interaction,
                    COUNT(*) as count
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                GROUP BY EXTRACT(HOUR FROM created_at), EXTRACT(DOW FROM created_at)
                ORDER BY day_of_week, hour
            """, (int(serial),))
            time_heatmap = cursor.fetchall()

            # 9. æˆé•·è¶¨å‹¢ï¼ˆæœˆåº¦çµ±è¨ˆï¼Œæœ€è¿‘6å€‹æœˆï¼‰
            cursor.execute("""
                SELECT
                    TO_CHAR(created_at, 'YYYY-MM') as month,
                    COUNT(*) as count,
                    COALESCE(SUM(likes + comments + shares), 0) as total_interactions
                FROM post_records
                WHERE kol_serial = %s
                  AND status = 'published'
                  AND created_at >= CURRENT_DATE - INTERVAL '6 months'
                GROUP BY TO_CHAR(created_at, 'YYYY-MM')
                ORDER BY month ASC
            """, (int(serial),))
            growth_trend = cursor.fetchall()

            logger.info(f"æŸ¥è©¢åˆ°å®Œæ•´çµ±è¨ˆæ•¸æ“š - Serial: {serial}")

            return {
                "success": True,
                "data": {
                    "core_metrics": dict(core_metrics) if core_metrics else {},
                    "posting_trend": [dict(row) for row in posting_trend],
                    "interaction_trend": [dict(row) for row in interaction_trend],
                    "top_posts": [dict(row) for row in top_posts],
                    "bottom_posts": [dict(row) for row in bottom_posts],
                    "topic_stats": [dict(row) for row in topic_stats],
                    "stock_stats": [dict(row) for row in stock_stats],
                    "time_heatmap": [dict(row) for row in time_heatmap],
                    "growth_trend": [dict(row) for row in growth_trend]
                }
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢ KOL çµ±è¨ˆæ•¸æ“šå¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e)
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/kol/test-login")
async def test_kol_login(request: Request):
    """
    æ¸¬è©¦ CMoney ç™»å…¥ä¸¦ç²å– Bearer Token

    ç”¨æ–¼åœ¨å‰µå»º KOL å‰é©—è­‰å¸³è™Ÿå¯†ç¢¼æ˜¯å¦æ­£ç¢º

    Request body:
    - email: CMoney ç™»å…¥éƒµç®±
    - password: CMoney ç™»å…¥å¯†ç¢¼

    Response:
    - success: bool - ç™»å…¥æ˜¯å¦æˆåŠŸ
    - token: str - Bearer tokenï¼ˆåƒ…åœ¨æˆåŠŸæ™‚è¿”å›ï¼‰
    - error: str - éŒ¯èª¤è¨Šæ¯ï¼ˆåƒ…åœ¨å¤±æ•—æ™‚è¿”å›ï¼‰
    """
    try:
        data = await request.json()
        email = data.get("email")
        password = data.get("password")

        if not email or not password:
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…å¡«æ¬„ä½: email, password",
                "timestamp": get_current_time().isoformat()
            }

        # å°‡ src è·¯å¾‘åŠ å…¥ Python path
        src_path = '/app/src'
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials
        cmoney_client = CMoneyClient()

        # å˜—è©¦ç™»å…¥
        credentials = LoginCredentials(email=email, password=password)
        try:
            access_token = await cmoney_client.login(credentials)
            logger.info(f"âœ… æ¸¬è©¦ç™»å…¥æˆåŠŸ: {email}")

            return {
                "success": True,
                "token": access_token.token,
                "message": "ç™»å…¥æˆåŠŸï¼ŒBearer Token å·²ç²å–",
                "timestamp": get_current_time().isoformat()
            }
        except Exception as login_error:
            logger.error(f"âŒ æ¸¬è©¦ç™»å…¥å¤±æ•—: {login_error}")
            return {
                "success": False,
                "error": f"ç™»å…¥å¤±æ•—: {str(login_error)}",
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ test_kol_login ç•°å¸¸: {e}")
        return {
            "success": False,
            "error": f"æ¸¬è©¦ç™»å…¥æ™‚ç™¼ç”Ÿç•°å¸¸: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }


@app.post("/api/kol/test-nickname")
async def test_kol_nickname(request: Request):
    """
    æ¸¬è©¦ CMoney æš±ç¨±æ›´æ–°

    ç”¨æ–¼åœ¨å‰µå»º KOL å‰é©—è­‰æš±ç¨±æ˜¯å¦å¯ç”¨

    Request body:
    - email: CMoney ç™»å…¥éƒµç®±
    - password: CMoney ç™»å…¥å¯†ç¢¼
    - nickname: æœŸæœ›çš„æš±ç¨±

    Response:
    - success: bool - æš±ç¨±æ›´æ–°æ˜¯å¦æˆåŠŸ
    - new_nickname: str - å¯¦éš›æ›´æ–°å¾Œçš„æš±ç¨±ï¼ˆåƒ…åœ¨æˆåŠŸæ™‚è¿”å›ï¼‰
    - error: str - éŒ¯èª¤è¨Šæ¯ï¼ˆåƒ…åœ¨å¤±æ•—æ™‚è¿”å›ï¼‰
    """
    try:
        data = await request.json()
        email = data.get("email")
        password = data.get("password")
        nickname = data.get("nickname")

        if not email or not password or not nickname:
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…å¡«æ¬„ä½: email, password, nickname",
                "timestamp": get_current_time().isoformat()
            }

        # å°‡ src è·¯å¾‘åŠ å…¥ Python path
        src_path = '/app/src'
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials
        cmoney_client = CMoneyClient()

        # å…ˆç™»å…¥ç²å– token
        credentials = LoginCredentials(email=email, password=password)
        try:
            access_token = await cmoney_client.login(credentials)
            logger.info(f"âœ… æ¸¬è©¦ç™»å…¥æˆåŠŸ: {email}")
        except Exception as login_error:
            logger.error(f"âŒ æ¸¬è©¦ç™»å…¥å¤±æ•—: {login_error}")
            return {
                "success": False,
                "error": f"ç™»å…¥å¤±æ•—: {str(login_error)}",
                "timestamp": get_current_time().isoformat()
            }

        # å˜—è©¦æ›´æ–°æš±ç¨±
        try:
            nickname_result = await cmoney_client.update_nickname(access_token.token, nickname)

            if not nickname_result.success:
                logger.warning(f"âš ï¸ æ¸¬è©¦æš±ç¨±æ›´æ–°å¤±æ•—: {nickname_result.error_message}")
                return {
                    "success": False,
                    "error": f"æš±ç¨±æ›´æ–°å¤±æ•—: {nickname_result.error_message}",
                    "detail": "æš±ç¨±å¯èƒ½å·²è¢«ä½¿ç”¨ï¼Œè«‹å˜—è©¦å…¶ä»–æš±ç¨±",
                    "timestamp": get_current_time().isoformat()
                }

            logger.info(f"âœ… æ¸¬è©¦æš±ç¨±æ›´æ–°æˆåŠŸ: {nickname}")
            return {
                "success": True,
                "new_nickname": nickname_result.new_nickname or nickname,
                "message": "æš±ç¨±æ›´æ–°æˆåŠŸ",
                "timestamp": get_current_time().isoformat()
            }

        except Exception as nickname_error:
            logger.error(f"âŒ æ¸¬è©¦æš±ç¨±æ›´æ–°ç•°å¸¸: {nickname_error}")
            return {
                "success": False,
                "error": f"æš±ç¨±æ›´æ–°æ™‚ç™¼ç”Ÿç•°å¸¸: {str(nickname_error)}",
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ test_kol_nickname ç•°å¸¸: {e}")
        return {
            "success": False,
            "error": f"æ¸¬è©¦æš±ç¨±æ™‚ç™¼ç”Ÿç•°å¸¸: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }


@app.post("/api/kol/create")
async def create_kol(request: Request):
    """
    å‰µå»ºæ–°çš„ KOL è§’è‰²

    Phase 1: åŸºæœ¬è³‡è¨Šï¼ˆå¿…å¡«ï¼‰
    - email: CMoney ç™»å…¥éƒµç®±
    - password: CMoney ç™»å…¥å¯†ç¢¼
    - nickname: æœŸæœ›çš„ KOL æš±ç¨±

    Phase 2: AI ç”Ÿæˆå€‹æ€§åŒ–è³‡æ–™ï¼ˆé¸å¡«ï¼‰
    - ai_description: 1000å­—ä»¥å…§çš„ KOL æè¿°ï¼Œç”¨æ–¼ AI ç”Ÿæˆå€‹æ€§åŒ–æ¬„ä½
    """
    logger.info("æ”¶åˆ° create_kol è«‹æ±‚")

    conn = None
    try:
        # è§£æè«‹æ±‚æ•¸æ“š
        data = await request.json()
        email = data.get("email")
        password = data.get("password")
        nickname = data.get("nickname")
        member_id_from_user = data.get("member_id", "")  # ç”¨æˆ¶æ‰‹å‹•è¼¸å…¥çš„ member_idï¼ˆé¸å¡«ï¼‰
        ai_description = data.get("ai_description", "")  # AI æè¿°ï¼ˆé¸å¡«ï¼‰
        model_id = data.get("model_id", "gpt-4o-mini")  # AI æ¨¡å‹ ID

        # Prompt æ¬„ä½ï¼ˆPhase 1: æ‰‹å‹•å¡«å¯«ï¼‰
        prompt_persona = data.get("prompt_persona", "")
        prompt_style = data.get("prompt_style", "")
        prompt_guardrails = data.get("prompt_guardrails", "")
        prompt_skeleton = data.get("prompt_skeleton", "")

        logger.info(f"ğŸ“ æ”¶åˆ°å‰µå»º KOL è«‹æ±‚: email={email}, nickname={nickname}, member_id={member_id_from_user or '(æœªæä¾›)'}")
        logger.info(f"ğŸ“ Prompt æ¬„ä½: persona={bool(prompt_persona)}, style={bool(prompt_style)}, guardrails={bool(prompt_guardrails)}, skeleton={bool(prompt_skeleton)}")

        # é©—è­‰å¿…å¡«æ¬„ä½ (nickname æ”¹ç‚ºé¸å¡«)
        if not email or not password:
            logger.error("âŒ ç¼ºå°‘å¿…å¡«æ¬„ä½")
            return {
                "success": False,
                "error": "ç¼ºå°‘å¿…å¡«æ¬„ä½: email, password",
                "timestamp": get_current_time().isoformat()
            }

        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        # Phase 1: ä½¿ç”¨ CMoney API ç™»å…¥ (æš±ç¨±æ›´æ–°ç‚ºé¸å¡«)
        if nickname:
            logger.info(f"ğŸ“ Phase 1: å˜—è©¦ä½¿ç”¨ {email} ç™»å…¥ CMoney ä¸¦æ›´æ–°æš±ç¨±ç‚º {nickname}")
        else:
            logger.info(f"ğŸ“ Phase 1: å˜—è©¦ä½¿ç”¨ {email} ç™»å…¥ CMoney (ä¸æ›´æ–°æš±ç¨±ï¼Œä½¿ç”¨ç¾æœ‰æš±ç¨±)")

        # å°‡ src è·¯å¾‘åŠ å…¥ Python path
        src_path = '/app/src'
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials
        cmoney_client = CMoneyClient()

        # ç™»å…¥ CMoney
        credentials = LoginCredentials(email=email, password=password)
        try:
            access_token = await cmoney_client.login(credentials)
            logger.info(f"âœ… CMoney ç™»å…¥æˆåŠŸ: {email}")
        except Exception as login_error:
            logger.error(f"âŒ CMoney ç™»å…¥å¤±æ•—: {login_error}")
            return {
                "success": False,
                "error": f"CMoney ç™»å…¥å¤±æ•—: {str(login_error)}",
                "phase": "login",
                "timestamp": get_current_time().isoformat()
            }

        # å˜—è©¦æ›´æ–°æš±ç¨± (åƒ…ç•¶æä¾›æš±ç¨±æ™‚)
        actual_nickname = nickname  # Default to provided nickname
        if nickname:
            try:
                nickname_result = await cmoney_client.update_nickname(access_token.token, nickname)

                if not nickname_result.success:
                    logger.warning(f"âš ï¸ æš±ç¨±æ›´æ–°å¤±æ•—: {nickname_result.error_message}")
                    return {
                        "success": False,
                        "error": f"æš±ç¨±æ›´æ–°å¤±æ•—: {nickname_result.error_message}",
                        "phase": "nickname_update",
                        "detail": "å¯èƒ½æ˜¯æš±ç¨±å·²è¢«ä½¿ç”¨ï¼Œè«‹å˜—è©¦å…¶ä»–æš±ç¨±",
                        "timestamp": get_current_time().isoformat()
                    }

                logger.info(f"âœ… æš±ç¨±æ›´æ–°æˆåŠŸ: {nickname}")
                actual_nickname = nickname_result.new_nickname or nickname

            except Exception as nickname_error:
                logger.error(f"âŒ æš±ç¨±æ›´æ–°ç•°å¸¸: {nickname_error}")
                return {
                    "success": False,
                    "error": f"æš±ç¨±æ›´æ–°ç•°å¸¸: {str(nickname_error)}",
                    "phase": "nickname_update",
                    "timestamp": get_current_time().isoformat()
                }
        else:
            # ä¸æ›´æ–°æš±ç¨±ï¼Œä½¿ç”¨ CMoney å¸³è™Ÿç¾æœ‰æš±ç¨±
            logger.info(f"â­ï¸  è·³éæš±ç¨±æ›´æ–°ï¼Œå°‡ä½¿ç”¨ CMoney å¸³è™Ÿç¾æœ‰æš±ç¨±")
            actual_nickname = f"KOL-{email.split('@')[0]}"  # Temporary placeholder

        # Phase 2: AI ç”Ÿæˆå€‹æ€§åŒ–è³‡æ–™ï¼ˆå¦‚æœæä¾›äº† ai_descriptionï¼‰
        ai_generated_profile = {}
        if ai_description and gpt_generator:
            logger.info(f"ğŸ¤– Phase 2: ä½¿ç”¨ AI ç”Ÿæˆå€‹æ€§åŒ–è³‡æ–™...")
            try:
                # æº–å‚™ AI prompt
                ai_prompt = f"""
                æ ¹æ“šä»¥ä¸‹æè¿°ï¼Œç‚ºé€™å€‹ KOL ç”Ÿæˆå®Œæ•´çš„å€‹æ€§åŒ–è¨­å®šã€‚è«‹ä»¥ JSON æ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š

                KOL æè¿°ï¼š
                {ai_description}

                è«‹ç”Ÿæˆä»¥ä¸‹æ¬„ä½çš„å€¼ï¼š
                - persona: äººè¨­é¡å‹ï¼ˆfundamental/technical/news/casual ä¹‹ä¸€ï¼‰
                - target_audience: ç›®æ¨™å—çœ¾
                - expertise: å°ˆæ¥­é ˜åŸŸ
                - backstory: èƒŒæ™¯æ•…äº‹ï¼ˆ50-100å­—ï¼‰
                - tone_style: èªæ°£é¢¨æ ¼æè¿°
                - typing_habit: æ‰“å­—ç¿’æ…£æè¿°
                - common_terms: å¸¸ç”¨è¡“èªï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
                - colloquial_terms: å£èªç”¨è©ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
                - signature: å€‹äººç°½å
                - emoji_pack: å¸¸ç”¨è¡¨æƒ…ç¬¦è™Ÿï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
                - tone_formal: æ­£å¼ç¨‹åº¦ï¼ˆ1-10ï¼‰
                - tone_emotion: æƒ…æ„Ÿç¨‹åº¦ï¼ˆ1-10ï¼‰
                - tone_confidence: è‡ªä¿¡ç¨‹åº¦ï¼ˆ1-10ï¼‰
                - tone_urgency: ç·Šæ€¥ç¨‹åº¦ï¼ˆ1-10ï¼‰
                - tone_interaction: äº’å‹•ç¨‹åº¦ï¼ˆ1-10ï¼‰
                - question_ratio: å•é¡Œæ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰
                - content_length: å…§å®¹é•·åº¦åå¥½ï¼ˆshort/medium/longï¼‰

                è«‹ç¢ºä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚
                """

                # èª¿ç”¨ GPT ç”Ÿæˆ
                import openai
                openai.api_key = os.getenv("OPENAI_API_KEY")

                response = openai.ChatCompletion.create(
                    model="gpt-4",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ KOL äººè¨­è¨­è¨ˆå¸«ï¼Œæ“…é•·æ ¹æ“šæè¿°ç”Ÿæˆå®Œæ•´çš„ KOL å€‹æ€§åŒ–è¨­å®šã€‚"},
                        {"role": "user", "content": ai_prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1500
                )

                # è§£æ AI å›æ‡‰
                ai_response = response.choices[0].message.content
                import json
                ai_generated_profile = json.loads(ai_response)
                logger.info(f"âœ… AI å€‹æ€§åŒ–è³‡æ–™ç”ŸæˆæˆåŠŸ")

            except Exception as ai_error:
                logger.warning(f"âš ï¸ AI ç”Ÿæˆå¤±æ•—: {ai_error}ï¼Œä½¿ç”¨é è¨­å€¼")
                # AI ç”Ÿæˆå¤±æ•—ä¸é˜»æ–·æµç¨‹ï¼Œä½¿ç”¨é è¨­å€¼

        # æº–å‚™å¯«å…¥æ•¸æ“šåº«çš„è³‡æ–™
        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # ğŸ”¥ FIX: Extract serial from email (æ”¯æ´å…©ç¨®æ ¼å¼)
            # 1. forum_XXX@cmoney.com.tw â†’ ä½¿ç”¨ XXX ä½œç‚º serial
            # 2. å…¶ä»–æ ¼å¼ â†’ å¾ 1000 é–‹å§‹åˆ†é…
            import re
            email_pattern = r'forum_(\d+)@cmoney\.com\.tw'
            match = re.match(email_pattern, email)

            if match:
                # æ ¼å¼ 1: forum_XXX@cmoney.com.tw
                next_serial = int(match.group(1))
                logger.info(f"âœ… å¾éƒµç®±æå– KOL serial: {next_serial} (email: {email})")
            else:
                # æ ¼å¼ 2: å…¶ä»–æ ¼å¼ï¼Œå¾ 1000 é–‹å§‹åˆ†é…
                logger.info(f"ğŸ“§ éƒµç®±ä¸ç¬¦åˆ forum_XXX æ ¼å¼ï¼Œå¾ 1000 é–‹å§‹åˆ†é… serial: {email}")

                # æŸ¥æ‰¾å·²ä½¿ç”¨çš„æœ€å¤§ serial (>= 1000)
                cursor.execute("""
                    SELECT MAX(CAST(serial AS INTEGER)) as max_serial
                    FROM kol_profiles
                    WHERE CAST(serial AS INTEGER) >= 1000
                """)
                result = cursor.fetchone()
                max_serial = result['max_serial'] if result and result['max_serial'] else 999

                next_serial = max_serial + 1
                logger.info(f"âœ… åˆ†é…æ–° serial: {next_serial} (å¾ {max_serial} éå¢)")

            # è™•ç† member_id - å¦‚æœç”¨æˆ¶æ²’æä¾›ï¼Œä½¿ç”¨ serial ä½œç‚º member_id
            member_id = member_id_from_user if member_id_from_user else str(next_serial)
            logger.info(f"âœ… Member ID è¨­å®šç‚º: {member_id} {'(ç”¨æˆ¶æä¾›)' if member_id_from_user else '(ä½¿ç”¨ serial)'}")

            # Check if serial already exists
            cursor.execute("SELECT serial FROM kol_profiles WHERE serial = %s", (str(next_serial),))
            existing = cursor.fetchone()
            if existing:
                logger.error(f"âŒ KOL serial {next_serial} å·²å­˜åœ¨")
                return {
                    "success": False,
                    "error": f"KOL serial {next_serial} å·²å­˜åœ¨ï¼Œè«‹ä½¿ç”¨ä¸åŒçš„éƒµç®±",
                    "phase": "validation",
                    "timestamp": get_current_time().isoformat()
                }

            # åˆä½µ AI ç”Ÿæˆçš„å€¼å’Œé è¨­å€¼
            persona = ai_generated_profile.get("persona", "casual")
            target_audience = ai_generated_profile.get("target_audience", "ä¸€èˆ¬æŠ•è³‡è€…")
            expertise = ai_generated_profile.get("expertise", "")
            backstory = ai_generated_profile.get("backstory", "")
            tone_style = ai_generated_profile.get("tone_style", "å‹å–„ã€è¦ªåˆ‡")
            typing_habit = ai_generated_profile.get("typing_habit", "æ­£å¸¸")
            common_terms = ai_generated_profile.get("common_terms", "")
            colloquial_terms = ai_generated_profile.get("colloquial_terms", "")
            signature = ai_generated_profile.get("signature", "")
            emoji_pack = ai_generated_profile.get("emoji_pack", "ğŸ˜Š,ğŸ‘,ğŸ’ª")
            tone_formal = ai_generated_profile.get("tone_formal", 5)
            tone_emotion = ai_generated_profile.get("tone_emotion", 5)
            tone_confidence = ai_generated_profile.get("tone_confidence", 7)
            tone_urgency = ai_generated_profile.get("tone_urgency", 5)
            tone_interaction = ai_generated_profile.get("tone_interaction", 7)
            question_ratio = ai_generated_profile.get("question_ratio", 0.3)
            content_length = ai_generated_profile.get("content_length", "medium")

            # æ’å…¥æ–°çš„ KOL åˆ°æ•¸æ“šåº«
            insert_sql = """
                INSERT INTO kol_profiles (
                    serial, nickname, member_id, persona, status, owner, email, password,
                    whitelist, notes, post_times, target_audience, interaction_threshold,
                    common_terms, colloquial_terms, tone_style, typing_habit, backstory,
                    expertise, signature, emoji_pack, tone_formal, tone_emotion,
                    tone_confidence, tone_urgency, tone_interaction, question_ratio,
                    content_length, model_id,
                    prompt_persona, prompt_style, prompt_guardrails, prompt_skeleton,
                    created_time, last_updated
                ) VALUES (
                    %s, %s, %s, %s, 'active', 'system', %s, %s,
                    true, 'é€šé API å‰µå»º', '09:00,12:00,15:00', %s, 0,
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s,
                    %s, %s,
                    %s, %s, %s, %s,
                    CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                )
                RETURNING serial, nickname, member_id, persona, email
            """

            cursor.execute(insert_sql, (
                next_serial, actual_nickname, member_id, persona, email, password,
                target_audience, common_terms, colloquial_terms, tone_style, typing_habit, backstory,
                expertise, signature, emoji_pack, tone_formal, tone_emotion,
                tone_confidence, tone_urgency, tone_interaction, question_ratio,
                content_length, model_id,
                prompt_persona, prompt_style, prompt_guardrails, prompt_skeleton
            ))

            new_kol = cursor.fetchone()
            conn.commit()

            logger.info(f"âœ… KOL å‰µå»ºæˆåŠŸ: Serial={new_kol['serial']}, Nickname={new_kol['nickname']}")

            return {
                "success": True,
                "message": "KOL å‰µå»ºæˆåŠŸ",
                "data": {
                    "serial": new_kol['serial'],
                    "nickname": new_kol['nickname'],
                    "member_id": new_kol['member_id'],
                    "persona": new_kol['persona'],
                    "email": new_kol['email'],
                    "ai_generated": bool(ai_description and gpt_generator),
                    "ai_profile": ai_generated_profile if ai_generated_profile else None
                },
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ å‰µå»º KOL å¤±æ•—: {e}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": f"å‰µå»º KOL å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/kol/fix-sequence")
async def fix_kol_sequence():
    """
    ä¿®å¾© kol_profiles ID åºåˆ—ä¸åŒæ­¥å•é¡Œï¼ˆæ°¸ä¹…è§£æ±ºæ–¹æ¡ˆï¼‰

    å‰µå»ºè§¸ç™¼å™¨è‡ªå‹•åŒæ­¥åºåˆ—ï¼Œé˜²æ­¢æœªä¾†å†æ¬¡ç™¼ç”Ÿ duplicate key éŒ¯èª¤

    Response:
    - success: bool - æ˜¯å¦ä¿®å¾©æˆåŠŸ
    - message: str - æ“ä½œè¨Šæ¯
    - data: dict - ä¿®å¾©å‰å¾Œçš„ç‹€æ…‹è³‡è¨Š
    """
    logger.info("æ”¶åˆ°ä¿®å¾© KOL åºåˆ—çš„è«‹æ±‚")

    conn = None
    try:
        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Step 1: æŸ¥è©¢ç•¶å‰ç‹€æ³
            cursor.execute("SELECT MAX(id) FROM kol_profiles")
            max_id_result = cursor.fetchone()
            max_id = max_id_result['max'] if max_id_result else 0

            cursor.execute("SELECT last_value FROM kol_profiles_id_seq")
            seq_result = cursor.fetchone()
            old_seq_value = seq_result['last_value'] if seq_result else 0

            logger.info(f"ğŸ“Š ç•¶å‰ç‹€æ³: max_id={max_id}, seq_value={old_seq_value}")

            # Step 2: ä¿®å¾©ç•¶å‰åºåˆ—
            cursor.execute("SELECT setval('kol_profiles_id_seq', %s)", (max_id,))
            new_seq_result = cursor.fetchone()
            new_seq_value = new_seq_result['setval'] if new_seq_result else 0

            logger.info(f"âœ… åºåˆ—å·²æ›´æ–°: {old_seq_value} â†’ {new_seq_value}")

            # Step 3: å‰µå»ºè§¸ç™¼å™¨å‡½æ•¸
            cursor.execute("""
                CREATE OR REPLACE FUNCTION sync_kol_profiles_sequence()
                RETURNS TRIGGER AS $$
                BEGIN
                    PERFORM setval('kol_profiles_id_seq', (SELECT COALESCE(MAX(id), 0) FROM kol_profiles));
                    RETURN NEW;
                END;
                $$ LANGUAGE plpgsql;
            """)
            logger.info("âœ… è§¸ç™¼å™¨å‡½æ•¸å·²å‰µå»º")

            # Step 4: å‰µå»ºè§¸ç™¼å™¨
            cursor.execute("DROP TRIGGER IF EXISTS sync_kol_sequence_trigger ON kol_profiles")
            cursor.execute("""
                CREATE TRIGGER sync_kol_sequence_trigger
                    AFTER INSERT ON kol_profiles
                    FOR EACH STATEMENT
                    EXECUTE FUNCTION sync_kol_profiles_sequence();
            """)
            logger.info("âœ… è§¸ç™¼å™¨å·²å‰µå»º")

            # æäº¤è®Šæ›´
            conn.commit()
            logger.info("ğŸ’¾ è®Šæ›´å·²æäº¤")

            # é©—è­‰è§¸ç™¼å™¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT COUNT(*) as count
                FROM pg_trigger
                WHERE tgname = 'sync_kol_sequence_trigger'
            """)
            trigger_result = cursor.fetchone()
            trigger_exists = trigger_result['count'] > 0 if trigger_result else False

            return {
                "success": True,
                "message": "KOL ID åºåˆ—å·²æ°¸ä¹…ä¿®å¾©ï¼è§¸ç™¼å™¨å·²å•Ÿç”¨ï¼Œæœªä¾†ä¸æœƒå†å‡ºç¾ duplicate key éŒ¯èª¤",
                "data": {
                    "before": {
                        "max_id": max_id,
                        "sequence_value": old_seq_value,
                        "is_synced": old_seq_value >= max_id
                    },
                    "after": {
                        "max_id": max_id,
                        "sequence_value": new_seq_value,
                        "next_id": new_seq_value + 1,
                        "is_synced": True,
                        "trigger_enabled": trigger_exists
                    }
                },
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ ä¿®å¾©åºåˆ—å¤±æ•—: {e}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": f"ä¿®å¾©åºåˆ—å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.delete("/api/kol/{serial}")
async def delete_kol(serial: str):
    """
    åˆªé™¤ KOL è§’è‰²

    Parameters:
    - serial: KOL åºè™Ÿ

    Response:
    - success: bool - æ˜¯å¦åˆªé™¤æˆåŠŸ
    - message: str - æˆåŠŸè¨Šæ¯
    - error: str - éŒ¯èª¤è¨Šæ¯ï¼ˆåƒ…åœ¨å¤±æ•—æ™‚è¿”å›ï¼‰
    """
    logger.info(f"æ”¶åˆ°åˆªé™¤ KOL è«‹æ±‚: serial={serial}")

    conn = None
    try:
        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # æª¢æŸ¥ KOL æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT serial, nickname FROM kol_profiles WHERE serial = %s", (serial,))
            existing_kol = cursor.fetchone()

            if not existing_kol:
                logger.warning(f"âš ï¸ KOL serial {serial} ä¸å­˜åœ¨")
                return {
                    "success": False,
                    "error": f"KOL serial {serial} ä¸å­˜åœ¨",
                    "timestamp": get_current_time().isoformat()
                }

            # åŸ·è¡Œåˆªé™¤
            cursor.execute("DELETE FROM kol_profiles WHERE serial = %s", (serial,))
            conn.commit()

            logger.info(f"âœ… KOL åˆªé™¤æˆåŠŸ: Serial={serial}, Nickname={existing_kol['nickname']}")

            return {
                "success": True,
                "message": f"KOL åˆªé™¤æˆåŠŸ (Serial: {serial}, Nickname: {existing_kol['nickname']})",
                "deleted_kol": {
                    "serial": existing_kol['serial'],
                    "nickname": existing_kol['nickname']
                },
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ åˆªé™¤ KOL å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": f"åˆªé™¤ KOL å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.put("/api/kol/{serial}")
async def update_kol(serial: str, request: Request):
    """
    æ›´æ–° KOL æ‰€æœ‰æ¬„ä½

    Parameters:
    - serial: KOL åºè™Ÿ
    - request body: ä»»ä½• kol_profiles è¡¨ä¸­çš„å¯æ›´æ–°æ¬„ä½
    """
    logger.info(f"æ”¶åˆ°æ›´æ–° KOL è«‹æ±‚: serial={serial}")

    conn = None
    try:
        data = await request.json()
        logger.info(f"æ¥æ”¶åˆ°æ›´æ–°æ•¸æ“š: {data}")

        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # æª¢æŸ¥ KOL æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT serial, nickname FROM kol_profiles WHERE serial = %s", (serial,))
            existing_kol = cursor.fetchone()

            if not existing_kol:
                logger.warning(f"âš ï¸ KOL serial {serial} ä¸å­˜åœ¨")
                return {
                    "success": False,
                    "error": f"KOL serial {serial} ä¸å­˜åœ¨",
                    "timestamp": get_current_time().isoformat()
                }

            # å»ºç«‹å¯æ›´æ–°çš„æ¬„ä½åˆ—è¡¨ï¼ˆæ’é™¤ä¸æ‡‰æ‰‹å‹•æ›´æ–°çš„æ¬„ä½ï¼‰
            excluded_fields = ['id', 'serial', 'created_time', 'last_updated']

            # ç‰¹åˆ¥è™•ç† JSONB æ¬„ä½
            jsonb_fields = [
                'content_types', 'content_style_probabilities',
                'analysis_depth_probabilities', 'content_length_probabilities'
            ]

            # å»ºç«‹ UPDATE SQL
            update_fields = []
            params = []

            for key, value in data.items():
                if key not in excluded_fields and value is not None:
                    import json
                    if key in jsonb_fields:
                        # JSONB æ¬„ä½éœ€è¦åºåˆ—åŒ–
                        if isinstance(value, (dict, list)):
                            update_fields.append(f"{key} = %s")
                            params.append(json.dumps(value))
                        else:
                            update_fields.append(f"{key} = %s")
                            params.append(value)
                    else:
                        update_fields.append(f"{key} = %s")
                        params.append(value)

            if not update_fields:
                return {
                    "success": False,
                    "error": "æ²’æœ‰å¯æ›´æ–°çš„æ¬„ä½",
                    "timestamp": get_current_time().isoformat()
                }

            # æ·»åŠ  last_updated
            update_fields.append("last_updated = NOW()")
            params.append(serial)

            update_sql = f"""
                UPDATE kol_profiles
                SET {', '.join(update_fields)}
                WHERE serial = %s
            """

            cursor.execute(update_sql, params)
            conn.commit()

            logger.info(f"âœ… KOL æ›´æ–°æˆåŠŸ: Serial={serial}, æ›´æ–°æ¬„ä½={list(data.keys())}")

            # è¿”å›æ›´æ–°å¾Œçš„æ•¸æ“š
            cursor.execute("SELECT * FROM kol_profiles WHERE serial = %s", (serial,))
            updated_kol = cursor.fetchone()

            return {
                "success": True,
                "message": f"KOL æ›´æ–°æˆåŠŸ (Serial: {serial})",
                "data": dict(updated_kol) if updated_kol else None,
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"æ›´æ–° KOL å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.put("/api/kol/{serial}/personalization")
async def update_kol_personalization(serial: str, request: Request):
    """
    æ›´æ–° KOL å€‹äººåŒ–è¨­å®šï¼ˆå…§å®¹é¢¨æ ¼ã€åˆ†ææ·±åº¦ã€å…§å®¹é•·åº¦çš„æ©Ÿç‡åˆ†å¸ƒï¼‰

    Parameters:
    - serial: KOL åºè™Ÿ
    - request body: {
        content_style_probabilities: dict,
        analysis_depth_probabilities: dict,
        content_length_probabilities: dict
      }
    """
    logger.info(f"æ”¶åˆ°æ›´æ–° KOL å€‹äººåŒ–è¨­å®šè«‹æ±‚: serial={serial}")

    conn = None
    try:
        data = await request.json()
        logger.info(f"æ¥æ”¶åˆ°å€‹äººåŒ–è¨­å®š: {data}")

        # æª¢æŸ¥æ•¸æ“šåº«é€£æ¥
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # æª¢æŸ¥ KOL æ˜¯å¦å­˜åœ¨
            cursor.execute("SELECT serial, nickname FROM kol_profiles WHERE serial = %s", (serial,))
            existing_kol = cursor.fetchone()

            if not existing_kol:
                logger.warning(f"âš ï¸ KOL serial {serial} ä¸å­˜åœ¨")
                return {
                    "success": False,
                    "error": f"KOL serial {serial} ä¸å­˜åœ¨",
                    "timestamp": get_current_time().isoformat()
                }

            # æå–å€‹äººåŒ–è¨­å®š
            content_style_probabilities = data.get('content_style_probabilities', {})
            analysis_depth_probabilities = data.get('analysis_depth_probabilities', {})
            content_length_probabilities = data.get('content_length_probabilities', {})

            # æ›´æ–° KOL å€‹äººåŒ–è¨­å®š
            import json
            update_sql = """
                UPDATE kol_profiles
                SET content_style_probabilities = %s,
                    analysis_depth_probabilities = %s,
                    content_length_probabilities = %s,
                    last_updated = NOW()
                WHERE serial = %s
            """
            cursor.execute(update_sql, (
                json.dumps(content_style_probabilities),
                json.dumps(analysis_depth_probabilities),
                json.dumps(content_length_probabilities),
                serial
            ))
            conn.commit()

            logger.info(f"âœ… KOL å€‹äººåŒ–è¨­å®šæ›´æ–°æˆåŠŸ: Serial={serial}, Nickname={existing_kol['nickname']}")

            return {
                "success": True,
                "message": f"KOL å€‹äººåŒ–è¨­å®šæ›´æ–°æˆåŠŸ (Serial: {serial}, Nickname: {existing_kol['nickname']})",
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"âŒ æ›´æ–° KOL å€‹äººåŒ–è¨­å®šå¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "error": f"æ›´æ–°å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


# ==================== Schedule API åŠŸèƒ½ ====================

@app.get("/api/schedule/tasks")
async def get_schedule_tasks(
    status: str = Query(None, description="ç‹€æ…‹ç¯©é¸"),
    limit: int = Query(100, description="è¿”å›çš„è¨˜éŒ„æ•¸")
):
    """ç²å–æ’ç¨‹ä»»å‹™åˆ—è¡¨"""
    logger.info(f"æ”¶åˆ° get_schedule_tasks è«‹æ±‚: status={status}, limit={limit}")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨ï¼Œè¿”å›ç©ºæ•¸æ“š")
            return {
                "success": False,
                "tasks": [],
                "count": 0,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            query = "SELECT * FROM schedule_tasks"
            params = []

            if status:
                query += " WHERE status = %s"
                params.append(status)

            query += " ORDER BY created_at DESC LIMIT %s"
            params.append(limit)

            logger.info(f"åŸ·è¡Œ SQL æŸ¥è©¢: {query} with params: {params}")
            cursor.execute(query, params)
            tasks = cursor.fetchall()

            logger.info(f"æŸ¥è©¢åˆ° {len(tasks)} å€‹æ’ç¨‹ä»»å‹™")

            conn.commit()

            # ğŸ”¥ FIX: Handle JSON fields (JSONB or TEXT types)
            import json
            parsed_tasks = []
            for task in tasks:
                task_dict = dict(task)

                # Parse JSON fields - handle both JSONB (already dict) and TEXT (needs parsing)
                json_fields = ['trigger_config', 'schedule_config', 'batch_info', 'generation_config']
                for field in json_fields:
                    field_value = task_dict.get(field)
                    if field_value:
                        # If already a dict (JSONB type from PostgreSQL), keep as is
                        if isinstance(field_value, dict):
                            continue
                        # If string (TEXT type), parse it
                        elif isinstance(field_value, str):
                            try:
                                task_dict[field] = json.loads(field_value)
                            except (json.JSONDecodeError, TypeError) as e:
                                logger.warning(f"Failed to parse {field} for task {task_dict.get('schedule_id')}: {e}")
                                task_dict[field] = None

                # ğŸ”¥ FIX: Add display-friendly fields to help frontend
                # Extract stock_sorting for easier display
                if task_dict.get('generation_config'):
                    gen_config = task_dict['generation_config']
                    # Ensure gen_config is a dict
                    if isinstance(gen_config, dict):
                        stock_sorting = gen_config.get('stock_sorting', {})
                        # Ensure stock_sorting is a dict before accessing .get()
                        if isinstance(stock_sorting, dict):
                            task_dict['stock_sorting_display'] = {
                                'method': stock_sorting.get('method', 'none'),
                                'direction': stock_sorting.get('direction', 'desc'),
                                'label': _get_sorting_label(stock_sorting)
                            }
                        else:
                            # Fallback for non-dict stock_sorting
                            task_dict['stock_sorting_display'] = {
                                'method': 'none',
                                'direction': 'desc',
                                'label': 'éš¨æ©Ÿæ’åº'
                            }

                # Ensure daily_execution_time is present (for frontend compatibility)
                if not task_dict.get('daily_execution_time') and task_dict.get('schedule_config'):
                    # Try to extract from schedule_config.posting_time_slots
                    schedule_config = task_dict['schedule_config']
                    if isinstance(schedule_config, dict):
                        posting_time_slots = schedule_config.get('posting_time_slots', [])
                        if posting_time_slots and len(posting_time_slots) > 0:
                            task_dict['daily_execution_time'] = posting_time_slots[0]

                parsed_tasks.append(task_dict)

            return {
                "success": True,
                "tasks": parsed_tasks,
                "count": len(tasks),
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢æ’ç¨‹ä»»å‹™å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "tasks": [],
            "count": 0,
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.get("/api/schedule/daily-stats")
async def get_daily_stats():
    """ç²å–æ¯æ—¥æ’ç¨‹çµ±è¨ˆ"""
    logger.info("æ”¶åˆ° get_daily_stats è«‹æ±‚")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "data": {},
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Get today's date range
            today_start = get_current_time().replace(hour=0, minute=0, second=0, microsecond=0)
            today_end = get_current_time().replace(hour=23, minute=59, second=59, microsecond=999999)

            # Count total posts generated today from schedule_tasks
            cursor.execute("""
                SELECT COALESCE(SUM(total_posts_generated), 0) as today_posts
                FROM schedule_tasks
                WHERE last_run >= %s AND last_run <= %s
            """, (today_start, today_end))
            today_posts = cursor.fetchone()['today_posts']

            # Count scheduled posts (active schedules with next_run in future)
            cursor.execute("""
                SELECT COUNT(*) as scheduled_posts
                FROM schedule_tasks
                WHERE status = 'active' AND next_run > NOW()
            """)
            scheduled_posts = cursor.fetchone()['scheduled_posts']

            # Count completed posts today (schedules that completed today)
            cursor.execute("""
                SELECT COALESCE(SUM(success_count), 0) as completed_posts
                FROM schedule_tasks
                WHERE last_run >= %s AND last_run <= %s
            """, (today_start, today_end))
            completed_posts = cursor.fetchone()['completed_posts']

            # Count failed posts today
            cursor.execute("""
                SELECT COALESCE(SUM(failure_count), 0) as failed_posts
                FROM schedule_tasks
                WHERE last_run >= %s AND last_run <= %s
            """, (today_start, today_end))
            failed_posts = cursor.fetchone()['failed_posts']

            # Count active schedules
            cursor.execute("""
                SELECT COUNT(*) as active_schedules
                FROM schedule_tasks
                WHERE status = 'active'
            """)
            active_schedules = cursor.fetchone()['active_schedules']

            # Count total schedules
            cursor.execute("SELECT COUNT(*) as total_schedules FROM schedule_tasks")
            total_schedules = cursor.fetchone()['total_schedules']

            conn.commit()

            result = {
                "success": True,
                "data": {
                    "today_posts": int(today_posts),
                    "scheduled_posts": int(scheduled_posts),
                    "completed_posts": int(completed_posts),
                    "failed_posts": int(failed_posts),
                    "active_schedules": int(active_schedules),
                    "total_schedules": int(total_schedules)
                },
                "timestamp": get_current_time().isoformat()
            }

            logger.info(f"è¿”å›æ¯æ—¥æ’ç¨‹çµ±è¨ˆæ•¸æ“š: {result['data']}")
            return result

    except Exception as e:
        logger.error(f"æŸ¥è©¢æ¯æ—¥çµ±è¨ˆå¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "data": {},
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.get("/api/schedule/scheduler/status")
async def get_scheduler_status():
    """ç²å–æ’ç¨‹å™¨ç‹€æ…‹"""
    logger.info("æ”¶åˆ° get_scheduler_status è«‹æ±‚")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥æ± ä¸å¯ç”¨")
            return {
                "success": False,
                "data": {},
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        # Get connection from pool
        conn = get_db_connection()

        # Reset connection if in failed state
        conn.rollback()

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Count active tasks (status='active')
            cursor.execute("""
                SELECT COUNT(*) as active_tasks
                FROM schedule_tasks
                WHERE status = 'active'
            """)
            active_tasks = cursor.fetchone()['active_tasks']

            # Count pending tasks (status='pending' or next_run is in the future)
            cursor.execute("""
                SELECT COUNT(*) as pending_tasks
                FROM schedule_tasks
                WHERE status = 'pending' OR (status = 'active' AND next_run > NOW())
            """)
            pending_tasks = cursor.fetchone()['pending_tasks']

            # Get next run time (earliest next_run from active schedules)
            cursor.execute("""
                SELECT MIN(next_run) as next_run
                FROM schedule_tasks
                WHERE status = 'active' AND next_run IS NOT NULL
            """)
            next_run_row = cursor.fetchone()
            next_run = next_run_row['next_run'].isoformat() if next_run_row['next_run'] else None

            # Get last run time (most recent last_run)
            cursor.execute("""
                SELECT MAX(last_run) as last_run
                FROM schedule_tasks
                WHERE last_run IS NOT NULL
            """)
            last_run_row = cursor.fetchone()
            last_run = last_run_row['last_run'].isoformat() if last_run_row['last_run'] else None

            # Calculate uptime from earliest started_at
            cursor.execute("""
                SELECT MIN(started_at) as earliest_start
                FROM schedule_tasks
                WHERE started_at IS NOT NULL
            """)
            earliest_start_row = cursor.fetchone()
            uptime = "N/A"
            if earliest_start_row['earliest_start']:
                # Ensure both datetimes are timezone-aware to avoid subtraction error
                earliest_start = earliest_start_row['earliest_start']
                if earliest_start.tzinfo is None:
                    # Database returns naive datetime, assume it's UTC and convert to Taipei
                    tz = pytz.timezone('Asia/Taipei')
                    earliest_start = pytz.utc.localize(earliest_start).astimezone(tz)

                uptime_delta = get_current_time() - earliest_start
                days = uptime_delta.days
                hours = uptime_delta.seconds // 3600
                uptime = f"{days} days, {hours} hours"

            # Determine overall status based on active tasks
            status = "running" if active_tasks > 0 else "idle"
            scheduler_running = active_tasks > 0  # ğŸ”¥ FIX: Add boolean field for frontend

            result = {
                "success": True,
                "data": {
                    "status": status,
                    "scheduler_running": scheduler_running,  # ğŸ”¥ FIX: Frontend expects this boolean field
                    "active_tasks": int(active_tasks),
                    "pending_tasks": int(pending_tasks),
                    "next_run": next_run,
                    "last_run": last_run,
                    "uptime": uptime
                },
                "timestamp": get_current_time().isoformat()
            }

            conn.commit()  # Commit transaction
            logger.info(f"è¿”å›æ’ç¨‹å™¨ç‹€æ…‹æ•¸æ“š: {result['data']}")
            return result

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"æŸ¥è©¢æ’ç¨‹å™¨ç‹€æ…‹å¤±æ•—: {e}")
        return {
            "success": False,
            "data": {},
            "error": str(e),
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/schedule/scheduler/start")
async def start_scheduler():
    """å•Ÿå‹•å…¨å±€æ’ç¨‹å™¨ - å°‡æ‰€æœ‰ paused ç‹€æ…‹çš„ä»»å‹™è¨­ç½®ç‚º active"""
    logger.info("æ”¶åˆ° start_scheduler è«‹æ±‚ - å•Ÿå‹•å…¨å±€æ’ç¨‹å™¨")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "message": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Update all paused tasks to active
            cursor.execute("""
                UPDATE schedule_tasks
                SET status = 'active', updated_at = NOW()
                WHERE status = 'paused'
                RETURNING schedule_id, schedule_name, schedule_type
            """)
            activated_tasks = cursor.fetchall()

            conn.commit()

            logger.info(f"å…¨å±€æ’ç¨‹å™¨å·²å•Ÿå‹•ï¼Œæ¿€æ´»äº† {len(activated_tasks)} å€‹ä»»å‹™")

            return {
                "success": True,
                "message": f"å…¨å±€æ’ç¨‹å™¨å·²å•Ÿå‹•ï¼Œæ¿€æ´»äº† {len(activated_tasks)} å€‹ä»»å‹™",
                "activated_count": len(activated_tasks),
                "activated_tasks": [dict(task) for task in activated_tasks],
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"å•Ÿå‹•å…¨å±€æ’ç¨‹å™¨å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "message": f"å•Ÿå‹•å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/schedule/scheduler/stop")
async def stop_scheduler():
    """åœæ­¢å…¨å±€æ’ç¨‹å™¨ - å°‡æ‰€æœ‰ active ç‹€æ…‹çš„ä»»å‹™è¨­ç½®ç‚º paused"""
    logger.info("æ”¶åˆ° stop_scheduler è«‹æ±‚ - åœæ­¢å…¨å±€æ’ç¨‹å™¨")

    conn = None
    try:
        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "message": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Update all active tasks to paused
            cursor.execute("""
                UPDATE schedule_tasks
                SET status = 'paused', updated_at = NOW()
                WHERE status = 'active'
                RETURNING schedule_id, schedule_name, schedule_type
            """)
            paused_tasks = cursor.fetchall()

            conn.commit()

            logger.info(f"å…¨å±€æ’ç¨‹å™¨å·²åœæ­¢ï¼Œæš«åœäº† {len(paused_tasks)} å€‹ä»»å‹™")

            return {
                "success": True,
                "message": f"å…¨å±€æ’ç¨‹å™¨å·²åœæ­¢ï¼Œæš«åœäº† {len(paused_tasks)} å€‹ä»»å‹™",
                "paused_count": len(paused_tasks),
                "paused_tasks": [dict(task) for task in paused_tasks],
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"åœæ­¢å…¨å±€æ’ç¨‹å™¨å¤±æ•—: {e}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "message": f"åœæ­¢å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/schedule/create")
async def create_schedule(request: Request):
    """å‰µå»ºæ–°çš„æ’ç¨‹ä»»å‹™"""
    logger.info("æ”¶åˆ° create_schedule è«‹æ±‚")

    conn = None
    try:
        data = await request.json()
        logger.info(f"æ¥æ”¶åˆ°æ’ç¨‹é…ç½®: {data}")

        if not db_pool:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨")
            return {
                "success": False,
                "message": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨",
                "timestamp": get_current_time().isoformat()
            }

        conn = get_db_connection()
        conn.rollback()  # Clear any failed transactions

        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # ç”Ÿæˆå”¯ä¸€çš„ task_id (UUID)
            import uuid
            task_id = str(uuid.uuid4())

            # å¾è«‹æ±‚ä¸­æå–å­—æ®µ
            schedule_name = data.get('schedule_name', 'Unnamed Schedule')
            schedule_description = data.get('schedule_description', '')
            schedule_type = data.get('schedule_type', 'weekday_daily')
            interval_seconds = data.get('interval_seconds', 300)
            enabled = data.get('enabled', True)
            timezone = data.get('timezone', 'Asia/Taipei')
            weekdays_only = data.get('weekdays_only', True)
            auto_posting = data.get('auto_posting', False)
            max_posts_per_hour = data.get('max_posts_per_hour', 2)

            # ç”Ÿæˆé…ç½® (generation_config)
            generation_config = data.get('generation_config', {})

            # ğŸ”¥ FIX: Extract correct parameters from full_triggers_config
            schedule_config_data = data.get('schedule_config', {})
            full_triggers_config = schedule_config_data.get('full_triggers_config', {})

            # Map stockCountLimit â†’ max_stocks
            if 'stockCountLimit' in full_triggers_config:
                generation_config['max_stocks'] = full_triggers_config['stockCountLimit']
                logger.info(f"ğŸ”§ Mapped stockCountLimit={full_triggers_config['stockCountLimit']} â†’ max_stocks")

            # Map stockFilterCriteria â†’ stock_sorting
            if 'stockFilterCriteria' in full_triggers_config and full_triggers_config['stockFilterCriteria']:
                criteria_list = full_triggers_config['stockFilterCriteria']
                if isinstance(criteria_list, list) and len(criteria_list) > 0:
                    # Map the first criteria to stock_sorting
                    criteria_map = {
                        'five_day_gain': 'five_day_change_desc',
                        'five_day_loss': 'five_day_loss_desc',
                        'daily_gain': 'daily_change_desc',
                        'daily_loss': 'daily_change_asc',
                        'volume_high': 'volume_desc',
                        'volume_low': 'volume_asc'
                    }
                    first_criteria = criteria_list[0]
                    stock_sorting = criteria_map.get(first_criteria, first_criteria)
                    generation_config['stock_sorting'] = stock_sorting
                    logger.info(f"ğŸ”§ Mapped stockFilterCriteria={first_criteria} â†’ stock_sorting={stock_sorting}")

            # Extract daily_execution_time first (needed for next_run calculation later)
            daily_execution_time = data.get('daily_execution_time')

            # ğŸ”¥ DEBUG: Log what frontend sent
            logger.info(f"ğŸ” Frontend sent trigger_config type: {type(data.get('trigger_config'))}")
            logger.info(f"ğŸ” Frontend sent trigger_config: {data.get('trigger_config')}")
            logger.info(f"ğŸ” Frontend sent schedule_config type: {type(data.get('schedule_config'))}")
            logger.info(f"ğŸ” Frontend sent schedule_config: {str(data.get('schedule_config'))[:500]}")

            # ğŸ”¥ FIX: Use trigger_config from frontend if provided AND not empty
            trigger_config = data.get('trigger_config')
            # Check if trigger_config is None or empty dict (both should use fallback)
            if not trigger_config or (isinstance(trigger_config, dict) and len(trigger_config) == 0):
                # Fallback: Build from generation_config for backward compatibility
                trigger_type = generation_config.get('trigger_type', 'limit_up_after_hours')
                stock_sorting = generation_config.get('stock_sorting', {})
                kol_assignment = generation_config.get('kol_assignment', 'random')
                max_stocks = generation_config.get('max_stocks', 5)

                trigger_config = {
                    "trigger_type": trigger_type,
                    "stock_codes": [],  # å°‡ç”±æ’ç¨‹å™¨åŸ·è¡Œæ™‚æ ¹æ“šè§¸ç™¼å™¨å‹•æ…‹ç²å–
                    "kol_assignment": kol_assignment,
                    "max_stocks": max_stocks,
                    "stock_sorting": stock_sorting
                }
            else:
                # ğŸ”¥ FIX: Ensure trigger_config uses the correct max_stocks from stockCountLimit
                if 'stockCountLimit' in full_triggers_config:
                    trigger_config['max_stocks'] = full_triggers_config['stockCountLimit']
                    logger.info(f"ğŸ”§ Updated trigger_config.max_stocks={full_triggers_config['stockCountLimit']}")

            # ğŸ”¥ FIX: Use schedule_config from frontend if provided, otherwise build from data
            schedule_config = data.get('schedule_config')
            if not schedule_config:
                # Fallback: Build from daily_execution_time for backward compatibility
                posting_time_slots = [daily_execution_time] if daily_execution_time else []

                schedule_config = {
                    "enabled": enabled,
                    "posting_time_slots": posting_time_slots,
                    "timezone": timezone,
                    "weekdays_only": weekdays_only
                }

            # Log what we're storing
            logger.info(f"ğŸ“ Storing trigger_config: {str(trigger_config)[:200]}...")
            logger.info(f"ğŸ“ Storing schedule_config: {str(schedule_config)[:200]}...")

            # æ‰¹æ¬¡ä¿¡æ¯ (batch_info)
            batch_info = data.get('batch_info', {})
            session_id = data.get('session_id')
            if session_id:
                batch_info['session_id'] = str(session_id)

            # è¨ˆç®—ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“ (next_run)
            from datetime import datetime, timedelta
            import pytz

            now = datetime.now(pytz.timezone(timezone))
            next_run = None

            if daily_execution_time and enabled:
                # è§£ææ™‚é–“ (HH:mm æ ¼å¼)
                time_parts = daily_execution_time.split(':')
                if len(time_parts) == 2:
                    hour = int(time_parts[0])
                    minute = int(time_parts[1])

                    # å‰µå»ºä»Šå¤©çš„åŸ·è¡Œæ™‚é–“
                    next_run = now.replace(hour=hour, minute=minute, second=0, microsecond=0)

                    # å¦‚æœä»Šå¤©çš„æ™‚é–“å·²éï¼Œè¨­ç½®ç‚ºæ˜å¤©
                    if next_run <= now:
                        next_run = next_run + timedelta(days=1)

                    # å¦‚æœæ˜¯å·¥ä½œæ—¥æ¨¡å¼ï¼Œè·³éé€±æœ«
                    if weekdays_only:
                        while next_run.weekday() >= 5:  # 5=Saturday, 6=Sunday
                            next_run = next_run + timedelta(days=1)

            # æ’å…¥æ’ç¨‹ä»»å‹™åˆ°è³‡æ–™åº«
            insert_sql = """
                INSERT INTO schedule_tasks (
                    schedule_id, schedule_name, schedule_description, status, schedule_type,
                    interval_seconds, auto_posting, max_posts_per_hour,
                    timezone, weekdays_only, daily_execution_time, batch_info, generation_config,
                    trigger_config, schedule_config,
                    next_run, created_at, updated_at,
                    run_count, success_count, failure_count
                ) VALUES (
                    %s, %s, %s, %s, %s,
                    %s, %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s,
                    %s, NOW(), NOW(),
                    0, 0, 0
                )
                RETURNING schedule_id, schedule_name, status, next_run, created_at
            """

            status = 'active' if enabled else 'paused'

            import json
            cursor.execute(insert_sql, (
                task_id,  # This maps to schedule_id column
                schedule_name,
                schedule_description,
                status,
                schedule_type,
                interval_seconds,
                auto_posting,
                max_posts_per_hour,
                timezone,
                weekdays_only,
                daily_execution_time,  # ğŸ”¥ FIX: Add daily_execution_time
                json.dumps(batch_info),
                json.dumps(generation_config),
                json.dumps(trigger_config),  # ğŸ”¥ FIX: Add trigger_config
                json.dumps(schedule_config),  # ğŸ”¥ FIX: Add schedule_config
                next_run
            ))

            result = cursor.fetchone()
            conn.commit()

            logger.info(f"æ’ç¨‹å‰µå»ºæˆåŠŸ: schedule_id={task_id}, name={schedule_name}")

            return {
                "success": True,
                "message": "æ’ç¨‹å‰µå»ºæˆåŠŸ",
                "task_id": result['schedule_id'],  # Return as task_id for backwards compatibility
                "task_name": result['schedule_name'],
                "status": result['status'],
                "next_run": result['next_run'].isoformat() if result['next_run'] else None,
                "created_at": result['created_at'].isoformat(),
                "timestamp": get_current_time().isoformat()
            }

    except Exception as e:
        logger.error(f"å‰µå»ºæ’ç¨‹å¤±æ•—: {e}")
        logger.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        if conn:
            conn.rollback()
        return {
            "success": False,
            "message": f"å‰µå»ºå¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)

@app.post("/api/schedule/{task_id}/auto-posting")
async def toggle_auto_posting(task_id: str, request: Request):
    """åˆ‡æ›æ’ç¨‹çš„è‡ªå‹•ç™¼æ–‡åŠŸèƒ½"""
    logger.info(f"æ”¶åˆ° toggle_auto_posting è«‹æ±‚ - Task ID: {task_id}")

    conn = None
    try:
        body = await request.json()
        enabled = body.get('enabled', False)

        logger.info(f"è¨­å®šè‡ªå‹•ç™¼æ–‡: {enabled}")

        conn = get_db_connection()
        with conn.cursor() as cursor:
            cursor.execute("""
                UPDATE schedule_tasks
                SET auto_posting = %s, updated_at = NOW()
                WHERE schedule_id = %s
            """, (enabled, task_id))

            conn.commit()

            if cursor.rowcount == 0:
                raise ValueError(f"Schedule {task_id} not found")

            logger.info(f"âœ… è‡ªå‹•ç™¼æ–‡è¨­å®šæ›´æ–°æˆåŠŸ - Task ID: {task_id}, enabled={enabled}")

        return {
            "success": True,
            "task_id": task_id,
            "message": f"è‡ªå‹•ç™¼æ–‡å·²{'é–‹å•Ÿ' if enabled else 'é—œé–‰'}",
            "enabled": enabled
        }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ æ›´æ–°è‡ªå‹•ç™¼æ–‡è¨­å®šå¤±æ•—: {e}")
        return {
            "success": False,
            "message": f"æ›´æ–°å¤±æ•—: {str(e)}",
            "timestamp": get_current_time().isoformat()
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/schedule/execute/{task_id}")
async def execute_schedule_now(task_id: str, request: Request):
    """
    ç«‹å³åŸ·è¡Œæ’ç¨‹ (æ‰‹å‹•è§¸ç™¼)
    Execute a schedule immediately without waiting for scheduled time
    """
    logger.info(f"æ”¶åˆ°ç«‹å³åŸ·è¡Œæ’ç¨‹è«‹æ±‚ - Task ID: {task_id}")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        # Get schedule details from database
        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            cursor.execute("""
                SELECT *
                FROM schedule_tasks
                WHERE schedule_id = %s
            """, (task_id,))

            schedule = cursor.fetchone()

            if not schedule:
                return {
                    "success": False,
                    "error": f"æ‰¾ä¸åˆ°æ’ç¨‹: {task_id}"
                }

        logger.info(f"ğŸ“‹ æ’ç¨‹è³‡è¨Š: {schedule['schedule_name']}")

        # Extract configuration
        trigger_config = schedule.get('trigger_config', {})
        schedule_config = schedule.get('schedule_config', {})
        generation_config = schedule.get('generation_config', {})

        # Parse JSON fields if they're strings
        if isinstance(trigger_config, str):
            trigger_config = json.loads(trigger_config)
        if isinstance(schedule_config, str):
            schedule_config = json.loads(schedule_config)
        if isinstance(generation_config, str):
            generation_config = json.loads(generation_config)

        logger.info(f"ğŸ” trigger_config: {json.dumps(trigger_config, ensure_ascii=False)[:200]}")
        logger.info(f"ğŸ” schedule_config: {json.dumps(schedule_config, ensure_ascii=False)[:200]}")

        # ğŸ”¥ FIX: Prioritize values from full_triggers_config if available
        full_triggers_config = schedule_config.get('full_triggers_config', {})

        # Extract KOL assignment and max stocks
        kol_assignment = trigger_config.get('kol_assignment', 'random')

        # Use stockCountLimit from full_triggers_config if available, otherwise fallback to trigger_config
        if 'stockCountLimit' in full_triggers_config:
            max_stocks = full_triggers_config['stockCountLimit']
            logger.info(f"ğŸ”§ Using max_stocks={max_stocks} from full_triggers_config.stockCountLimit")
        else:
            max_stocks = trigger_config.get('max_stocks', 5)
            logger.info(f"ğŸ”§ Using max_stocks={max_stocks} from trigger_config (fallback)")

        # Extract stock_sorting criteria from full_triggers_config if available
        stock_filter_criteria = full_triggers_config.get('stockFilterCriteria', [])
        if stock_filter_criteria:
            logger.info(f"ğŸ”§ Using stockFilterCriteria={stock_filter_criteria} from full_triggers_config")

        # ğŸ”¥ FIX: Support both old format (stock_codes) and new format (triggerKey)
        stock_codes = trigger_config.get('stock_codes', [])
        trigger_key = trigger_config.get('triggerKey') or trigger_config.get('trigger_type')

        # If no pre-configured stock codes, execute trigger to get stocks
        if not stock_codes and trigger_key:
            logger.info(f"ğŸ¯ åŸ·è¡Œè§¸ç™¼å™¨: {trigger_key}")

            # Get threshold and filters from trigger_config
            threshold = trigger_config.get('threshold', 20)
            filters = trigger_config.get('filters', {})

            # ğŸ”¥ FIX: Map stock_filter_criteria to sortBy parameter
            sortBy = None
            if stock_filter_criteria and len(stock_filter_criteria) > 0:
                criteria_to_sortBy = {
                    'five_day_gain': 'five_day_gain',
                    'five_day_loss': 'five_day_loss',
                    'daily_gain': 'change_percent_desc',
                    'daily_loss': 'change_percent_asc',
                    'volume_high': 'volume_desc',
                    'volume_low': 'volume_asc'
                }
                sortBy = criteria_to_sortBy.get(stock_filter_criteria[0])
                logger.info(f"ğŸ”§ Mapped stockFilterCriteria[0]={stock_filter_criteria[0]} â†’ sortBy={sortBy}")

            # Execute trigger based on type
            if trigger_key == 'limit_up_after_hours':
                trigger_result = await get_after_hours_limit_up_stocks(
                    limit=max_stocks * 2,  # Fetch more than needed for filtering
                    changeThreshold=9.5,
                    industries="",
                    sortBy=sortBy
                )
                if 'stocks' in trigger_result:
                    stock_codes = [stock['stock_code'] for stock in trigger_result['stocks']]
                    logger.info(f"âœ… è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨ (sortBy={sortBy})")
            elif trigger_key == 'limit_down_after_hours':
                trigger_result = await get_after_hours_limit_down_stocks(
                    limit=max_stocks * 2,  # Fetch more than needed for filtering
                    changeThreshold=9.5,
                    industries="",
                    sortBy=sortBy
                )
                if 'stocks' in trigger_result:
                    stock_codes = [stock['stock_code'] for stock in trigger_result['stocks']]
                    logger.info(f"âœ… è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨ (sortBy={sortBy})")
            elif trigger_key == 'intraday_gainers_by_amount':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­æ¼²å¹…æ’åº+æˆäº¤é¡è§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_gainers_by_amount(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        # Extract stock_code from dict objects
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­è§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'intraday_volume_leaders':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­æˆäº¤é‡æ’åºè§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_volume_leaders(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­æˆäº¤é‡è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­æˆäº¤é‡è§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'intraday_amount_leaders':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­æˆäº¤é¡æ’åºè§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_amount_leaders(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­æˆäº¤é¡è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­æˆäº¤é¡è§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'intraday_limit_down':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­è·Œåœç¯©é¸è§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_limit_down(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­è·Œåœè§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­è·Œåœè§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'intraday_limit_up':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­æ¼²åœç¯©é¸è§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_limit_up(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­æ¼²åœè§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­æ¼²åœè§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'intraday_limit_down_by_amount':
                logger.info("ğŸ“¡ åŸ·è¡Œç›¤ä¸­è·Œåœç¯©é¸+æˆäº¤é¡è§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_intraday_limit_down_by_amount(limit=max_stocks)
                    if 'stocks' in trigger_result:
                        stocks = trigger_result['stocks']
                        stock_codes = [s['stock_code'] if isinstance(s, dict) else s for s in stocks]
                        logger.info(f"âœ… ç›¤ä¸­è·Œåœ+æˆäº¤é¡è§¸ç™¼å™¨è¿”å› {len(stock_codes)} æª”è‚¡ç¥¨")
                except Exception as e:
                    logger.error(f"âŒ ç›¤ä¸­è·Œåœ+æˆäº¤é¡è§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            elif trigger_key == 'trending_topics':
                logger.info("ğŸ“¡ åŸ·è¡Œç†±é–€è©±é¡Œè§¸ç™¼å™¨...")
                try:
                    trigger_result = await get_trending_topics(limit=max_stocks)
                    if 'topics' in trigger_result:
                        topics = trigger_result['topics']

                        # ğŸ”¥ FIX: Store topics for later use (don't just extract stock_codes)
                        trending_topics_data = topics[:max_stocks]

                        # Extract stock codes from all topics
                        stock_codes = []
                        for topic in trending_topics_data:
                            if 'stock_ids' in topic and topic['stock_ids']:
                                # Topic has related stocks - extract them
                                stock_codes.extend(topic['stock_ids'])

                        logger.info(f"âœ… ç†±é–€è©±é¡Œè§¸ç™¼å™¨: {len(trending_topics_data)} å€‹è©±é¡Œ, æå– {len(stock_codes)} æª”ç›¸é—œè‚¡ç¥¨")

                        # ğŸ”¥ Store trending_topics_data for use in post generation
                        # This will be used to create posts for both:
                        # 1. Topic + Stock combinations
                        # 2. Pure topic posts (when topic has no stocks)
                        trigger_config['trending_topics_data'] = trending_topics_data
                except Exception as e:
                    logger.error(f"âŒ ç†±é–€è©±é¡Œè§¸ç™¼å™¨å¤±æ•—: {e}")
                    import traceback
                    logger.error(traceback.format_exc())
            else:
                logger.warning(f"âš ï¸ æœªæ”¯æŒçš„è§¸ç™¼å™¨é¡å‹: {trigger_key}")

        # ğŸ”¥ FIX: Allow trending_topics to run without stock_codes (pure topic mode)
        trending_topics_data = trigger_config.get('trending_topics_data', [])
        is_trending_topics_trigger = trigger_key == 'trending_topics'

        if not stock_codes and not trending_topics_data:
            return {
                "success": False,
                "error": "ç„¡æ³•ç²å–è‚¡ç¥¨åˆ—è¡¨ï¼šæ’ç¨‹æœªé…ç½®è‚¡ç¥¨ä¸”è§¸ç™¼å™¨æœªè¿”å›çµæœ"
            }

        # Apply max_stocks limit
        if stock_codes:
            stock_codes = stock_codes[:max_stocks]
            logger.info(f"ğŸ“Š æœ€çµ‚é¸å®š {len(stock_codes)} æª”è‚¡ç¥¨: {stock_codes}")

        # ğŸ”¥ NEW: Handle pure trending topics (no stocks)
        if is_trending_topics_trigger and trending_topics_data:
            pure_topics = [topic for topic in trending_topics_data if not topic.get('stock_ids')]
            if pure_topics:
                logger.info(f"ğŸ“° ç™¼ç¾ {len(pure_topics)} å€‹ç´”è©±é¡Œï¼ˆç„¡è‚¡ç¥¨ï¼‰: {[t['title'] for t in pure_topics]}")

        # Generate unique session ID for this execution
        import time
        session_id = int(time.time() * 1000)  # Milliseconds timestamp

        logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œæ’ç¨‹: session_id={session_id}, stocks={stock_codes}, kol_assignment={kol_assignment}")

        # TODO: Implement stock selection logic based on trigger_type
        # For now, use custom_stocks mode with provided stock codes

        #  Generate posts for each stock
        generated_posts = []
        failed_posts = []

        # ğŸ”¥ FIX: Get KOL list from database (not hardcoded!)
        # Support different assignment modes:
        # - 'random': Use all active KOLs
        # - 'pool_random': Use selected_kols pool (user-defined)
        # - 'fixed': Use selected_kols in order
        kol_serials = []

        # Check if schedule has selected_kols (pool_random or fixed mode)
        selected_kols = schedule.get('selected_kols', [])

        if kol_assignment == 'pool_random' or kol_assignment == 'fixed':
            # Use user-selected KOL pool
            if selected_kols and len(selected_kols) > 0:
                kol_serials = selected_kols
                logger.info(f"âœ… Using selected KOL pool ({kol_assignment} mode): {kol_serials}")
            else:
                return {
                    "success": False,
                    "error": f"{kol_assignment} mode requires selected_kols to be configured"
                }
        else:
            # random mode: fetch all active KOLs from database
            try:
                database_url = os.getenv("DATABASE_URL")
                kol_conn = await asyncpg.connect(database_url)
                kol_rows = await kol_conn.fetch("""
                    SELECT serial
                    FROM kol_profiles
                    WHERE status = 'active'
                    ORDER BY serial
                """)
                kol_serials = [row['serial'] for row in kol_rows]
                await kol_conn.close()
                logger.info(f"âœ… Fetched {len(kol_serials)} active KOLs from database (random mode): {kol_serials}")
            except Exception as e:
                logger.error(f"âŒ Failed to fetch KOLs from database: {e}")
                # Fallback: use schedule's selected KOLs if available
                if selected_kols:
                    kol_serials = selected_kols
                    logger.warning(f"âš ï¸ Using schedule's selected KOLs as fallback: {kol_serials}")
                else:
                    return {
                        "success": False,
                        "error": "No KOLs available. Please configure KOLs in the system."
                    }

        for stock_code in stock_codes:
            # Select KOL based on assignment strategy
            if kol_assignment == 'random' or kol_assignment == 'pool_random':
                import random
                kol_serial = random.choice(kol_serials)
                logger.info(f"ğŸ² Random KOL selected from {kol_assignment} pool: {kol_serial}")
            else:
                # fixed or dynamic mode
                kol_serial = kol_serials[0]  # Use first KOL for fixed assignment
                logger.info(f"ğŸ“Œ Fixed KOL selected: {kol_serial}")

            try:
                # ğŸ”¥ FIX: Get actual stock name from stock_mapping (extract company_name from dict)
                stock_info = stock_mapping.get(stock_code, {})
                stock_name = stock_info.get('company_name', stock_code) if isinstance(stock_info, dict) else stock_code
                logger.info(f"ğŸ“Š Stock: {stock_code} â†’ {stock_name}")

                # ğŸ”¥ FIX: Map content_style to kol_persona
                content_style = generation_config.get('content_style', 'chart_analysis')
                kol_persona_mapping = {
                    'chart_analysis': 'technical',
                    'technical_analysis': 'technical',
                    'fundamental_analysis': 'fundamental',
                    'macro_analysis': 'fundamental',
                    'news_analysis': 'news_driven',
                    'mixed_analysis': 'mixed'
                }
                kol_persona = generation_config.get('kol_persona') or kol_persona_mapping.get(content_style, 'technical')

                logger.info(f"ğŸ” Content style: {content_style} â†’ KOL persona: {kol_persona}")

                # ğŸ”¥ NEW: Find matching topic for this stock (if trending_topics trigger)
                matched_topic = None
                if is_trending_topics_trigger and trending_topics_data:
                    for topic in trending_topics_data:
                        if stock_code in topic.get('stock_ids', []):
                            matched_topic = topic
                            logger.info(f"ğŸ”— è‚¡ç¥¨ {stock_code} åŒ¹é…åˆ°è©±é¡Œ: {topic.get('title')}")
                            break

                # Call manual_posting logic internally
                # Build request body
                post_body = {
                    "stock_code": stock_code,
                    "stock_name": stock_name,  # ğŸ”¥ FIX: Use actual stock name
                    "kol_serial": kol_serial,
                    "kol_persona": kol_persona,  # ğŸ”¥ FIX: Use mapped kol_persona
                    "session_id": session_id,
                    "trigger_type": trigger_key or 'custom_stocks',  # ğŸ”¥ FIX: Use actual trigger_key that was executed
                    "generation_mode": "scheduled",  # ğŸ”¥ NEW: Mark as scheduled generation
                    "posting_type": generation_config.get('posting_type', 'analysis'),
                    "max_words": generation_config.get('max_words', 200),
                    # ğŸ”¥ FIX: Pass news_config from generation_config (user's batch settings)
                    "news_config": generation_config.get('news_config', {}),
                    # ğŸ”¥ FIX: Pass model override settings from generation_config
                    "model_id_override": generation_config.get('model_id_override'),
                    "use_kol_default_model": generation_config.get('use_kol_default_model', True),
                    # ğŸ”¥ NEW: Pass topic info if matched
                    "has_trending_topic": matched_topic is not None,
                    "topic_id": matched_topic.get('id') if matched_topic else None,
                    "topic_title": matched_topic.get('title') if matched_topic else None,
                    "topic_content": matched_topic.get('content') if matched_topic else None,
                    "full_triggers_config": {
                        "trigger_type": trigger_key,  # ğŸ”¥ FIX: Use actual trigger_key
                        "stock_codes": stock_codes,
                        "kol_assignment": kol_assignment,
                        "max_stocks": max_stocks
                    }
                }

                # Create a Request object from the body
                from starlette.requests import Request
                from starlette.datastructures import Headers

                # Use the existing manual_posting function
                # We'll call it directly by reconstructing the request
                scope = {
                    'type': 'http',
                    'method': 'POST',
                    'headers': [(b'content-type', b'application/json')],
                }

                # Create mock request
                import io
                body_bytes = json.dumps(post_body).encode('utf-8')

                async def receive():
                    return {'type': 'http.request', 'body': body_bytes}

                async def send(message):
                    pass

                mock_request = Request(scope, receive, send)
                mock_request._body = body_bytes

                # Call manual_posting
                result = await manual_posting(mock_request)

                # ğŸ”¥ FIX: Handle JSONResponse objects (FastAPI wraps dict returns)
                from fastapi.responses import JSONResponse
                if isinstance(result, JSONResponse):
                    # Extract the content from JSONResponse
                    result = json.loads(result.body.decode('utf-8'))
                    logger.info(f"ğŸ”§ Extracted dict from JSONResponse: success={result.get('success')}")

                if isinstance(result, dict) and result.get('success'):
                    generated_posts.append({
                        "post_id": result.get('post_id'),
                        "stock_code": stock_code,
                        "kol_serial": kol_serial,
                        "title": result.get('content', {}).get('title', ''),
                        "content": result.get('content', {}).get('content', '')
                    })
                    logger.info(f"âœ… ç”ŸæˆæˆåŠŸ: {stock_code} - KOL {kol_serial}")
                else:
                    failed_posts.append({
                        "stock_code": stock_code,
                        "error": result.get('message', 'Unknown error')
                    })
                    logger.error(f"âŒ ç”Ÿæˆå¤±æ•—: {stock_code}")

            except Exception as e:
                logger.error(f"âŒ ç”Ÿæˆè²¼æ–‡å¤±æ•—: {stock_code}, error: {e}")
                failed_posts.append({
                    "stock_code": stock_code,
                    "error": str(e)
                })

        # ğŸ”¥ NEW: Generate posts for trending topics (both with and without stocks)
        if is_trending_topics_trigger and trending_topics_data:
            logger.info(f"ğŸ“° é–‹å§‹è™•ç†ç†±é–€è©±é¡Œè²¼æ–‡ç”Ÿæˆ...")

            for topic in trending_topics_data:
                topic_id = topic.get('id')
                topic_title = topic.get('title')
                topic_stock_ids = topic.get('stock_ids', [])

                # Scenario 1: Topic with stocks - already handled above
                # We just need to tag those posts with topic info

                # Scenario 2: Pure topic (no stocks) - generate one post
                if not topic_stock_ids:
                    logger.info(f"ğŸ“° ç”Ÿæˆç´”è©±é¡Œè²¼æ–‡: {topic_title}")

                    # Select KOL based on assignment mode
                    if kol_assignment == 'random' or kol_assignment == 'pool_random':
                        import random
                        kol_serial = random.choice(kol_serials)
                        logger.info(f"ğŸ² Random KOL selected for topic ({kol_assignment}): {kol_serial}")
                    else:
                        kol_serial = kol_serials[0]
                        logger.info(f"ğŸ“Œ Fixed KOL selected for topic: {kol_serial}")

                    try:
                        # Build request body for pure topic post
                        post_body = {
                            "stock_code": None,  # ğŸ”¥ No stock code for pure topic
                            "stock_name": None,
                            "kol_serial": kol_serial,
                            "kol_persona": generation_config.get('kol_persona', 'news_driven'),
                            "session_id": session_id,
                            "trigger_type": 'trending_topics',
                            "generation_mode": "scheduled",
                            "posting_type": generation_config.get('posting_type', 'analysis'),
                            "max_words": generation_config.get('max_words', 200),
                            "news_config": generation_config.get('news_config', {}),
                            "model_id_override": generation_config.get('model_id_override'),
                            "use_kol_default_model": generation_config.get('use_kol_default_model', True),
                            # ğŸ”¥ NEW: Pass topic info
                            "topic_id": topic_id,
                            "topic_title": topic_title,
                            "topic_content": topic.get('content', ''),
                            "has_trending_topic": True,
                            "full_triggers_config": {
                                "trigger_type": 'trending_topics',
                                "kol_assignment": kol_assignment,
                                "max_stocks": max_stocks
                            }
                        }

                        # Create mock request
                        from starlette.requests import Request
                        import io
                        body_bytes = json.dumps(post_body).encode('utf-8')

                        async def receive():
                            return {'type': 'http.request', 'body': body_bytes}

                        async def send(message):
                            pass

                        scope = {
                            'type': 'http',
                            'method': 'POST',
                            'headers': [(b'content-type', b'application/json')],
                        }

                        mock_request = Request(scope, receive, send)
                        mock_request._body = body_bytes

                        # Call manual_posting
                        result = await manual_posting(mock_request)

                        # Handle JSONResponse
                        from fastapi.responses import JSONResponse
                        if isinstance(result, JSONResponse):
                            result = json.loads(result.body.decode('utf-8'))

                        if isinstance(result, dict) and result.get('success'):
                            generated_posts.append({
                                "post_id": result.get('post_id'),
                                "stock_code": None,
                                "topic_id": topic_id,
                                "topic_title": topic_title,
                                "kol_serial": kol_serial,
                                "title": result.get('content', {}).get('title', ''),
                                "content": result.get('content', {}).get('content', '')
                            })
                            logger.info(f"âœ… ç´”è©±é¡Œè²¼æ–‡ç”ŸæˆæˆåŠŸ: {topic_title}")
                        else:
                            failed_posts.append({
                                "topic_title": topic_title,
                                "error": result.get('message', 'Unknown error')
                            })
                            logger.error(f"âŒ ç´”è©±é¡Œè²¼æ–‡ç”Ÿæˆå¤±æ•—: {topic_title}")

                    except Exception as e:
                        logger.error(f"âŒ ç”Ÿæˆç´”è©±é¡Œè²¼æ–‡å¤±æ•—: {topic_title}, error: {e}")
                        failed_posts.append({
                            "topic_title": topic_title,
                            "error": str(e)
                        })

        logger.info(f"ğŸ“Š æ’ç¨‹åŸ·è¡Œå®Œæˆ: æˆåŠŸ={len(generated_posts)}, å¤±æ•—={len(failed_posts)}")

        return {
            "success": True,
            "message": f"æ’ç¨‹åŸ·è¡Œå®Œæˆ",
            "task_id": task_id,
            "session_id": session_id,
            "schedule_name": schedule['schedule_name'],
            "generated_count": len(generated_posts),
            "failed_count": len(failed_posts),
            "posts": generated_posts,
            "errors": failed_posts,
            "timestamp": get_current_time().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ ç«‹å³åŸ·è¡Œæ’ç¨‹å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "task_id": task_id
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.put("/api/schedule/tasks/{task_id}")
async def update_schedule(task_id: str, request: Request):
    """
    ç·¨è¼¯æ’ç¨‹è¨­å®š
    Update schedule configuration
    """
    logger.info(f"æ”¶åˆ°ç·¨è¼¯æ’ç¨‹è«‹æ±‚ - Task ID: {task_id}")

    conn = None
    try:
        data = await request.json()
        logger.info(f"æ¥æ”¶åˆ°çš„æ•¸æ“š: {data}")

        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Check if schedule exists
            cursor.execute("""
                SELECT schedule_id
                FROM schedule_tasks
                WHERE schedule_id = %s
            """, (task_id,))

            existing = cursor.fetchone()
            if not existing:
                return {
                    "success": False,
                    "error": f"æ‰¾ä¸åˆ°æ’ç¨‹: {task_id}"
                }

            # Build UPDATE query dynamically based on provided fields
            update_fields = []
            update_values = []

            # Handle basic fields
            if 'schedule_name' in data:
                update_fields.append("schedule_name = %s")
                update_values.append(data['schedule_name'])

            if 'schedule_description' in data:
                update_fields.append("schedule_description = %s")
                update_values.append(data['schedule_description'])

            if 'auto_posting' in data:
                update_fields.append("auto_posting = %s")
                update_values.append(data['auto_posting'])

            if 'weekdays_only' in data:
                update_fields.append("weekdays_only = %s")
                update_values.append(data['weekdays_only'])

            # Handle JSON fields
            if 'generation_config' in data:
                update_fields.append("generation_config = %s")
                update_values.append(json.dumps(data['generation_config']))

            if 'trigger_config' in data:
                update_fields.append("trigger_config = %s")
                update_values.append(json.dumps(data['trigger_config']))

            if 'schedule_config' in data:
                update_fields.append("schedule_config = %s")
                update_values.append(json.dumps(data['schedule_config']))

            # Always update timestamp
            update_fields.append("updated_at = NOW()")

            if not update_fields:
                return {
                    "success": False,
                    "error": "æ²’æœ‰æä¾›è¦æ›´æ–°çš„æ¬„ä½"
                }

            # Execute UPDATE
            query = f"""
                UPDATE schedule_tasks
                SET {', '.join(update_fields)}
                WHERE schedule_id = %s
                RETURNING *
            """
            update_values.append(task_id)

            cursor.execute(query, update_values)
            updated_schedule = cursor.fetchone()

            conn.commit()

            logger.info(f"âœ… æ’ç¨‹æ›´æ–°æˆåŠŸ: {task_id}")

            return {
                "success": True,
                "message": "æ’ç¨‹æ›´æ–°æˆåŠŸ",
                "task_id": task_id,
                "schedule": dict(updated_schedule) if updated_schedule else None
            }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ ç·¨è¼¯æ’ç¨‹å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "task_id": task_id
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/schedule/cancel/{task_id}")
async def cancel_schedule(task_id: str):
    """
    å–æ¶ˆ/åœæ­¢æ’ç¨‹
    Cancel or stop a schedule
    """
    logger.info(f"æ”¶åˆ°å–æ¶ˆæ’ç¨‹è«‹æ±‚ - Task ID: {task_id}")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Check if schedule exists
            cursor.execute("""
                SELECT schedule_id, status
                FROM schedule_tasks
                WHERE schedule_id = %s
            """, (task_id,))

            existing = cursor.fetchone()
            if not existing:
                return {
                    "success": False,
                    "error": f"æ‰¾ä¸åˆ°æ’ç¨‹: {task_id}"
                }

            # Update status to cancelled
            cursor.execute("""
                UPDATE schedule_tasks
                SET status = 'cancelled',
                    updated_at = NOW()
                WHERE schedule_id = %s
                RETURNING *
            """, (task_id,))

            updated_schedule = cursor.fetchone()
            conn.commit()

            logger.info(f"âœ… æ’ç¨‹å·²å–æ¶ˆ: {task_id}")

            return {
                "success": True,
                "message": "æ’ç¨‹å·²å–æ¶ˆ",
                "task_id": task_id,
                "previous_status": existing['status'],
                "new_status": "cancelled"
            }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ å–æ¶ˆæ’ç¨‹å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "task_id": task_id
        }
    finally:
        if conn:
            return_db_connection(conn)


@app.post("/api/schedule/start/{task_id}")
async def start_schedule(task_id: str):
    """
    å•Ÿå‹•/æ¢å¾©æ’ç¨‹
    Start or resume a schedule
    """
    logger.info(f"æ”¶åˆ°å•Ÿå‹•æ’ç¨‹è«‹æ±‚ - Task ID: {task_id}")

    conn = None
    try:
        if not db_pool:
            return {
                "success": False,
                "error": "æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨"
            }

        conn = get_db_connection()
        with conn.cursor(cursor_factory=RealDictCursor) as cursor:
            # Check if schedule exists
            cursor.execute("""
                SELECT schedule_id, status
                FROM schedule_tasks
                WHERE schedule_id = %s
            """, (task_id,))

            existing = cursor.fetchone()
            if not existing:
                return {
                    "success": False,
                    "error": f"æ‰¾ä¸åˆ°æ’ç¨‹: {task_id}"
                }

            # Update status to active
            cursor.execute("""
                UPDATE schedule_tasks
                SET status = 'active',
                    updated_at = NOW()
                WHERE schedule_id = %s
                RETURNING *
            """, (task_id,))

            updated_schedule = cursor.fetchone()
            conn.commit()

            logger.info(f"âœ… æ’ç¨‹å·²å•Ÿå‹•: {task_id}")

            return {
                "success": True,
                "message": "æ’ç¨‹å·²å•Ÿå‹•",
                "task_id": task_id,
                "previous_status": existing['status'],
                "new_status": "active"
            }

    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"âŒ å•Ÿå‹•æ’ç¨‹å¤±æ•—: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e),
            "task_id": task_id
        }
    finally:
        if conn:
            return_db_connection(conn)


# ==================== è¨»å†Š Reaction Bot Routes ====================
# Register reaction bot router after all dependencies are loaded
try:
    from reaction_bot_routes import router as reaction_bot_router
    app.include_router(reaction_bot_router)
    logger.info("âœ… Reaction Bot routes registered successfully")
except Exception as e:
    logger.error(f"âŒ Failed to register Reaction Bot routes: {e}")
    import traceback
    logger.error(traceback.format_exc())


if __name__ == "__main__":
    import uvicorn

    # Railway ä½¿ç”¨ PORT ç’°å¢ƒè®Šæ•¸ï¼Œæœ¬åœ°é–‹ç™¼ä½¿ç”¨ 8000
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ğŸš€ å•Ÿå‹• Unified API æœå‹™å™¨: {host}:{port}")
    uvicorn.run(app, host=host, port=port)
