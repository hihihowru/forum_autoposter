"""
çµ±ä¸€çš„ API æœå‹™ - å®Œæ•´ç‰ˆæœ¬
Railway éƒ¨ç½²æ™‚ä½¿ç”¨æ­¤æœå‹™ä½œç‚ºå”¯ä¸€çš„ API å…¥å£
æ•´åˆæ‰€æœ‰å¾®æœå‹™åŠŸèƒ½åˆ°ä¸€å€‹ API
"""

from fastapi import FastAPI, Query, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import httpx
import json
import sys
import os
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import RealDictCursor

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(
    title="Forum Autoposter Unified API",
    description="çµ±ä¸€çš„ API æœå‹™ï¼Œæ•´åˆæ‰€æœ‰å¾®æœå‹™åŠŸèƒ½",
    version="1.0.0"
)

# é…ç½® CORS - å…è¨±æ‰€æœ‰ä¾†æºï¼ˆå› ç‚ºæˆ‘å€‘æœƒç”¨ Vercel Proxyï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# ==================== FinLab åˆå§‹åŒ– ====================

import finlab
from finlab import data
import psycopg2
from psycopg2.extras import RealDictCursor

stock_mapping = {}
db_connection = None

def create_post_records_table():
    """å‰µå»º post_records è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    try:
        if not db_connection:
            logger.error("âŒ æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨ï¼Œç„¡æ³•å‰µå»ºè¡¨")
            return
            
        with db_connection.cursor() as cursor:
            logger.info("ğŸ” æª¢æŸ¥ post_records è¡¨æ˜¯å¦å­˜åœ¨...")
            # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            cursor.execute("""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = 'post_records'
                );
            """)
            table_exists = cursor.fetchone()[0]
            logger.info(f"ğŸ“Š è¡¨å­˜åœ¨ç‹€æ…‹: {table_exists}")
            
            if not table_exists:
                logger.info("ğŸ“‹ é–‹å§‹å‰µå»º post_records è¡¨...")
                cursor.execute("""
                    CREATE TABLE post_records (
                        post_id VARCHAR PRIMARY KEY,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        session_id BIGINT,
                        kol_serial INTEGER NOT NULL,
                        kol_nickname VARCHAR NOT NULL,
                        kol_persona VARCHAR,
                        stock_code VARCHAR NOT NULL,
                        stock_name VARCHAR NOT NULL,
                        title VARCHAR NOT NULL,
                        content TEXT NOT NULL,
                        content_md TEXT,
                        status VARCHAR DEFAULT 'draft',
                        reviewer_notes TEXT,
                        approved_by VARCHAR,
                        approved_at TIMESTAMP,
                        scheduled_at TIMESTAMP,
                        published_at TIMESTAMP,
                        cmoney_post_id VARCHAR,
                        cmoney_post_url VARCHAR,
                        publish_error TEXT,
                        views BIGINT DEFAULT 0,
                        likes BIGINT DEFAULT 0,
                        comments BIGINT DEFAULT 0,
                        shares BIGINT DEFAULT 0,
                        topic_id VARCHAR,
                        topic_title VARCHAR,
                        technical_analysis TEXT,
                        serper_data TEXT,
                        quality_score FLOAT,
                        ai_detection_score FLOAT,
                        risk_level VARCHAR,
                        generation_params TEXT,
                        commodity_tags TEXT,
                        alternative_versions TEXT
                    );
                """)
                db_connection.commit()
                logger.info("âœ… post_records è¡¨å‰µå»ºæˆåŠŸ")
            else:
                logger.info("âœ… post_records è¡¨å·²å­˜åœ¨")
                
    except Exception as e:
        logger.error(f"âŒ å‰µå»º post_records è¡¨å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
        raise

@app.on_event("startup")
def startup_event():
    """å•Ÿå‹•æ™‚åˆå§‹åŒ– FinLab å’Œæ•¸æ“šåº«é€£æ¥"""
    global stock_mapping, db_connection

    # æª¢æŸ¥æ‰€æœ‰é—œéµç’°å¢ƒè®Šæ•¸
    logger.info("ğŸ” [å•Ÿå‹•æª¢æŸ¥] é–‹å§‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸...")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FINLAB_API_KEY å­˜åœ¨: {os.getenv('FINLAB_API_KEY') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FORUM_200_EMAIL å­˜åœ¨: {os.getenv('FORUM_200_EMAIL') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] FORUM_200_PASSWORD å­˜åœ¨: {os.getenv('FORUM_200_PASSWORD') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] DATABASE_URL å­˜åœ¨: {os.getenv('DATABASE_URL') is not None}")
    logger.info(f"ğŸ” [å•Ÿå‹•æª¢æŸ¥] PORT: {os.getenv('PORT', 'æœªè¨­å®š')}")

    # åˆå§‹åŒ–æ•¸æ“šåº«é€£æ¥
    try:
        database_url = os.getenv("DATABASE_URL")
        if database_url:
            logger.info(f"ğŸ”— å˜—è©¦é€£æ¥æ•¸æ“šåº«: {database_url[:20]}...")
            # Railway PostgreSQL URL æ ¼å¼è½‰æ›ï¼ˆpostgresql:// -> postgres://ï¼‰
            if database_url.startswith("postgres://"):
                database_url = database_url.replace("postgres://", "postgresql://", 1)

            db_connection = psycopg2.connect(database_url)
            logger.info("âœ… PostgreSQL æ•¸æ“šåº«é€£æ¥æˆåŠŸ")
            
            # å‰µå»º post_records è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            logger.info("ğŸ“‹ é–‹å§‹å‰µå»º post_records è¡¨...")
            create_post_records_table()
            logger.info("âœ… post_records è¡¨å‰µå»ºå®Œæˆ")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ° DATABASE_URL ç’°å¢ƒè®Šæ•¸ï¼Œå°‡ç„¡æ³•æŸ¥è©¢è²¼æ–‡æ•¸æ“š")
    except Exception as e:
        logger.error(f"âŒ PostgreSQL æ•¸æ“šåº«é€£æ¥å¤±æ•—: {e}")
        logger.error(f"âŒ éŒ¯èª¤è©³æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ å®Œæ•´éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")

    api_key = os.getenv("FINLAB_API_KEY")
    if api_key:
        finlab.login(api_key)
        logger.info("âœ… FinLab API ç™»å…¥æˆåŠŸ")
    else:
        logger.warning("âŒ æœªæ‰¾åˆ° FINLAB_API_KEY ç’°å¢ƒè®Šæ•¸")

    # è¼‰å…¥è‚¡ç¥¨æ˜ å°„è¡¨ï¼ˆå…ˆå˜—è©¦éœæ…‹æ–‡ä»¶ï¼‰
    try:
        stock_mapping_path = '/app/stock_mapping.json'
        if os.path.exists(stock_mapping_path):
            with open(stock_mapping_path, 'r', encoding='utf-8') as f:
                stock_mapping = json.load(f)
            logger.info(f"âœ… è¼‰å…¥éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨æˆåŠŸ: {len(stock_mapping)} æ”¯è‚¡ç¥¨")
        else:
            logger.warning(f"âš ï¸ éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨ä¸å­˜åœ¨: {stock_mapping_path}")
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥éœæ…‹è‚¡ç¥¨æ˜ å°„è¡¨å¤±æ•—: {e}")

    # å¾ FinLab å‹•æ…‹è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨Š
    try:
        if api_key:
            logger.info("ğŸ“Š æ­£åœ¨å¾ FinLab è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨Š...")
            company_info = data.get('company_basic_info')
            if company_info is not None and not company_info.empty:
                # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
                for stock_id in company_info['stock_id']:
                    stock_data = company_info[company_info['stock_id'] == stock_id].iloc[0]
                    stock_mapping[stock_id] = {
                        'company_name': stock_data.get('å…¬å¸ç°¡ç¨±', stock_data.get('å…¬å¸åç¨±', f'è‚¡ç¥¨{stock_id}')),
                        'industry': stock_data.get('ç”¢æ¥­é¡åˆ¥', 'æœªçŸ¥ç”¢æ¥­')
                    }
                logger.info(f"âœ… å¾ FinLab è¼‰å…¥å®Œæ•´å…¬å¸è³‡è¨ŠæˆåŠŸ: {len(stock_mapping)} æ”¯è‚¡ç¥¨")
            else:
                logger.warning("âš ï¸ ç„¡æ³•å¾ FinLab å–å¾—å…¬å¸è³‡è¨Š")
    except Exception as e:
        logger.error(f"âŒ å¾ FinLab è¼‰å…¥å…¬å¸è³‡è¨Šå¤±æ•—: {e}")

def ensure_finlab_login():
    """ç¢ºä¿ FinLab å·²ç™»å…¥"""
    try:
        test_data = data.get('market_transaction_info:æ”¶ç›¤æŒ‡æ•¸')
        if test_data is None:
            api_key = os.getenv("FINLAB_API_KEY")
            if api_key:
                finlab.login(api_key)
                logger.info("ğŸ”„ FinLab API é‡æ–°ç™»å…¥æˆåŠŸ")
            else:
                raise Exception("æœªæ‰¾åˆ° FINLAB_API_KEY")
    except Exception as e:
        logger.error(f"âŒ FinLab ç™»å…¥æª¢æŸ¥å¤±æ•—: {e}")
        raise e

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def get_stock_name(stock_code: str) -> str:
    """æ ¹æ“šè‚¡ç¥¨ä»£è™Ÿç²å–è‚¡ç¥¨åç¨±"""
    if stock_code in stock_mapping:
        return stock_mapping[stock_code].get('company_name', f"è‚¡ç¥¨{stock_code}")

    stock_names = {
        "2330": "å°ç©é›»", "2454": "è¯ç™¼ç§‘", "2317": "é´»æµ·", "2881": "å¯Œé‚¦é‡‘",
        "2882": "åœ‹æ³°é‡‘", "1101": "å°æ³¥", "1102": "äºæ³¥", "1216": "çµ±ä¸€",
        "1303": "å—äº", "1326": "å°åŒ–", "2002": "ä¸­é‹¼", "2308": "å°é”é›»",
        "2377": "å¾®æ˜Ÿ", "2382": "å»£é”", "2408": "å—äºç§‘", "2474": "å¯æˆ",
        "2498": "å®é”é›»", "3008": "å¤§ç«‹å…‰", "3034": "è¯è© ", "3231": "ç·¯å‰µ",
        "3711": "æ—¥æœˆå…‰æŠ•æ§", "4938": "å’Œç¢©", "6505": "å°å¡‘åŒ–", "8046": "å—é›»",
        "9910": "è±æ³°", "2412": "ä¸­è¯é›»", "1301": "å°å¡‘", "2603": "é•·æ¦®"
    }
    return stock_names.get(stock_code, f"è‚¡ç¥¨{stock_code}")

def get_stock_industry(stock_code: str) -> str:
    """æ ¹æ“šè‚¡ç¥¨ä»£è™Ÿç²å–ç”¢æ¥­é¡åˆ¥"""
    if stock_code in stock_mapping:
        return stock_mapping[stock_code].get('industry', 'æœªçŸ¥ç”¢æ¥­')
    return 'æœªçŸ¥ç”¢æ¥­'

def calculate_trading_stats(stock_id: str, latest_date: datetime, close_df: pd.DataFrame) -> dict:
    """è¨ˆç®—éå»äº”å€‹äº¤æ˜“æ—¥çš„çµ±è¨ˆè³‡è¨Š"""
    try:
        trading_days = close_df.index[close_df.index <= latest_date].sort_values(ascending=False)[:5]

        if len(trading_days) < 2:
            return {"up_days": 0, "five_day_change": 0.0}

        up_days = 0
        for i in range(len(trading_days) - 1):
            current_price = close_df.loc[trading_days[i], stock_id]
            previous_price = close_df.loc[trading_days[i + 1], stock_id]

            if not pd.isna(current_price) and not pd.isna(previous_price) and current_price > previous_price:
                up_days += 1

        five_days_ago_price = close_df.loc[trading_days[-1], stock_id]
        latest_price = close_df.loc[trading_days[0], stock_id]

        if not pd.isna(five_days_ago_price) and not pd.isna(latest_price) and five_days_ago_price != 0:
            five_day_change = ((latest_price - five_days_ago_price) / five_days_ago_price) * 100
        else:
            five_day_change = 0.0

        return {
            "up_days": up_days,
            "five_day_change": round(five_day_change, 2)
        }
    except Exception as e:
        logger.error(f"è¨ˆç®— {stock_id} äº¤æ˜“çµ±è¨ˆå¤±æ•—: {e}")
        return {"up_days": 0, "five_day_change": 0.0}

# ==================== å¥åº·æª¢æŸ¥ ====================

@app.get("/")
async def root():
    """æ ¹è·¯å¾‘"""
    logger.info("æ”¶åˆ°æ ¹è·¯å¾‘è«‹æ±‚")
    return {
        "message": "Forum Autoposter Unified API",
        "status": "running",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    logger.info("æ”¶åˆ°å¥åº·æª¢æŸ¥è«‹æ±‚")
    return {
        "status": "healthy",
        "message": "Unified API is running successfully",
        "timestamp": datetime.now().isoformat()
    }

# ==================== OHLC API åŠŸèƒ½ ====================

@app.get("/after_hours_limit_up")
async def get_after_hours_limit_up_stocks(
    limit: int = Query(1000, description="è‚¡ç¥¨æ•¸é‡é™åˆ¶"),
    changeThreshold: float = Query(9.5, description="æ¼²è·Œå¹…é–¾å€¼ç™¾åˆ†æ¯”"),
    industries: str = Query("", description="ç”¢æ¥­é¡åˆ¥ç¯©é¸")
):
    """ç²å–ç›¤å¾Œæ¼²åœè‚¡ç¥¨åˆ—è¡¨ - æ”¯æŒå‹•æ…‹æ¼²è·Œå¹…è¨­å®š"""
    try:
        ensure_finlab_login()

        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–æ”¶ç›¤åƒ¹æ•¸æ“š"}

        close_df = close_df.sort_index()
        if volume_df is not None:
            volume_df = volume_df.sort_index()

        latest_date = close_df.index[-1]
        latest_close = close_df.loc[latest_date]

        previous_dates = close_df.index[close_df.index < latest_date]
        if len(previous_dates) == 0:
            return {"error": f"ç„¡æ³•æ‰¾åˆ° {latest_date.strftime('%Y-%m-%d')} ä¹‹å‰çš„äº¤æ˜“æ—¥æ•¸æ“š"}

        previous_date = previous_dates[-1]
        previous_close = close_df.loc[previous_date]

        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]

        selected_industries = []
        if industries:
            selected_industries = [industry.strip() for industry in industries.split(',') if industry.strip()]

        limit_up_stocks = []

        for stock_id in latest_close.index:
            try:
                if selected_industries:
                    stock_industry = get_stock_industry(stock_id)
                    if stock_industry not in selected_industries:
                        continue

                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]

                if pd.isna(today_price) or pd.isna(yesterday_price) or yesterday_price == 0:
                    continue

                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100

                if change_percent >= changeThreshold:
                    stock_name = get_stock_name(stock_id)
                    stock_industry = get_stock_industry(stock_id)

                    volume = 0
                    volume_amount = 0
                    if latest_volume is not None and stock_id in latest_volume.index:
                        vol = latest_volume[stock_id]
                        if not pd.isna(vol):
                            volume = int(vol)
                            volume_amount = volume * float(today_price)

                    trading_stats = calculate_trading_stats(stock_id, latest_date, close_df)

                    limit_up_stocks.append({
                        'stock_code': stock_id,
                        'stock_name': stock_name,
                        'industry': stock_industry,
                        'current_price': float(today_price),
                        'yesterday_close': float(yesterday_price),
                        'change_amount': float(today_price - yesterday_price),
                        'change_percent': float(change_percent),
                        'volume': volume,
                        'volume_amount': volume_amount,
                        'date': latest_date.strftime('%Y-%m-%d'),
                        'previous_date': previous_date.strftime('%Y-%m-%d'),
                        'up_days_5': trading_stats['up_days'],
                        'five_day_change': trading_stats['five_day_change']
                    })

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        limit_up_stocks.sort(key=lambda x: x['volume'], reverse=True)
        limit_up_stocks = limit_up_stocks[:limit]

        return {
            'success': True,
            'total_count': len(limit_up_stocks),
            'stocks': limit_up_stocks,
            'timestamp': datetime.now().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'changeThreshold': changeThreshold
        }

    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œæ¼²åœè‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/after_hours_limit_down")
async def get_after_hours_limit_down_stocks(
    limit: int = Query(1000, description="è‚¡ç¥¨æ•¸é‡é™åˆ¶"),
    changeThreshold: float = Query(-9.5, description="è·Œå¹…é–¾å€¼ç™¾åˆ†æ¯”"),
    industries: str = Query("", description="ç”¢æ¥­é¡åˆ¥ç¯©é¸")
):
    """ç²å–ç›¤å¾Œè·Œåœè‚¡ç¥¨åˆ—è¡¨"""
    try:
        ensure_finlab_login()

        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if close_df is None or close_df.empty:
            return {"error": "ç„¡æ³•ç²å–æ”¶ç›¤åƒ¹æ•¸æ“š"}

        close_df = close_df.sort_index()
        if volume_df is not None:
            volume_df = volume_df.sort_index()

        latest_date = close_df.index[-1]
        latest_close = close_df.loc[latest_date]

        previous_dates = close_df.index[close_df.index < latest_date]
        if len(previous_dates) == 0:
            return {"error": f"ç„¡æ³•æ‰¾åˆ° {latest_date.strftime('%Y-%m-%d')} ä¹‹å‰çš„äº¤æ˜“æ—¥æ•¸æ“š"}

        previous_date = previous_dates[-1]
        previous_close = close_df.loc[previous_date]

        latest_volume = None
        if volume_df is not None and latest_date in volume_df.index:
            latest_volume = volume_df.loc[latest_date]

        selected_industries = []
        if industries:
            selected_industries = [industry.strip() for industry in industries.split(',') if industry.strip()]

        limit_down_stocks = []

        for stock_id in latest_close.index:
            try:
                if selected_industries:
                    stock_industry = get_stock_industry(stock_id)
                    if stock_industry not in selected_industries:
                        continue

                today_price = latest_close[stock_id]
                yesterday_price = previous_close[stock_id]

                if pd.isna(today_price) or pd.isna(yesterday_price) or yesterday_price == 0:
                    continue

                change_percent = ((today_price - yesterday_price) / yesterday_price) * 100

                if change_percent <= changeThreshold:
                    stock_name = get_stock_name(stock_id)
                    stock_industry = get_stock_industry(stock_id)

                    volume = 0
                    volume_amount = 0
                    if latest_volume is not None and stock_id in latest_volume.index:
                        vol = latest_volume[stock_id]
                        if not pd.isna(vol):
                            volume = int(vol)
                            volume_amount = volume * float(today_price)

                    trading_stats = calculate_trading_stats(stock_id, latest_date, close_df)

                    limit_down_stocks.append({
                        'stock_code': stock_id,
                        'stock_name': stock_name,
                        'industry': stock_industry,
                        'current_price': float(today_price),
                        'yesterday_close': float(yesterday_price),
                        'change_amount': float(today_price - yesterday_price),
                        'change_percent': float(change_percent),
                        'volume': volume,
                        'volume_amount': volume_amount,
                        'date': latest_date.strftime('%Y-%m-%d'),
                        'previous_date': previous_date.strftime('%Y-%m-%d'),
                        'up_days_5': trading_stats['up_days'],
                        'five_day_change': trading_stats['five_day_change']
                    })

            except Exception as e:
                logger.error(f"è™•ç†è‚¡ç¥¨ {stock_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        limit_down_stocks.sort(key=lambda x: x['volume'], reverse=True)
        limit_down_stocks = limit_down_stocks[:limit]

        return {
            'success': True,
            'total_count': len(limit_down_stocks),
            'stocks': limit_down_stocks,
            'timestamp': datetime.now().isoformat(),
            'date': latest_date.strftime('%Y-%m-%d'),
            'previous_date': previous_date.strftime('%Y-%m-%d'),
            'changeThreshold': changeThreshold
        }

    except Exception as e:
        logger.error(f"ç²å–ç›¤å¾Œè·Œåœè‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/stock_mapping.json")
async def get_stock_mapping():
    """ç²å–å®Œæ•´è‚¡ç¥¨æ˜ å°„è¡¨ï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰"""
    logger.info("æ”¶åˆ° stock_mapping è«‹æ±‚")

    try:
        return {
            "success": True,
            "data": stock_mapping,
            "count": len(stock_mapping),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ç²å–è‚¡ç¥¨æ˜ å°„è¡¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/industries")
async def get_industries():
    """ç²å–æ‰€æœ‰ç”¢æ¥­é¡åˆ¥"""
    logger.info("æ”¶åˆ° industries è«‹æ±‚")

    try:
        industries = set()
        for stock_code, info in stock_mapping.items():
            if 'industry' in info:
                industries.add(info['industry'])

        industries_list = [{"id": ind, "name": ind} for ind in sorted(list(industries))]

        result = {
            "success": True,
            "data": industries_list,
            "count": len(industries_list),
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"è¿”å› industries æ•¸æ“š: {len(result['data'])} æ¢è¨˜éŒ„")
        return result
    except Exception as e:
        logger.error(f"ç²å–ç”¢æ¥­é¡åˆ¥å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/stocks_by_industry")
async def get_stocks_by_industry(industry: str = Query(..., description="ç”¢æ¥­é¡åˆ¥")):
    """æ ¹æ“šç”¢æ¥­ç²å–è‚¡ç¥¨åˆ—è¡¨"""
    logger.info(f"æ”¶åˆ° stocks_by_industry è«‹æ±‚: industry={industry}")

    try:
        stocks = []
        for stock_code, info in stock_mapping.items():
            if info.get('industry') == industry:
                stocks.append({
                    "stock_id": stock_code,
                    "stock_name": info.get('company_name', stock_code),
                    "industry": industry
                })

        result = {
            "success": True,
            "data": stocks,
            "count": len(stocks),
            "industry": industry,
            "timestamp": datetime.now().isoformat()
        }

        logger.info(f"è¿”å› {industry} ç”¢æ¥­è‚¡ç¥¨: {len(result['data'])} æ”¯")
        return result
    except Exception as e:
        logger.error(f"ç²å–ç”¢æ¥­è‚¡ç¥¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.get("/get_ohlc")
async def get_ohlc(stock_id: str = Query(..., description="è‚¡ç¥¨ä»£ç¢¼")):
    """ç²å–ç‰¹å®šè‚¡ç¥¨çš„ OHLC æ•¸æ“š"""
    logger.info(f"æ”¶åˆ° get_ohlc è«‹æ±‚: stock_id={stock_id}")

    try:
        ensure_finlab_login()

        open_df = data.get('price:é–‹ç›¤åƒ¹')
        high_df = data.get('price:æœ€é«˜åƒ¹')
        low_df = data.get('price:æœ€ä½åƒ¹')
        close_df = data.get('price:æ”¶ç›¤åƒ¹')
        volume_df = data.get('price:æˆäº¤è‚¡æ•¸')

        if stock_id not in open_df.columns:
            return {"error": f"Stock ID {stock_id} not found."}

        ohlcv_df = pd.DataFrame({
            'open': open_df[stock_id],
            'high': high_df[stock_id],
            'low': low_df[stock_id],
            'close': close_df[stock_id],
            'volume': volume_df[stock_id]
        })

        ohlcv_df = ohlcv_df.dropna().reset_index()
        ohlcv_df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']

        one_year_ago = datetime.today() - timedelta(days=365)
        ohlcv_df = ohlcv_df[ohlcv_df['date'] >= one_year_ago]

        return json.loads(ohlcv_df.to_json(orient="records", date_format="iso"))
    except Exception as e:
        logger.error(f"ç²å– OHLC æ•¸æ“šå¤±æ•—: {e}")
        return {"error": str(e)}

# ==================== ç›¤ä¸­è§¸ç™¼å™¨åŠŸèƒ½ ====================

# CMoney API é…ç½®
CMONEY_API_BASE = "https://asterisk-chipsapi.cmoney.tw"

def get_forum_200_credentials():
    """å¾ç’°å¢ƒè®Šæ•¸ç²å– forum_200 æ†‘è­‰"""
    email = os.getenv("FORUM_200_EMAIL")
    password = os.getenv("FORUM_200_PASSWORD")
    member_id = os.getenv("FORUM_200_MEMBER_ID", "9505546")  # é è¨­å€¼

    # è¨˜éŒ„ç’°å¢ƒè®Šæ•¸ç‹€æ…‹ï¼ˆä¸è¨˜éŒ„å¯¦éš›å€¼ä»¥ä¿è­·éš±ç§ï¼‰
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_EMAIL å­˜åœ¨: {email is not None}")
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_PASSWORD å­˜åœ¨: {password is not None}")
    logger.info(f"ğŸ“‹ [æ†‘è­‰æª¢æŸ¥] FORUM_200_MEMBER_ID: {member_id}")

    if not email or not password:
        # æ›´è©³ç´°çš„éŒ¯èª¤è¨Šæ¯
        missing = []
        if not email:
            missing.append("FORUM_200_EMAIL")
        if not password:
            missing.append("FORUM_200_PASSWORD")
        error_msg = f"ç¼ºå°‘ç’°å¢ƒè®Šæ•¸: {', '.join(missing)}"
        logger.error(f"âŒ {error_msg}")
        raise Exception(error_msg)

    logger.info(f"âœ… [æ†‘è­‰æª¢æŸ¥] æˆåŠŸè¼‰å…¥ forum_200 æ†‘è­‰: {email}")
    return {
        "email": email,
        "password": password,
        "member_id": member_id
    }

_token_cache = {
    "token": None,
    "expires_at": None,
    "created_at": None
}

async def get_dynamic_auth_token() -> str:
    """ä½¿ç”¨ forum_200 KOL æ†‘è­‰å‹•æ…‹å–å¾— CMoney API token"""
    try:
        if (_token_cache["token"] and
            _token_cache["expires_at"] and
            datetime.now() < _token_cache["expires_at"]):
            logger.info("âœ… ä½¿ç”¨å¿«å–çš„ CMoney API token")
            return _token_cache["token"]

        logger.info("ğŸ” é–‹å§‹ä½¿ç”¨ forum_200 æ†‘è­‰ç™»å…¥ CMoney...")

        # å°‡ src è·¯å¾‘åŠ å…¥ Python path
        src_path = '/app/src'
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        from src.clients.cmoney.cmoney_client import CMoneyClient, LoginCredentials

        cmoney_client = CMoneyClient()
        forum_credentials = get_forum_200_credentials()
        credentials = LoginCredentials(
            email=forum_credentials["email"],
            password=forum_credentials["password"]
        )

        login_result = await cmoney_client.login(credentials)

        if not login_result or not login_result.token:
            raise Exception("forum_200 ç™»å…¥å¤±æ•—")

        _token_cache["token"] = login_result.token
        _token_cache["expires_at"] = login_result.expires_at
        _token_cache["created_at"] = datetime.now()

        logger.info(f"âœ… forum_200 ç™»å…¥æˆåŠŸï¼Œtoken æœ‰æ•ˆæœŸè‡³: {login_result.expires_at}")
        return login_result.token

    except Exception as e:
        logger.error(f"âŒ å‹•æ…‹å–å¾— CMoney API token å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"èªè­‰å¤±æ•—: {str(e)}")

@app.post("/intraday-trigger/execute")
async def get_intraday_trigger_stocks(request: Request):
    """ç²å–ç›¤ä¸­è§¸ç™¼å™¨è‚¡ç¥¨åˆ—è¡¨"""
    try:
        # å¾ POST body ç²å–é…ç½®
        body = await request.json()
        endpoint = body.get("endpoint")
        processing = body.get("processing", [])
        
        logger.info(f"æ”¶åˆ°ç›¤ä¸­è§¸ç™¼å™¨è«‹æ±‚: endpoint={endpoint}, processing={processing}")
        
        if not endpoint:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘ endpoint åƒæ•¸")

        # æº–å‚™è«‹æ±‚åƒæ•¸
        columns = "äº¤æ˜“æ™‚é–“,å‚³è¼¸åºè™Ÿ,å…§å¤–ç›¤æ——æ¨™,å³æ™‚æˆäº¤åƒ¹,å³æ™‚æˆäº¤é‡,æœ€ä½åƒ¹,æœ€é«˜åƒ¹,æ¨™çš„,æ¼²è·Œ,æ¼²è·Œå¹…,ç´¯è¨ˆæˆäº¤ç¸½é¡,ç´¯è¨ˆæˆäº¤é‡,é–‹ç›¤åƒ¹"

        request_data = {
            "AppId": 2,
            "Guid": "583defeb-f7cb-49e7-964d-d0817e944e4f",
            "Processing": processing
        }

        # å‹•æ…‹å–å¾—èªè­‰ token
        logger.info("ğŸ” [ç›¤ä¸­è§¸ç™¼å™¨] é–‹å§‹å‹•æ…‹å–å¾—èªè­‰ token...")
        try:
            auth_token = await get_dynamic_auth_token()
            logger.info(f"âœ… [ç›¤ä¸­è§¸ç™¼å™¨] æˆåŠŸå–å¾— token: {auth_token[:20]}...")
        except Exception as e:
            logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] å‹•æ…‹èªè­‰å¤±æ•—: {e}")
            raise

        # æº–å‚™è«‹æ±‚é ­
        headers = {
            "Accept-Encoding": "gzip",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {auth_token}",
            "cmoneyapi-trace-context": '{"appVersion":"10.111.0","osName":"iOS","platform":1,"manufacturer":"Apple","osVersion":"18.6.2","appId":2,"model":"iPhone15,2"}'
        }

        # ç™¼é€è«‹æ±‚åˆ° CMoney API
        logger.info(f"ğŸŒ [ç›¤ä¸­è§¸ç™¼å™¨] ç™¼é€è«‹æ±‚åˆ° CMoney API: {endpoint}")
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{endpoint}?columns={columns}",
                json=request_data,
                headers=headers
            )

            logger.info(f"ğŸ“¡ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API éŸ¿æ‡‰ç‹€æ…‹: {response.status_code}")

            if response.status_code != 200:
                logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API è«‹æ±‚å¤±æ•—: HTTP {response.status_code}, éŸ¿æ‡‰: {response.text}")
                raise HTTPException(status_code=500, detail=f"CMoney API è«‹æ±‚å¤±æ•—: {response.status_code}")

            # è§£æéŸ¿æ‡‰æ•¸æ“š
            raw_data = response.json()
            logger.info(f"ğŸ“Š [ç›¤ä¸­è§¸ç™¼å™¨] æ”¶åˆ°æ•¸æ“š: {len(raw_data)} ç­†è¨˜éŒ„")

            # æå–è‚¡ç¥¨ä»£ç¢¼ä¸¦æ˜ å°„åˆ°è‚¡ç¥¨åç¨±å’Œç”¢æ¥­
            stock_codes = [item[7] for item in raw_data if len(item) > 7 and item[7]]

            # æ§‹å»ºåŒ…å«è‚¡ç¥¨åç¨±å’Œç”¢æ¥­çš„å®Œæ•´æ•¸æ“š
            stocks_with_info = []
            for stock_code in stock_codes:
                stock_name = get_stock_name(stock_code)
                stock_industry = get_stock_industry(stock_code)
                stocks_with_info.append({
                    "stock_code": stock_code,
                    "stock_name": stock_name,
                    "industry": stock_industry
                })

            logger.info(f"âœ… [ç›¤ä¸­è§¸ç™¼å™¨] åŸ·è¡ŒæˆåŠŸï¼Œç²å– {len(stocks_with_info)} æ”¯è‚¡ç¥¨")
            # æå–è‚¡ç¥¨åˆ—è¡¨é¿å… f-string åµŒå¥—å•é¡Œ
            stock_list = [f"{s['stock_code']}({s['stock_name']})" for s in stocks_with_info[:5]]
            logger.info(f"ğŸ“‹ [ç›¤ä¸­è§¸ç™¼å™¨] è‚¡ç¥¨åˆ—è¡¨: {stock_list}...")

            return {
                "success": True,
                "stocks": stock_codes,  # è¿”å›è‚¡ç¥¨ä»£ç¢¼å­—ç¬¦ä¸²æ•¸çµ„ï¼Œèˆ‡å‰ç«¯æœŸæœ›æ ¼å¼ä¸€è‡´
                "data": raw_data,  # ä¿ç•™åŸå§‹ CMoney æ•¸æ“š
                "count": len(stock_codes)
            }

    except httpx.TimeoutException:
        logger.error("âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API è«‹æ±‚è¶…æ™‚")
        raise HTTPException(status_code=504, detail="CMoney API è«‹æ±‚è¶…æ™‚")
    except httpx.ConnectError:
        logger.error("âŒ [ç›¤ä¸­è§¸ç™¼å™¨] CMoney API é€£æ¥å¤±æ•—")
        raise HTTPException(status_code=503, detail="CMoney API é€£æ¥å¤±æ•—")
    except Exception as e:
        logger.error(f"âŒ [ç›¤ä¸­è§¸ç™¼å™¨] åŸ·è¡Œå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"åŸ·è¡Œå¤±æ•—: {str(e)}")

# ==================== Posting Service åŠŸèƒ½ ====================

@app.post("/api/posting")
async def create_posting(request: Request):
    """å‰µå»ºè²¼æ–‡"""
    logger.info("æ”¶åˆ° create_posting è«‹æ±‚")

    try:
        body = await request.json()
        logger.info(f"è²¼æ–‡å…§å®¹: {body}")

        # ç”Ÿæˆå”¯ä¸€çš„ post_id
        import uuid
        post_id = str(uuid.uuid4())
        
        # æº–å‚™æ’å…¥æ•¸æ“š
        post_data = {
            'post_id': post_id,
            'created_at': datetime.now(),
            'updated_at': datetime.now(),
            'session_id': body.get('session_id', 1),
            'kol_serial': body.get('kol_serial', 200),
            'kol_nickname': body.get('kol_nickname', 'KOL-200'),
            'kol_persona': body.get('kol_persona', 'åˆ†æå¸«'),
            'stock_code': body.get('stock_code', ''),
            'stock_name': body.get('stock_name', ''),
            'title': body.get('title', ''),
            'content': body.get('content', ''),
            'content_md': body.get('content_md', ''),
            'status': body.get('status', 'draft'),
            'reviewer_notes': body.get('reviewer_notes'),
            'approved_by': body.get('approved_by'),
            'approved_at': body.get('approved_at'),
            'scheduled_at': body.get('scheduled_at'),
            'published_at': body.get('published_at'),
            'cmoney_post_id': body.get('cmoney_post_id'),
            'cmoney_post_url': body.get('cmoney_post_url'),
            'publish_error': body.get('publish_error'),
            'views': body.get('views', 0),
            'likes': body.get('likes', 0),
            'comments': body.get('comments', 0),
            'shares': body.get('shares', 0),
            'topic_id': body.get('topic_id'),
            'topic_title': body.get('topic_title'),
            'technical_analysis': json.dumps(body.get('technical_analysis', {})) if body.get('technical_analysis') else None,
            'serper_data': json.dumps(body.get('serper_data', {})) if body.get('serper_data') else None,
            'quality_score': body.get('quality_score'),
            'ai_detection_score': body.get('ai_detection_score'),
            'risk_level': body.get('risk_level'),
            'generation_params': json.dumps(body.get('generation_params', {})) if body.get('generation_params') else None,
            'commodity_tags': json.dumps(body.get('commodity_tags', [])) if body.get('commodity_tags') else None,
            'alternative_versions': json.dumps(body.get('alternative_versions', {})) if body.get('alternative_versions') else None
        }
        
        # æ’å…¥åˆ°æ•¸æ“šåº«
        if db_connection:
            with db_connection.cursor() as cursor:
                insert_sql = """
                    INSERT INTO post_records (
                        post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                        kol_persona, stock_code, stock_name, title, content, content_md, 
                        status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                        published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                        views, likes, comments, shares, topic_id, topic_title, 
                        technical_analysis, serper_data, quality_score, ai_detection_score, 
                        risk_level, generation_params, commodity_tags, alternative_versions
                    ) VALUES (
                        %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                        %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                        %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                        %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                        %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                        %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                        %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                        %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                        %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                    )
                """
                
                cursor.execute(insert_sql, post_data)
                db_connection.commit()
                logger.info(f"âœ… è²¼æ–‡å·²æ’å…¥æ•¸æ“šåº«: {post_id}")
        else:
            logger.warning("âš ï¸ æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨ï¼Œç„¡æ³•ä¿å­˜è²¼æ–‡")

        result = {
            "success": True,
            "message": "è²¼æ–‡å‰µå»ºæˆåŠŸ",
            "post_id": post_id,
            "data": body,
            "timestamp": datetime.now().isoformat()
        }

        logger.info("è²¼æ–‡å‰µå»ºæˆåŠŸ")
        return result

    except Exception as e:
        logger.error(f"è²¼æ–‡å‰µå»ºå¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"è²¼æ–‡å‰µå»ºå¤±æ•—: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
        )

@app.post("/api/manual-posting")
async def manual_posting(request: Request):
    """æ‰‹å‹•è²¼æ–‡"""
    logger.info("æ”¶åˆ° manual_posting è«‹æ±‚")

    try:
        body = await request.json()
        logger.info(f"æ‰‹å‹•è²¼æ–‡å…§å®¹: {body}")

        result = {
            "success": True,
            "message": "æ‰‹å‹•è²¼æ–‡æˆåŠŸ",
            "post_id": "manual_post_67890",
            "data": body,
            "timestamp": datetime.now().isoformat()
        }

        logger.info("æ‰‹å‹•è²¼æ–‡æˆåŠŸ")
        return result

    except Exception as e:
        logger.error(f"æ‰‹å‹•è²¼æ–‡å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "message": f"æ‰‹å‹•è²¼æ–‡å¤±æ•—: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
        )

# ==================== Dashboard API åŠŸèƒ½ ====================

@app.get("/dashboard/system-monitoring")
async def get_system_monitoring():
    """ç²å–ç³»çµ±ç›£æ§æ•¸æ“š"""
    logger.info("æ”¶åˆ° system-monitoring è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "cpu_usage": 45.2,
            "memory_usage": 67.8,
            "disk_usage": 23.1,
            "active_connections": 156,
            "uptime": "5 days, 12 hours",
            "last_restart": "2025-10-10T08:30:00Z"
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info("è¿”å›ç³»çµ±ç›£æ§æ•¸æ“š")
    return result

@app.get("/dashboard/content-management")
async def get_content_management():
    """ç²å–å…§å®¹ç®¡ç†æ•¸æ“š"""
    logger.info("æ”¶åˆ° content-management è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "total_posts": 1250,
            "published_posts": 1180,
            "draft_posts": 70,
            "scheduled_posts": 45,
            "failed_posts": 5
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info("è¿”å›å…§å®¹ç®¡ç†æ•¸æ“š")
    return result

@app.get("/dashboard/interaction-analysis")
async def get_interaction_analysis():
    """ç²å–äº’å‹•åˆ†ææ•¸æ“š"""
    logger.info("æ”¶åˆ° interaction-analysis è«‹æ±‚")

    result = {
        "success": True,
        "data": {
            "total_interactions": 15680,
            "likes": 8920,
            "comments": 2340,
            "shares": 4420,
            "engagement_rate": 12.5,
            "top_performing_posts": [
                {"post_id": "post_001", "interactions": 1250},
                {"post_id": "post_002", "interactions": 980},
                {"post_id": "post_003", "interactions": 750}
            ]
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info("è¿”å›äº’å‹•åˆ†ææ•¸æ“š")
    return result

# ==================== Posts API åŠŸèƒ½ ====================

@app.get("/posts")
async def get_posts(
    skip: int = Query(0, description="è·³éçš„è¨˜éŒ„æ•¸"),
    limit: int = Query(10000, description="è¿”å›çš„è¨˜éŒ„æ•¸"),
    status: str = Query(None, description="ç‹€æ…‹ç¯©é¸")
):
    """ç²å–è²¼æ–‡åˆ—è¡¨ï¼ˆå¾ PostgreSQL æ•¸æ“šåº«ï¼‰"""
    logger.info(f"æ”¶åˆ° get_posts è«‹æ±‚: skip={skip}, limit={limit}, status={status}")

    try:
        if not db_connection:
            logger.warning("æ•¸æ“šåº«é€£æ¥ä¸å¯ç”¨ï¼Œè¿”å›ç©ºæ•¸æ“š")
            return {
                "success": True,
                "posts": [],
                "count": 0,
                "skip": skip,
                "limit": limit,
                "timestamp": datetime.now().isoformat()
            }

        # æŸ¥è©¢ post_records è¡¨
        with db_connection.cursor(cursor_factory=RealDictCursor) as cursor:
            # æ§‹å»ºæŸ¥è©¢
            query = "SELECT * FROM post_records"
            params = []

            if status:
                query += " WHERE status = %s"
                params.append(status)

            query += " ORDER BY created_at DESC LIMIT %s OFFSET %s"
            params.extend([limit, skip])

            logger.info(f"åŸ·è¡Œ SQL æŸ¥è©¢: {query} with params: {params}")
            cursor.execute(query, params)
            posts = cursor.fetchall()

            # ç²å–ç¸½æ•¸
            count_query = "SELECT COUNT(*) as count FROM post_records"
            if status:
                count_query += " WHERE status = %s"
                cursor.execute(count_query, [status] if status else [])
            else:
                cursor.execute(count_query)

            total_count = cursor.fetchone()['count']

            logger.info(f"æŸ¥è©¢åˆ° {len(posts)} æ¢è²¼æ–‡æ•¸æ“šï¼Œç¸½æ•¸: {total_count}")

            return {
                "success": True,
                "posts": [dict(post) for post in posts],
                "count": total_count,
                "skip": skip,
                "limit": limit,
                "timestamp": datetime.now().isoformat()
            }

    except Exception as e:
        logger.error(f"æŸ¥è©¢è²¼æ–‡æ•¸æ“šå¤±æ•—: {e}")
        return {
            "success": False,
            "posts": [],
            "count": 0,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ==================== Trending API åŠŸèƒ½ ====================

@app.get("/trending")
async def get_trending_topics():
    """ç²å–ç†±é–€è©±é¡Œ"""
    logger.info("æ”¶åˆ° trending è«‹æ±‚")

    result = {
        "success": True,
        "data": [
            {"topic": "AIäººå·¥æ™ºæ…§", "trend_score": 95.5, "posts_count": 1250},
            {"topic": "é›»å‹•è»Š", "trend_score": 88.2, "posts_count": 980},
            {"topic": "åŠå°é«”", "trend_score": 82.1, "posts_count": 750},
            {"topic": "æ–°èƒ½æº", "trend_score": 76.8, "posts_count": 650}
        ],
        "timestamp": datetime.now().isoformat()
    }

    logger.info("è¿”å›ç†±é–€è©±é¡Œæ•¸æ“š")
    return result

@app.get("/extract-keywords")
async def extract_keywords(text: str = Query(..., description="è¦æå–é—œéµå­—çš„æ–‡æœ¬")):
    """æå–é—œéµå­—"""
    logger.info(f"æ”¶åˆ° extract-keywords è«‹æ±‚: text={text[:50]}...")

    keywords = ["AI", "äººå·¥æ™ºæ…§", "ç§‘æŠ€", "å‰µæ–°", "ç™¼å±•"]

    result = {
        "success": True,
        "data": {
            "keywords": keywords,
            "confidence_scores": [0.95, 0.88, 0.82, 0.76, 0.65],
            "original_text": text
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info(f"æå–åˆ° {len(keywords)} å€‹é—œéµå­—")
    return result

@app.get("/search-stocks-by-keywords")
async def search_stocks_by_keywords(keywords: str = Query(..., description="é—œéµå­—")):
    """æ ¹æ“šé—œéµå­—æœç´¢è‚¡ç¥¨"""
    logger.info(f"æ”¶åˆ° search-stocks-by-keywords è«‹æ±‚: keywords={keywords}")

    stocks = [
        {"stock_id": "2330", "stock_name": "å°ç©é›»", "relevance_score": 0.95},
        {"stock_id": "2454", "stock_name": "è¯ç™¼ç§‘", "relevance_score": 0.88},
        {"stock_id": "2317", "stock_name": "é´»æµ·", "relevance_score": 0.82}
    ]

    result = {
        "success": True,
        "data": {
            "stocks": stocks,
            "keywords": keywords,
            "total_found": len(stocks)
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info(f"æ‰¾åˆ° {len(stocks)} æ”¯ç›¸é—œè‚¡ç¥¨")
    return result

@app.get("/analyze-topic")
async def analyze_topic(topic: str = Query(..., description="è¦åˆ†æçš„è©±é¡Œ")):
    """åˆ†æè©±é¡Œ"""
    logger.info(f"æ”¶åˆ° analyze-topic è«‹æ±‚: topic={topic}")

    result = {
        "success": True,
        "data": {
            "topic": topic,
            "sentiment": "positive",
            "sentiment_score": 0.75,
            "key_points": [
                "æŠ€è¡“å‰µæ–°æŒçºŒæ¨é€²",
                "å¸‚å ´éœ€æ±‚ç©©å®šå¢é•·",
                "æ”¿ç­–æ”¯æŒåŠ›åº¦åŠ å¤§"
            ],
            "related_stocks": ["2330", "2454", "2317"]
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info(f"å®Œæˆè©±é¡Œåˆ†æ: {topic}")
    return result

@app.get("/generate-content")
async def generate_content(
    topic: str = Query(..., description="è©±é¡Œ"),
    style: str = Query("professional", description="å…§å®¹é¢¨æ ¼")
):
    """ç”Ÿæˆå…§å®¹"""
    logger.info(f"æ”¶åˆ° generate-content è«‹æ±‚: topic={topic}, style={style}")

    result = {
        "success": True,
        "data": {
            "content": f"é—œæ–¼{topic}çš„å°ˆæ¥­åˆ†æï¼šå¸‚å ´è¶¨å‹¢é¡¯ç¤ºè©²é ˜åŸŸå…·æœ‰å¼·å‹çš„æˆé•·æ½›åŠ›ï¼Œå»ºè­°æŠ•è³‡äººå¯†åˆ‡é—œæ³¨ç›¸é—œæ¨™çš„ã€‚",
            "topic": topic,
            "style": style,
            "word_count": 45,
            "generated_at": datetime.now().isoformat()
        },
        "timestamp": datetime.now().isoformat()
    }

    logger.info(f"ç”Ÿæˆå…§å®¹å®Œæˆ: {topic}")
    return result

# ==================== æ•¸æ“šåº«ç®¡ç†åŠŸèƒ½ ====================

@app.post("/admin/import-1788-posts")
async def import_1788_posts():
    """å°å…¥ 1788 ç­† post_records æ•¸æ“šï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        # è®€å– JSON æ•¸æ“šæ–‡ä»¶
        json_file_path = '/app/post_records_1788.json'
        if not os.path.exists(json_file_path):
            return {"error": "post_records_1788.json æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
        
        logger.info(f"ğŸ“Š å¾ JSON æ–‡ä»¶åŠ è¼‰ {len(records)} ç­†è¨˜éŒ„")
        
        with db_connection.cursor() as cursor:
            # æ¸…ç©ºç¾æœ‰æ•¸æ“š
            cursor.execute("DELETE FROM post_records")
            logger.info("ğŸ—‘ï¸ æ¸…ç©ºç¾æœ‰æ•¸æ“š")
            
            # æ‰¹é‡æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            # è½‰æ›è¨˜éŒ„æ ¼å¼
            records_dict = []
            for record in records:
                record_dict = dict(record)
                
                # è™•ç† datetime å­—ç¬¦ä¸²
                for datetime_field in ['created_at', 'updated_at', 'approved_at', 'scheduled_at', 'published_at']:
                    if record_dict.get(datetime_field):
                        if isinstance(record_dict[datetime_field], str):
                            try:
                                record_dict[datetime_field] = datetime.fromisoformat(record_dict[datetime_field].replace('Z', '+00:00'))
                            except:
                                record_dict[datetime_field] = None
                        elif not isinstance(record_dict[datetime_field], datetime):
                            record_dict[datetime_field] = None
                    else:
                        record_dict[datetime_field] = None
                
                # è™•ç† JSON å­—æ®µ - ç¢ºä¿æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                for json_field in ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']:
                    if record_dict.get(json_field):
                        if isinstance(record_dict[json_field], (dict, list)):
                            # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½‰æ›ç‚º JSON å­—ç¬¦ä¸²
                            try:
                                record_dict[json_field] = json.dumps(record_dict[json_field], ensure_ascii=False)
                            except:
                                record_dict[json_field] = None
                        elif isinstance(record_dict[json_field], str):
                            # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸è®Š
                            pass
                        else:
                            record_dict[json_field] = None
                    else:
                        record_dict[json_field] = None
                
                records_dict.append(record_dict)
            
            # æ‰¹é‡æ’å…¥
            logger.info(f"ğŸ“¥ é–‹å§‹æ’å…¥ {len(records_dict)} ç­†è¨˜éŒ„...")
            cursor.executemany(insert_sql, records_dict)
            db_connection.commit()
            
            logger.info(f"âœ… æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„")
            
            # é©—è­‰å°å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"âŒ å°å…¥ 1788 ç­†è¨˜éŒ„å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/test/insert-sample-data")
async def insert_sample_data():
    """æ’å…¥æ¨£æœ¬æ•¸æ“šåˆ° post_records è¡¨ï¼ˆæ¸¬è©¦åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        with db_connection.cursor() as cursor:
            # å‰µå»ºæ¨£æœ¬è¨˜éŒ„
            sample_records = [
                {
                    'post_id': 'test-001',
                    'created_at': datetime.now(),
                    'updated_at': datetime.now(),
                    'session_id': 1,
                    'kol_serial': 200,
                    'kol_nickname': 'KOL-200',
                    'kol_persona': 'æŠ€è¡“åˆ†æå°ˆå®¶',
                    'stock_code': '2330',
                    'stock_name': 'å°ç©é›»',
                    'title': 'å°ç©é›»(2330) æŠ€è¡“é¢åˆ†æèˆ‡æ“ä½œå»ºè­°',
                    'content': 'å°ç©é›»ä»Šæ—¥è¡¨ç¾å¼·å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™é¡¯ç¤ºå¤šé ­è¶¨å‹¢æ˜ç¢ºã€‚å»ºè­°é—œæ³¨580å…ƒæ”¯æ’ä½ï¼Œçªç ´600å…ƒå¯è€ƒæ…®åŠ ç¢¼ã€‚',
                    'content_md': '## å°ç©é›»(2330) æŠ€è¡“é¢åˆ†æ\n\nå°ç©é›»ä»Šæ—¥è¡¨ç¾å¼·å‹¢ï¼ŒæŠ€è¡“æŒ‡æ¨™é¡¯ç¤ºå¤šé ­è¶¨å‹¢æ˜ç¢ºã€‚',
                    'status': 'draft',
                    'reviewer_notes': None,
                    'approved_by': None,
                    'approved_at': None,
                    'scheduled_at': None,
                    'published_at': None,
                    'cmoney_post_id': None,
                    'cmoney_post_url': None,
                    'publish_error': None,
                    'views': 0,
                    'likes': 0,
                    'comments': 0,
                    'shares': 0,
                    'topic_id': 'tech_analysis',
                    'topic_title': 'æŠ€è¡“åˆ†æ',
                    'technical_analysis': '{"rsi": 65, "macd": "bullish", "support": 580}',
                    'serper_data': '{"search_volume": 1000, "trend": "up"}',
                    'quality_score': 8.5,
                    'ai_detection_score': 0.95,
                    'risk_level': 'medium',
                    'generation_params': '{"model": "gpt-4", "temperature": 0.7}',
                    'commodity_tags': '["åŠå°é«”", "ç§‘æŠ€è‚¡", "é¾é ­è‚¡"]',
                    'alternative_versions': '{"version_1": "çŸ­ç·šæ“ä½œ", "version_2": "é•·ç·šæŠ•è³‡"}'
                },
                {
                    'post_id': 'test-002',
                    'created_at': datetime.now(),
                    'updated_at': datetime.now(),
                    'session_id': 1,
                    'kol_serial': 201,
                    'kol_nickname': 'KOL-201',
                    'kol_persona': 'åŸºæœ¬é¢åˆ†æå¸«',
                    'stock_code': '2317',
                    'stock_name': 'é´»æµ·',
                    'title': 'é´»æµ·(2317) è²¡å ±åˆ†æèˆ‡æŠ•è³‡åƒ¹å€¼è©•ä¼°',
                    'content': 'é´»æµ·æœ€æ–°è²¡å ±é¡¯ç¤ºç‡Ÿæ”¶æˆé•·ç©©å®šï¼Œç²åˆ©èƒ½åŠ›æŒçºŒæ”¹å–„ã€‚æœ¬ç›Šæ¯”åˆç†ï¼Œé©åˆé•·æœŸæŠ•è³‡ã€‚',
                    'content_md': '## é´»æµ·(2317) è²¡å ±åˆ†æ\n\né´»æµ·æœ€æ–°è²¡å ±é¡¯ç¤ºç‡Ÿæ”¶æˆé•·ç©©å®šã€‚',
                    'status': 'approved',
                    'reviewer_notes': 'å…§å®¹å“è³ªè‰¯å¥½ï¼Œå»ºè­°ç™¼å¸ƒ',
                    'approved_by': 'admin',
                    'approved_at': datetime.now(),
                    'scheduled_at': None,
                    'published_at': None,
                    'cmoney_post_id': None,
                    'cmoney_post_url': None,
                    'publish_error': None,
                    'views': 150,
                    'likes': 12,
                    'comments': 3,
                    'shares': 2,
                    'topic_id': 'fundamental_analysis',
                    'topic_title': 'åŸºæœ¬é¢åˆ†æ',
                    'technical_analysis': '{"pe_ratio": 12.5, "pb_ratio": 1.2, "roe": 8.5}',
                    'serper_data': '{"search_volume": 800, "trend": "stable"}',
                    'quality_score': 9.0,
                    'ai_detection_score': 0.98,
                    'risk_level': 'low',
                    'generation_params': '{"model": "gpt-4", "temperature": 0.5}',
                    'commodity_tags': '["é›»å­è£½é€ ", "ä»£å·¥", "è˜‹æœæ¦‚å¿µè‚¡"]',
                    'alternative_versions': '{"version_1": "ä¿å®ˆæŠ•è³‡", "version_2": "åƒ¹å€¼æŠ•è³‡"}'
                }
            ]
            
            # æ’å…¥è¨˜éŒ„
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            cursor.executemany(insert_sql, sample_records)
            db_connection.commit()
            
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(sample_records)} ç­†æ¨£æœ¬è¨˜éŒ„")
            
            # é©—è­‰æ’å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸæ’å…¥ {len(sample_records)} ç­†æ¨£æœ¬è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"âŒ æ’å…¥æ¨£æœ¬æ•¸æ“šå¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/create-post-records-table")
async def create_table_manually():
    """æ‰‹å‹•å‰µå»º post_records è¡¨ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        create_post_records_table()
        return {
            "success": True,
            "message": "post_records è¡¨å‰µå»ºæˆåŠŸ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ æ‰‹å‹•å‰µå»ºè¡¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/drop-and-recreate-post-records-table")
async def drop_and_recreate_table():
    """åˆªé™¤ä¸¦é‡æ–°å‰µå»º post_records è¡¨ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        with db_connection.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            db_connection.commit()
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")
            
            # é‡æ–°å‰µå»ºè¡¨
            cursor.execute("""
                CREATE TABLE post_records (
                    post_id VARCHAR PRIMARY KEY,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id INTEGER,
                    kol_serial INTEGER NOT NULL,
                    kol_nickname VARCHAR NOT NULL,
                    kol_persona VARCHAR,
                    stock_code VARCHAR NOT NULL,
                    stock_name VARCHAR NOT NULL,
                    title VARCHAR NOT NULL,
                    content TEXT NOT NULL,
                    content_md TEXT,
                    status VARCHAR DEFAULT 'draft',
                    reviewer_notes TEXT,
                    approved_by VARCHAR,
                    approved_at TIMESTAMP,
                    scheduled_at TIMESTAMP,
                    published_at TIMESTAMP,
                    cmoney_post_id VARCHAR,
                    cmoney_post_url VARCHAR,
                    publish_error TEXT,
                    views BIGINT DEFAULT 0,
                    likes BIGINT DEFAULT 0,
                    comments BIGINT DEFAULT 0,
                    shares BIGINT DEFAULT 0,
                    topic_id VARCHAR,
                    topic_title VARCHAR,
                    technical_analysis TEXT,
                    serper_data TEXT,
                    quality_score FLOAT,
                    ai_detection_score FLOAT,
                    risk_level VARCHAR,
                    generation_params TEXT,
                    commodity_tags TEXT,
                    alternative_versions TEXT
                );
            """)
            db_connection.commit()
            logger.info("âœ… é‡æ–°å‰µå»º post_records è¡¨æˆåŠŸ")
            
        return {
            "success": True,
            "message": "post_records è¡¨å·²åˆªé™¤ä¸¦é‡æ–°å‰µå»º",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ åˆªé™¤ä¸¦é‡æ–°å‰µå»ºè¡¨å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/reset-database")
async def reset_database():
    """é‡ç½®æ•¸æ“šåº«ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        with db_connection.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            db_connection.commit()
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")
            
        return {
            "success": True,
            "message": "æ•¸æ“šåº«å·²é‡ç½®ï¼Œè¡¨å·²åˆªé™¤",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ é‡ç½®æ•¸æ“šåº«å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/fix-database")
async def fix_database():
    """ä¿®å¾©æ•¸æ“šåº«ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        if not db_connection:
            return {"error": "æ•¸æ“šåº«é€£æ¥ä¸å­˜åœ¨"}
        
        # å‰µå»ºæ–°çš„é€£æ¥ä¾†é¿å…äº‹å‹™å•é¡Œ
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–æ•¸æ“šåº«é€£æ¥ä¿¡æ¯
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            return {"error": "DATABASE_URL ç’°å¢ƒè®Šæ•¸ä¸å­˜åœ¨"}
        
        # å‰µå»ºæ–°é€£æ¥
        new_conn = psycopg2.connect(database_url)
        new_conn.autocommit = True
        
        with new_conn.cursor() as cursor:
            # åˆªé™¤ç¾æœ‰è¡¨
            cursor.execute("DROP TABLE IF EXISTS post_records CASCADE")
            logger.info("ğŸ—‘ï¸ åˆªé™¤ç¾æœ‰ post_records è¡¨")
            
            # é‡æ–°å‰µå»ºè¡¨
            cursor.execute("""
                CREATE TABLE post_records (
                    post_id VARCHAR PRIMARY KEY,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    session_id BIGINT,
                    kol_serial INTEGER NOT NULL,
                    kol_nickname VARCHAR NOT NULL,
                    kol_persona VARCHAR,
                    stock_code VARCHAR NOT NULL,
                    stock_name VARCHAR NOT NULL,
                    title VARCHAR NOT NULL,
                    content TEXT NOT NULL,
                    content_md TEXT,
                    status VARCHAR DEFAULT 'draft',
                    reviewer_notes TEXT,
                    approved_by VARCHAR,
                    approved_at TIMESTAMP,
                    scheduled_at TIMESTAMP,
                    published_at TIMESTAMP,
                    cmoney_post_id VARCHAR,
                    cmoney_post_url VARCHAR,
                    publish_error TEXT,
                    views BIGINT DEFAULT 0,
                    likes BIGINT DEFAULT 0,
                    comments BIGINT DEFAULT 0,
                    shares BIGINT DEFAULT 0,
                    topic_id VARCHAR,
                    topic_title VARCHAR,
                    technical_analysis TEXT,
                    serper_data TEXT,
                    quality_score FLOAT,
                    ai_detection_score FLOAT,
                    risk_level VARCHAR,
                    generation_params TEXT,
                    commodity_tags TEXT,
                    alternative_versions TEXT
                );
            """)
            logger.info("âœ… é‡æ–°å‰µå»º post_records è¡¨æˆåŠŸ")
            
        new_conn.close()
        
        return {
            "success": True,
            "message": "æ•¸æ“šåº«å·²ä¿®å¾©ï¼Œè¡¨å·²é‡æ–°å‰µå»º",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ ä¿®å¾©æ•¸æ“šåº«å¤±æ•—: {e}")
        return {"error": str(e)}

@app.post("/admin/import-post-records")
async def import_post_records():
    """å°å…¥ post_records æ•¸æ“šï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    try:
        # è®€å– JSON æ•¸æ“šæ–‡ä»¶
        json_file_path = '/app/post_records_1788.json'
        if not os.path.exists(json_file_path):
            return {"error": "post_records_1788.json æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(json_file_path, 'r', encoding='utf-8') as f:
            records = json.load(f)
        
        logger.info(f"ğŸ“Š å¾ JSON æ–‡ä»¶åŠ è¼‰ {len(records)} ç­†è¨˜éŒ„")
        
        with db_connection.cursor() as cursor:
            # æ¸…ç©ºç¾æœ‰æ•¸æ“š
            cursor.execute("DELETE FROM post_records")
            logger.info("ğŸ—‘ï¸ æ¸…ç©ºç¾æœ‰æ•¸æ“š")
            
            # æ‰¹é‡æ’å…¥æ•¸æ“š
            insert_sql = """
                INSERT INTO post_records (
                    post_id, created_at, updated_at, session_id, kol_serial, kol_nickname, 
                    kol_persona, stock_code, stock_name, title, content, content_md, 
                    status, reviewer_notes, approved_by, approved_at, scheduled_at, 
                    published_at, cmoney_post_id, cmoney_post_url, publish_error, 
                    views, likes, comments, shares, topic_id, topic_title, 
                    technical_analysis, serper_data, quality_score, ai_detection_score, 
                    risk_level, generation_params, commodity_tags, alternative_versions
                ) VALUES (
                    %(post_id)s, %(created_at)s, %(updated_at)s, %(session_id)s, 
                    %(kol_serial)s, %(kol_nickname)s, %(kol_persona)s, %(stock_code)s, 
                    %(stock_name)s, %(title)s, %(content)s, %(content_md)s, %(status)s, 
                    %(reviewer_notes)s, %(approved_by)s, %(approved_at)s, %(scheduled_at)s, 
                    %(published_at)s, %(cmoney_post_id)s, %(cmoney_post_url)s, 
                    %(publish_error)s, %(views)s, %(likes)s, %(comments)s, %(shares)s, 
                    %(topic_id)s, %(topic_title)s, %(technical_analysis)s, %(serper_data)s, 
                    %(quality_score)s, %(ai_detection_score)s, %(risk_level)s, 
                    %(generation_params)s, %(commodity_tags)s, %(alternative_versions)s
                )
            """
            
            # è½‰æ›è¨˜éŒ„æ ¼å¼
            records_dict = []
            for record in records:
                record_dict = dict(record)
                
                # è™•ç† datetime å­—ç¬¦ä¸²
                for datetime_field in ['created_at', 'updated_at', 'approved_at', 'scheduled_at', 'published_at']:
                    if record_dict.get(datetime_field):
                        if isinstance(record_dict[datetime_field], str):
                            try:
                                record_dict[datetime_field] = datetime.fromisoformat(record_dict[datetime_field].replace('Z', '+00:00'))
                            except:
                                record_dict[datetime_field] = None
                        elif not isinstance(record_dict[datetime_field], datetime):
                            record_dict[datetime_field] = None
                    else:
                        record_dict[datetime_field] = None
                
                # è™•ç† JSON å­—æ®µ - è½‰æ›ç‚º TEXT
                for json_field in ['technical_analysis', 'serper_data', 'generation_params', 'commodity_tags', 'alternative_versions']:
                    if record_dict.get(json_field):
                        if isinstance(record_dict[json_field], (dict, list)):
                            # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½‰æ›ç‚º JSON å­—ç¬¦ä¸²
                            try:
                                record_dict[json_field] = json.dumps(record_dict[json_field], ensure_ascii=False)
                            except:
                                record_dict[json_field] = None
                        elif isinstance(record_dict[json_field], str):
                            # å¦‚æœå·²ç¶“æ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸è®Š
                            pass
                        else:
                            record_dict[json_field] = None
                    else:
                        record_dict[json_field] = None
                
                records_dict.append(record_dict)
            
            # æ‰¹é‡æ’å…¥
            cursor.executemany(insert_sql, records_dict)
            db_connection.commit()
            
            logger.info(f"âœ… æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„")
            
            # é©—è­‰å°å…¥çµæœ
            cursor.execute("SELECT COUNT(*) FROM post_records")
            count = cursor.fetchone()[0]
            
            # é¡¯ç¤ºç‹€æ…‹çµ±è¨ˆ
            cursor.execute("SELECT status, COUNT(*) FROM post_records GROUP BY status")
            status_stats = cursor.fetchall()
            
            return {
                "success": True,
                "message": f"æˆåŠŸå°å…¥ {len(records_dict)} ç­†è¨˜éŒ„",
                "total_count": count,
                "status_stats": {status: count for status, count in status_stats},
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"âŒ å°å…¥ post_records å¤±æ•—: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    import uvicorn
    
    # Railway ä½¿ç”¨ PORT ç’°å¢ƒè®Šæ•¸ï¼Œæœ¬åœ°é–‹ç™¼ä½¿ç”¨ 8000
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    logger.info(f"ğŸš€ å•Ÿå‹• Unified API æœå‹™å™¨: {host}:{port}")
    uvicorn.run(app, host=host, port=port)
