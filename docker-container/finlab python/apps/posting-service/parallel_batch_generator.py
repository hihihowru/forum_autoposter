"""
ä¸¦è¡Œæ‰¹é‡è²¼æ–‡ç”Ÿæˆå™¨ - è§£æ±º API å¡è»Šå•é¡Œ
"""
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from fastapi.responses import StreamingResponse

from parallel_processor import post_generation_processor
from post_record_service import PostingRequest

logger = logging.getLogger(__name__)

class ParallelBatchGenerator:
    """ä¸¦è¡Œæ‰¹é‡è²¼æ–‡ç”Ÿæˆå™¨"""
    
    def __init__(self, max_concurrent: int = 2, timeout: int = 120):
        """
        åˆå§‹åŒ–ä¸¦è¡Œæ‰¹é‡ç”Ÿæˆå™¨
        
        Args:
            max_concurrent: æœ€å¤§ä¸¦ç™¼æ•¸é‡ï¼ˆè²¼æ–‡ç”Ÿæˆæ¯”è¼ƒè€—æ™‚ï¼Œå»ºè­°è¨­ç‚º 2-3ï¼‰
            timeout: å–®å€‹è²¼æ–‡ç”Ÿæˆè¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        
    async def generate_posts_parallel_stream(
        self,
        request: Any,  # BatchPostRequest
        progress_callback: Optional[callable] = None
    ) -> AsyncGenerator[str, None]:
        """
        ä¸¦è¡Œç”Ÿæˆè²¼æ–‡ä¸¦ä½¿ç”¨ Server-Sent Events æµå¼è¿”å›çµæœ
        
        Args:
            request: æ‰¹é‡è²¼æ–‡ç”Ÿæˆè«‹æ±‚
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
            
        Yields:
            Server-Sent Events æ ¼å¼çš„å­—ç¬¦ä¸²
        """
        total_posts = len(request.posts)
        successful_posts = 0
        failed_posts = 0
        
        logger.info(f"ğŸš€ é–‹å§‹ä¸¦è¡Œæ‰¹é‡ç”Ÿæˆ {total_posts} ç¯‡è²¼æ–‡ï¼Œæœ€å¤§ä¸¦ç™¼æ•¸: {self.max_concurrent}")
        
        # ç”Ÿæˆ batch ç´šåˆ¥çš„å…±äº« commodity tags
        batch_commodity_tags = []
        should_use_shared_tags = False
        
        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨å…±äº«æ¨™ç±¤
        if request.tags_config and request.tags_config.get('stock_tags', {}).get('batch_shared_tags', False):
            should_use_shared_tags = True
            logger.info("ğŸ·ï¸ æ ¹æ“šå‰ç«¯æ¨™ç±¤é…ç½®å•Ÿç”¨å…¨è²¼åŒç¾¤è‚¡æ¨™")
        elif request.batch_config.get('shared_commodity_tags', True):
            should_use_shared_tags = True
            logger.info("ğŸ·ï¸ æ ¹æ“šæ‰¹é‡é…ç½®å•Ÿç”¨å…±äº«æ¨™ç±¤")
        
        if should_use_shared_tags:
            logger.info("ğŸ·ï¸ ç”Ÿæˆ batch ç´šåˆ¥çš„å…±äº« commodity tags...")
            # é€™è£¡å¯ä»¥å¯¦ç¾å…±äº«æ¨™ç±¤ç”Ÿæˆé‚è¼¯
            logger.info(f"âœ… ç”Ÿæˆ {len(batch_commodity_tags)} å€‹å…±äº« commodity tags")
        else:
            logger.info("ğŸ·ï¸ æœªå•Ÿç”¨å…±äº«æ¨™ç±¤ï¼Œæ¯å€‹è²¼æ–‡å°‡ä½¿ç”¨ç¨ç«‹æ¨™ç±¤")
        
        # ç™¼é€é–‹å§‹äº‹ä»¶
        yield f"data: {json.dumps({'type': 'batch_start', 'total': total_posts, 'session_id': request.session_id, 'shared_tags_count': len(batch_commodity_tags), 'parallel_mode': True})}\n\n"
        
        try:
            # æº–å‚™è²¼æ–‡ç”Ÿæˆè«‹æ±‚åˆ—è¡¨
            post_requests = []
            for index, post_data in enumerate(request.posts):
                # ç‚ºæ¯å€‹è²¼æ–‡æ™ºèƒ½åˆ†é…æ•¸æ“šæº
                logger.info(f"ğŸ§  ç‚ºç¬¬ {index + 1} ç¯‡è²¼æ–‡æ™ºèƒ½åˆ†é…æ•¸æ“šæº...")
                
                # å‰µå»ºKOLå’Œè‚¡ç¥¨é…ç½®
                from smart_data_source_assigner import KOLProfile, StockProfile, smart_assigner
                
                kol_profile = KOLProfile(
                    serial=int(post_data.get('kol_serial', 150)),
                    nickname=f"KOL-{post_data.get('kol_serial', 150)}",
                    persona=request.batch_config.get('kol_persona', 'technical'),
                    expertise_areas=[],
                    content_style=request.batch_config.get('content_style', 'chart_analysis'),
                    target_audience=request.batch_config.get('target_audience', 'active_traders')
                )
                
                stock_profile = StockProfile(
                    stock_code=post_data.get('stock_code'),
                    stock_name=post_data.get('stock_name'),
                    industry='unknown',
                    market_cap='medium',
                    volatility='medium',
                    news_frequency='medium'
                )
                
                # æ™ºèƒ½åˆ†é…æ•¸æ“šæº
                data_source_assignment = smart_assigner.assign_data_sources(
                    kol_profile=kol_profile,
                    stock_profile=stock_profile,
                    batch_context={'trigger_type': 'manual_batch'}
                )
                
                hybrid_data_sources = data_source_assignment.get('recommended_sources', ['finlab', 'serper'])
                logger.info(f"âœ… ç¬¬ {index + 1} ç¯‡è²¼æ–‡åˆ†é…æ•¸æ“šæº: {hybrid_data_sources}")
                
                # æº–å‚™æ–°èé…ç½®
                news_config = request.news_config or {}
                
                # å¦‚æœæœ‰ç†±é–€è©±é¡Œï¼Œæ›´æ–°æ–°èæœç´¢é—œéµå­—
                if hasattr(request, 'trending_topics') and request.trending_topics:
                    topic_keywords = []
                    for topic in request.trending_topics[:3]:  # å–å‰3å€‹è©±é¡Œ
                        topic_id = topic.get('id', '')
                        topic_title = topic.get('title', '')
                        if topic_title:
                            topic_keywords.append({
                                "id": f"topic_{topic_id}",
                                "keyword": topic_title,
                                "type": "trigger_keyword",
                                "description": f"ç†±é–€è©±é¡Œé—œéµå­—: {topic_title}"
                            })
                    
                    news_config['search_keywords'] = topic_keywords
                    logger.info(f"âœ… æ›´æ–°å¾Œçš„æœç´¢é—œéµå­—: {topic_keywords}")
                
                # å‰µå»ºè²¼æ–‡ç”Ÿæˆè«‹æ±‚
                post_request = {
                    'stock_code': post_data.get('stock_code'),
                    'stock_name': post_data.get('stock_name'),
                    'kol_serial': post_data.get('kol_serial'),
                    'kol_persona': request.batch_config.get('kol_persona', 'technical'),
                    'content_style': request.batch_config.get('content_style', 'chart_analysis'),
                    'target_audience': request.batch_config.get('target_audience', 'active_traders'),
                    'batch_mode': True,
                    'session_id': request.session_id,
                    'data_sources': hybrid_data_sources,
                    'explainability_config': request.explainability_config,
                    'news_config': news_config,
                    'tags_config': request.tags_config,
                    'shared_commodity_tags': batch_commodity_tags
                }
                post_requests.append(post_request)
            
            # å®šç¾©é€²åº¦å›èª¿å‡½æ•¸
            async def progress_callback_internal(completed: int, total: int, result: Dict[str, Any]):
                """å…§éƒ¨é€²åº¦å›èª¿å‡½æ•¸"""
                nonlocal successful_posts, failed_posts
                
                percentage = round(completed / total * 100, 1)
                
                if result.get("success"):
                    successful_posts += 1
                    logger.info(f"âœ… é€²åº¦: {completed}/{total} ({percentage}%) - è²¼æ–‡ç”ŸæˆæˆåŠŸ")
                    
                    # ç™¼é€è²¼æ–‡ç”Ÿæˆå®Œæˆäº‹ä»¶
                    post_response = {
                        'type': 'post_generated',
                        'success': True,
                        'post_id': result.get("post_id"),
                        'content': result.get("content"),
                        'error': None,
                        'timestamp': result.get("timestamp"),
                        'progress': {
                            'current': completed,
                            'total': total,
                            'percentage': percentage,
                            'successful': successful_posts,
                            'failed': failed_posts
                        }
                    }
                    yield f"data: {json.dumps(post_response)}\n\n"
                    
                else:
                    failed_posts += 1
                    logger.warning(f"âŒ é€²åº¦: {completed}/{total} ({percentage}%) - è²¼æ–‡ç”Ÿæˆå¤±æ•—: {result.get('error', 'Unknown error')}")
                    
                    # ç™¼é€éŒ¯èª¤äº‹ä»¶
                    error_response = {
                        'type': 'post_error',
                        'success': False,
                        'error': result.get('error', 'Unknown error'),
                        'progress': {
                            'current': completed,
                            'total': total,
                            'percentage': percentage,
                            'successful': successful_posts,
                            'failed': failed_posts
                        }
                    }
                    yield f"data: {json.dumps(error_response)}\n\n"
                
                # èª¿ç”¨å¤–éƒ¨é€²åº¦å›èª¿
                if progress_callback:
                    await progress_callback(completed, total, result)
            
            # ä¸¦è¡Œç”Ÿæˆæ‰€æœ‰è²¼æ–‡
            logger.info(f"ğŸš€ é–‹å§‹ä¸¦è¡Œç”Ÿæˆ {len(post_requests)} ç¯‡è²¼æ–‡...")
            
            # ä½¿ç”¨è‡ªå®šç¾©çš„ä¸¦è¡Œè™•ç†å™¨
            results = await self._generate_posts_with_progress(
                post_requests,
                progress_callback_internal
            )
            
            # ç™¼é€å®Œæˆäº‹ä»¶
            completion_response = {
                'type': 'batch_complete',
                'success': True,
                'total_posts': total_posts,
                'successful_posts': successful_posts,
                'failed_posts': failed_posts,
                'completion_rate': round(successful_posts / total_posts * 100, 1) if total_posts > 0 else 0,
                'timestamp': datetime.now().isoformat(),
                'parallel_mode': True
            }
            yield f"data: {json.dumps(completion_response)}\n\n"
            
            logger.info(f"ğŸ‰ ä¸¦è¡Œæ‰¹é‡ç”Ÿæˆå®Œæˆ: æˆåŠŸ {successful_posts} ç¯‡ï¼Œå¤±æ•— {failed_posts} ç¯‡")
            
        except Exception as e:
            logger.error(f"âŒ ä¸¦è¡Œæ‰¹é‡ç”Ÿæˆå¤±æ•—: {e}")
            import traceback
            logger.error(f"ğŸ“‹ éŒ¯èª¤å †ç–Š: {traceback.format_exc()}")
            
            # ç™¼é€éŒ¯èª¤äº‹ä»¶
            error_response = {
                'type': 'batch_error',
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            yield f"data: {json.dumps(error_response)}\n\n"
    
    async def _generate_posts_with_progress(
        self,
        post_requests: List[Dict[str, Any]],
        progress_callback: callable
    ) -> List[Dict[str, Any]]:
        """
        å¸¶é€²åº¦å›èª¿çš„ä¸¦è¡Œè²¼æ–‡ç”Ÿæˆ
        
        Args:
            post_requests: è²¼æ–‡ç”Ÿæˆè«‹æ±‚åˆ—è¡¨
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
            
        Returns:
            ç”Ÿæˆçµæœåˆ—è¡¨
        """
        # å‰µå»ºä¿¡è™Ÿé‡ä¾†æ§åˆ¶ä¸¦ç™¼æ•¸
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def generate_single_post_with_semaphore(request: Dict[str, Any], index: int) -> Dict[str, Any]:
            """å¸¶ä¿¡è™Ÿé‡æ§åˆ¶çš„å–®å€‹è²¼æ–‡ç”Ÿæˆ"""
            async with semaphore:
                return await self._generate_single_post(request, index)
        
        # å‰µå»ºä»»å‹™åˆ—è¡¨
        tasks = []
        for i, request in enumerate(post_requests):
            task = asyncio.create_task(generate_single_post_with_semaphore(request, i))
            tasks.append(task)
        
        # ä¸¦è¡ŒåŸ·è¡Œä¸¦è™•ç†çµæœ
        results = []
        completed = 0
        
        # ä½¿ç”¨ asyncio.as_completed ä¾†è™•ç†å®Œæˆçš„ä»»å‹™
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                results.append(result)
                completed += 1
                
                # èª¿ç”¨é€²åº¦å›èª¿
                if progress_callback:
                    async for event in progress_callback(completed, len(post_requests), result):
                        yield event
                        
            except Exception as e:
                logger.error(f"âŒ ä»»å‹™åŸ·è¡Œå¤±æ•—: {e}")
                error_result = {
                    "success": False,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                results.append(error_result)
                completed += 1
                
                # èª¿ç”¨é€²åº¦å›èª¿
                if progress_callback:
                    async for event in progress_callback(completed, len(post_requests), error_result):
                        yield event
        
        return results
    
    async def _generate_single_post(self, request: Dict[str, Any], index: int) -> Dict[str, Any]:
        """ç”Ÿæˆå–®å€‹è²¼æ–‡"""
        try:
            logger.info(f"ğŸ”„ ç”Ÿæˆç¬¬ {index + 1} ç¯‡è²¼æ–‡...")
            
            # è½‰æ›ç‚º PostingRequest å°è±¡
            posting_request = PostingRequest(**request)
            
            # èª¿ç”¨ç¾æœ‰çš„è²¼æ–‡ç”Ÿæˆé‚è¼¯
            from main import manual_post_content
            result = await manual_post_content(posting_request)
            
            return {
                "success": result.success,
                "post_id": result.post_id,
                "content": result.content,
                "error": result.error,
                "timestamp": result.timestamp.isoformat(),
                "index": index
            }
            
        except Exception as e:
            logger.error(f"âŒ ç¬¬ {index + 1} ç¯‡è²¼æ–‡ç”Ÿæˆå¤±æ•—: {e}")
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "index": index
            }

# å…¨å±€å¯¦ä¾‹
parallel_batch_generator = ParallelBatchGenerator(max_concurrent=2, timeout=120)


